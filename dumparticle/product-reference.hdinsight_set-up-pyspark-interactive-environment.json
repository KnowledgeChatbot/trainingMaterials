{"Title":"Azure HDInsight 工具 - 为 Visual Studio Code 设置 PySpark 交互式环境","Description":"了解如何使用用于 Visual Studio Code 的 Azure HDInsight 工具来创建、提交查询和脚本。","Content":"# <a name=\"set-up-the-pyspark-interactive-environment-for-visual-studio-code\"></a>为 Visual Studio Code 设置 PySpark 交互式环境\r \r 以下步骤显示如何通过运行“HDInsight: PySpark Interactive”安装 Python 包。\r \r \r ## <a name=\"set-up-the-pyspark-interactive-environment-on-macos-and-linux\"></a>在 macOS 和 Linux 上设置 PySpark 交互式环境\r 如果使用 **python 3.x**，需要将命令 **pip3** 用于以下步骤：\r \r 1. 请确保已安装“Python”和“pip”。\r  \r     ![Python pip 版本](./media/set-up-pyspark-interactive-environment/check-python-pip-version.png)\r \r 2.  安装 Jupyter。\r     ```\r     sudo pip install jupyter\r     ```\r    Linux 和 macOS 上可能会显示以下错误消息：\r \r    ![错误 1](./media/set-up-pyspark-interactive-environment/error1.png)\r \r    ```Resolve:\r     sudo pip uninstall asyncio\r     sudo pip install trollies\r     ```\r \r 3. 安装 **libkrb5-dev**（仅适用于 Linux）。 可能会显示以下错误消息：\r \r    ![错误 2](./media/set-up-pyspark-interactive-environment/error2.png)\r        \r    ```Resolve:\r    sudo apt-get install libkrb5-dev \r    ```\r \r 3. 安装 **sparkmagic**。\r    ```\r    sudo pip install sparkmagic\r    ```\r \r 4. 通过运行以下命令，确保 **ipywidgets** 已正确安装：\r    ```\r    sudo jupyter nbextension enable --py --sys-prefix widgetsnbextension\r    ```\r    ![安装包装器内核](./media/set-up-pyspark-interactive-environment/ipywidget-enable.png)\r  \r \r 5. 安装包装器内核。 运行 **pip show sparkmagic**。 输出显示 **sparkmagic** 安装的路径。 \r \r     ![sparkmagic 位置](./media/set-up-pyspark-interactive-environment/sparkmagic-location.png)\r    \r 6. 转到该位置并运行：\r \r    ```Python2\r    sudo jupyter-kernelspec install sparkmagic/kernels/pysparkkernel   \r    ```\r    ```Python3\r    sudo jupyter-kernelspec install sparkmagic/kernels/pyspark3kernel\r    ```\r \r    ![jupyter kernelspec 安装](./media/set-up-pyspark-interactive-environment/jupyter-kernelspec-install.png)\r 7. 检查安装状态。\r \r     ```\r     jupyter-kernelspec list\r     ```\r     ![jupyter kernelspec 列表](./media/set-up-pyspark-interactive-environment/jupyter-kernelspec-list.png)\r \r     对于可用内核： \r     - **python2** 和 **pysparkkernel** 对应于 **python 2.x**。 \r     - **python3** 和 **pyspark3kernel** 对应于 **python 3.x**。 \r \r 8. 重新启动 VS Code 并回到运行“HDInsight: PySpark Interactive”的脚本编辑器。\r \r ## <a name=\"next-steps\"></a>后续步骤\r \r ### <a name=\"demo\"></a>演示\r * 用于 VS Code 的 HDInsight：[视频](https://go.microsoft.com/fwlink/?linkid=858706)\r \r ### <a name=\"tools-and-extensions\"></a>工具和扩展\r * [使用用于 Visual Studio Code 的 Azure HDInsight 工具](hdinsight-for-vscode.md)\r * [使用用于 IntelliJ 的 Azure 工具包创建和提交 Spark Scala 应用程序](spark/apache-spark-intellij-tool-plugin.md)\r * [通过 SSH 使用用于 IntelliJ 的 Azure 工具包远程调试 Spark 应用程序](spark/apache-spark-intellij-tool-debug-remotely-through-ssh.md)\r * [通过 VPN 使用用于 IntelliJ 的 Azure 工具包远程调试 Spark 应用程序](spark/apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)\r * [使用用于 Eclipse 的 Azure 工具包中的 HDInsight 工具创建 Spark 应用程序](spark/apache-spark-eclipse-tool-plugin.md)\r * [将用于 IntelliJ 的 HDInsight 工具与 Hortonworks 沙盒配合使用](hadoop/hdinsight-tools-for-intellij-with-hortonworks-sandbox.md)\r * [在 HDInsight 上的 Spark 群集中使用 Zeppelin 笔记本](spark/apache-spark-zeppelin-notebook.md)\r * [在 HDInsight 的 Spark 群集中可用于 Jupyter 笔记本的内核](spark/apache-spark-jupyter-notebook-kernels.md)\r * [Use external packages with Jupyter notebooks（将外部包与 Jupyter 笔记本配合使用）](spark/apache-spark-jupyter-notebook-use-external-packages.md)\r * [Install Jupyter on your computer and connect to an HDInsight Spark cluster（在计算机上安装 Jupyter 并连接到 HDInsight Spark 群集）](spark/apache-spark-jupyter-notebook-install-locally.md)\r * [在 Azure HDInsight 中使用 Zeppelin 运行 Hive 查询](hdinsight-connect-hive-zeppelin.md)\r "}