{"Title":"使用 Azure Cosmos DB 和 HDInsight 运行 Hadoop 作业","Description":"了解如何使用 Azure Cosmos DB 和 Azure HDInsight 运行一个简单的 Hive、Pig 和 MapReduce 作业。","Content":"# <a name=\"Azure Cosmos DB-HDInsight\"></a>使用 Azure Cosmos DB 和 HDInsight 运行 Apache Hive、Pig 或 Hadoop 作业\r 本教程介绍了如何在 Azure HDInsight 上使用 Cosmos DB 的 Hadoop 连接器运行 [Apache Hive][apache-hive]、[Apache Pig][apache-pig] 和 [Apache Hadoop][apache-hadoop] MapReduce 作业。 Cosmos DB 的 Hadoop 连接器使 Cosmos DB 可以充当 Hive、Pig 以及 MapReduce 作业的源和接收器。 本教程将 Cosmos DB 用作 Hadoop 作业的数据源和目的地。\r \r 完成本教程后，能够回答以下问题：\r \r * 如何使用 Hive、Pig 或 MapReduce 作业从 Cosmos DB 加载数据？\r * 如何使用 Hive、Pig 或 MapReduce 作业在 Cosmos DB 中存储数据？\r \r <!-- Not Available > [!VIDEO https://channel9.msdn.com/Blogs/Azure/Use-Azure-DocumentDB-Hadoop-Connector-with-Azure-HDInsight/player]-->\r \r 然后，返回到本文，在这里你将获得有关如何对 Cosmos DB 数据运行分析作业的完整详细信息。\r \r > [!TIP]\r > 本教程假定先前有使用 Apache Hadoop、Hive 和/或 Pig 的经验。 如果不熟悉 Apache Hadoop、Hive 和 Pig，建议访问 [Apache Hadoop 文档][apache-hadoop-doc]。 本教程还假定你具有使用 Cosmos DB 的经验，并且拥有一个 Cosmos DB 帐户。 如果不熟悉 Cosmos DB 或没有 Cosmos DB 帐户，请查看[入门][getting-started]页。\r >\r >\r \r 没有时间完成教程，只想获得有关 Hive、Pig 和 MapReduce 的完整示例 PowerShell 脚本？ 没问题，可在 [此处][hdinsight-samples]获得。 此下载还包含对于这些示例的 hql、pig 及 java 文件。\r \r ## <a name=\"NewestVersion\"></a>最新版本\r <table border='1'>\r     <tr><th>Hadoop 连接器版本</th>\r         <td>1.2.0</td></tr>\r     <tr><th>脚本 URI</th>\r         <td>https://portalcontent.blob.core.chinacloudapi.cn/scriptaction/documentdb-hadoop-installer-v04.ps1</td></tr>\r     <tr><th>修改日期</th>\r         <td>04/26/2016</td></tr>\r     <tr><th>支持的 HDInsight 版本</th>\r         <td>3.1、3.2</td></tr>\r     <tr><th>更改日志</th>\r         <td>将 Azure Cosmos DB Java SDK 更新到 1.6.0</br>\r             针对同时作为来源和接收器的已分区集合添加了支持</br>\r         </td></tr>\r </table>\r \r ## <a name=\"Prerequisites\"></a>先决条件\r 在按照本教程中的说明操作之前，请确保已有下列各项：\r \r * Cosmos DB 帐户、数据库以及其中包含文档的集合。 有关详细信息，请参阅 [Cosmos DB 入门][getting-started]。 使用 [Cosmos DB 导入工具][import-data]将示例数据导入到 Cosmos DB 帐户中。\r * 吞吐量。 从 HDInsight 进行的读取和写入操作将计入为集合分配的请求单位。\r * 在每个输出集合中用于其他存储的步骤的容量。 存储过程用于传输生成的文档。\r * 从 Hive、Pig 或 MapReduce 作业生成的文档的容量。\r * [可选] 可用于一个附加集合的容量。\r \r > [!WARNING]\r > 为了避免在任何作业期间创建一个新集合，可以将结果打印到 stdout，将输出保存到 WASB 容器，或指定一个现有集合。 指定现有集合时，会在集合内创建新文档，如果 *ID*中有冲突，只会影响现有文档。 \r             **连接器自动覆盖出现 ID 冲突的现有文档**。 通过将 upsert 选项设置为 false 可以关闭此功能。 如果 upsert 为 false，并且发生冲突，则 Hadoop 作业会失败；并报告 ID 冲突错误。\r >\r >\r \r ## <a name=\"ProvisionHDInsight\"></a>步骤 1：创建新的 HDInsight 群集。\r 本教程使用 Azure 门户中的脚本操作自定义 HDInsight 群集。 在本教程中，我们将使用 Azure 门户来创建 HDInsight 群集。 有关如何使用 PowerShell cmdlet 或 HDInsight .NET SDK 的说明，请查看[使用脚本操作自定义 HDInsight 群集][hdinsight-custom-provision]一文。\r \r 1. 登录到 [Azure 门户][azure-portal]。\r 2. 单击左侧导航栏顶部的“+ 新建”，在“新建”边栏选项卡的顶部搜索栏中，搜索“HDInsight”。\r 3. **Microsoft** 发布的 **HDInsight** 将出现在结果顶部。 单击，并单击“创建”。\r 4. 在“新建 HDInsight 群集”的“创建”边栏选项卡中，输入**群集名称**，并选择想要在其下预配此资源的**订阅**。\r \r     <table border='1'>\r         <tr><td>群集名称</td><td>为群集命名。<br/>\r DNS 名称必须以字母数字字符开头和结尾，并且可以包含短划线。<br/>\r 字段必须是介于 3 到 63 个字符之间的字符串。</td></tr>\r         <tr><td>订阅名称</td>\r             <td>如果具有多个 Azure 订阅，请选择将承载 HDInsight 群集的订阅。 </td></tr>\r     </table>\r 5. 单击 **选择群集类型** 并将以下属性设置为指定值。\r \r     <table border='1'>\r         <tr><td>群集类型</td><td><strong>Hadoop</strong></td></tr>\r         <tr><td>群集层</td><td><strong>标准</strong></td></tr>\r         <tr><td>操作系统</td><td><strong>Windows</strong></td></tr>\r         <tr><td>版本</td><td>最新版本</td></tr>\r     </table>\r \r     现在，单击“选择”。\r \r     ![提供 Hadoop HDInsight 初始群集详细信息][image-customprovision-page1]\r 6. 单击“凭据”来设置登录和远程访问凭据。 选择“群集登录用户名”“群集登录密码”。\r \r     如果希望远程进入群集，请选择边栏选项卡底部的“是”，并提供用户名和密码。\r 7. 单击“数据源”设置数据访问的主要位置。 选择“选取方法”并指定一个现有的存储帐户或创建一个新的帐户。\r 8. 在同一边栏选项卡上，指定一个**默认容器**和**位置**。 然后，单击“选择”。\r \r    > [!NOTE]\r    > 选择一个接近 Cosmos DB 帐户所在区域的位置，以实现更好的性能\r    >\r    >\r 9. 单击“定价”以选择节点的数量和类型。 可保留默认配置，稍后可调整辅助角色节点数。\r 10. 单击“可选配置”，并单击可选配置边栏选项卡的“脚本操作”。\r \r      在“脚本操作”中输入以下用于自定义 HDInsight 群集的信息。\r \r      <table border='1'>\r          <tr><th>属性</th><th>值</th></tr>\r          <tr><td>名称</td>\r              <td>指定脚本操作的名称。</td></tr>\r          <tr><td>脚本 URI</td>\r              <td>指定要调用来自定义群集的脚本的 URI。</br></br>\r 请输入： </br> <strong>https://portalcontent.blob.core.chinacloudapi.cn/scriptaction/documentdb-hadoop-installer-v04.ps1</strong>。</td></tr>\r          <tr><td>头</td>\r              <td>单击指示要在头节点中运行 PowerShell 脚本的复选框。</br></br>\r              <strong>选中此复选框</strong>。</td></tr>\r          <tr><td>辅助角色</td>\r              <td>单击指示要在辅助角色节点中运行 PowerShell 脚本的复选框。</br></br>\r              <strong>选中此复选框</strong>。</td></tr>\r          <tr><td>Zookeeper</td>\r              <td>单击指示要在 Zookeeper 中运行 PowerShell 脚本的复选框。</br></br>\r              <strong>非必需</strong>。\r              </td></tr>\r          <tr><td>Parameters</td>\r              <td>根据脚本的需要，请指定参数。</br></br>\r              <strong>参数不是必需的</strong>。</td></tr>\r      </table>\r 11. 创建一个新的 **资源组** 或使用 Azure 订阅下的现有资源组。\r 12. 现在，选中“固定到仪表板”来跟踪其部署并单击“创建”！\r \r ## <a name=\"InstallCmdlets\"></a>步骤 2：安装和配置 Azure PowerShell。\r 1. 安装 Azure PowerShell 中的说明进行操作。 可在 [此处][powershell-install-configure]找到说明。\r \r    > [!NOTE]\r    > 或者，只需了解 Hive 查询，可以使用 HDInsight 的联机 Hive 编辑器。 若要这样做，请登录到 [Azure 门户][azure-portal]，单击左侧窗格中的“HDInsight”以查看 HDInsight 群集的列表。 单击要对其运行 Hive 查询的群集，并单击“查询控制台”。\r    >\r    >\r 2. 打开 Azure PowerShell 集成脚本环境：\r \r    * 在运行 Windows 8 或 Windows Server 2012 或更高版本的计算机上，可以使用内置搜索。 从“开始”屏幕上，键入 **powershell ise**，并单击“Enter”。\r    * 在运行早于 Windows 8 或 Windows Server 2012 的版本的计算机上，请使用“开始”菜单。 从“开始”菜单上，在搜索框中键入**命令提示符**，并在结果列表中，单击“命令提示符”。 在命令提示符中，键入 **powershell_ise**，并单击“Enter”。\r 3. 添加 Azure 帐户。\r \r    1. 在控制台窗格中，键入 Add-AzureAccount -Environment AzureChinaCloud 并单击“Enter”。\r    2. 键入与 Azure 订阅相关联的电子邮件地址并单击“继续”。\r    3. 键入 Azure 订阅的密码。\r    4. 单击“登录” 。\r 4. 以下关系图标识 Azure PowerShell 脚本环境的重要部分。\r \r     ![Azure PowerShell 的关系图][azure-powershell-diagram]\r \r ## <a name=\"RunHive\"></a>步骤 3：使用 Cosmos DB 和 HDInsight 运行 Hive 作业\r > [!IMPORTANT]\r > 必须使用配置设置填写所有由 < > 表示的变量。\r >\r >\r \r 1. 在 PowerShell 脚本窗格中设置以下变量。\r \r         # Provide Azure subscription name, the Azure Storage account and container that is used for the default HDInsight file system.\r         $subscriptionName = \"<SubscriptionName>\"\r         $storageAccountName = \"<AzureStorageAccountName>\"\r         $containerName = \"<AzureStorageContainerName>\"\r \r         # Provide the HDInsight cluster name where you want to run the Hive job.\r         $clusterName = \"<HDInsightClusterName>\"\r 2. <p>让我们开始构造查询字符串。 我们将编写 Hive 查询，该查询采用来自 Azure Cosmos DB 集合的所有文档系统生成的时间戳 (_ts) 和唯一 ID (_rid)，按分钟计算所有文档，并将结果存储回新 Azure Cosmos DB 集合。</p>\r \r     <p>首先，从 Azure Cosmos DB 集合创建 Hive 表。 将以下代码片段添加到 PowerShell 脚本窗格中从 #1 开始的代码片段<strong>之后</strong>。 请确保包括可选的 DocumentDB.query 参数，以便将我们的文档调整为 just_ts 和 _rid。</p>\r \r    > [!NOTE]\r    > **命名 DocumentDB.inputCollections 不是一个错误。** 是，我们允许添加多个集合来作为输入： </br>\r    >\r    >\r \r         '*DocumentDB.inputCollections*' = '*\\<DocumentDB Input Collection Name 1\\>*,*\\<DocumentDB Input Collection Name 2\\>*' A1A</br> The collection names are separated without spaces, using only a single comma.\r \r         # Create a Hive table using data from DocumentDB. Pass DocumentDB the query to filter transferred data to _rid and _ts.\r         $queryStringPart1 = \"drop table DocumentDB_timestamps; \"  +\r                             \"create external table DocumentDB_timestamps(id string, ts BIGINT) \"  +\r                             \"stored by 'com.microsoft.azure.documentdb.hive.DocumentDBStorageHandler' \"  +\r                             \"tblproperties ( \" +\r                                 \"'DocumentDB.endpoint' = '<DocumentDB Endpoint>', \" +\r                                 \"'DocumentDB.key' = '<DocumentDB Primary Key>', \" +\r                                 \"'DocumentDB.db' = '<DocumentDB Database Name>', \" +\r                                 \"'DocumentDB.inputCollections' = '<DocumentDB Input Collection Name>', \" +\r                                 \"'DocumentDB.query' = 'SELECT r._rid AS id, r._ts AS ts FROM root r' ); \"\r \r 3. 接下来，让我们为输出集合创建 Hive 表。 输出文档属性将为月、日、小时、分钟和发生次数总数。\r \r    > [!NOTE]\r    > **同样，命名 DocumentDB.outputCollections 不是一个错误。** 是，我们允许添加多个集合作为输出： </br>\r    > '*DocumentDB.outputCollections*' = '*\\<DocumentDB Output Collection Name 1\\>*,*\\<DocumentDB Output Collection Name 2\\>*' </br> 不使用空格分隔集合名称，仅使用单个逗号。 </br></br>\r    > 文档将为跨多个集合的分布式轮循机制。 一批文档将存储在一个集合中，第二批文档则存储在下一个集合中，如此类推。\r    >\r    >\r \r        # Create a Hive table for the output data to DocumentDB.\r        $queryStringPart2 = \"drop table DocumentDB_analytics; \" +\r                              \"create external table DocumentDB_analytics(Month INT, Day INT, Hour INT, Minute INT, Total INT) \" +\r                              \"stored by 'com.microsoft.azure.documentdb.hive.DocumentDBStorageHandler' \" +\r                              \"tblproperties ( \" +\r                                  \"'DocumentDB.endpoint' = '<DocumentDB Endpoint>', \" +\r                                  \"'DocumentDB.key' = '<DocumentDB Primary Key>', \" +  \r                                  \"'DocumentDB.db' = '<DocumentDB Database Name>', \" +\r                                  \"'DocumentDB.outputCollections' = '<DocumentDB Output Collection Name>' ); \"\r 4. 最后，让我们按月、日、小时和分钟计算文档，并将结果插入回输出 Hive 表。\r \r         # GROUP BY minute, COUNT entries for each, INSERT INTO output Hive table.\r         $queryStringPart3 = \"INSERT INTO table DocumentDB_analytics \" +\r                               \"SELECT month(from_unixtime(ts)) as Month, day(from_unixtime(ts)) as Day, \" +\r                               \"hour(from_unixtime(ts)) as Hour, minute(from_unixtime(ts)) as Minute, \" +\r                               \"COUNT(*) AS Total \" +\r                               \"FROM DocumentDB_timestamps \" +\r                               \"GROUP BY month(from_unixtime(ts)), day(from_unixtime(ts)), \" +\r                               \"hour(from_unixtime(ts)) , minute(from_unixtime(ts)); \"\r 5. 添加以下脚本代码段以从之前的查询创建 Hive 作业定义。\r \r         # Create a Hive job definition.\r         $queryString = $queryStringPart1 + $queryStringPart2 + $queryStringPart3\r         $hiveJobDefinition = New-AzureHDInsightHiveJobDefinition -Query $queryString\r \r     还可以使用 -File 开关来指定 HDFS 上的 HiveQL 脚本文件。\r 6. 添加以下代码段以保存开始时间并提交 Hive 作业。\r \r         # Save the start time and submit the job to the cluster.\r         $startTime = Get-Date\r         Select-AzureSubscription $subscriptionName\r         $hiveJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $hiveJobDefinition\r 7. 添加以下内容来等待 Hive 作业完成。\r \r         # Wait for the Hive job to complete.\r         Wait-AzureHDInsightJob -Job $hiveJob -WaitTimeoutInSeconds 3600\r 8. 添加以下内容以打印标准输出以及开始和结束时间。\r \r         # Print the standard error, the standard output of the Hive job, and the start and end time.\r         $endTime = Get-Date\r         Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $hiveJob.JobId -StandardOutput\r         Write-Host \"Start: \" $startTime \", End: \" $endTime -ForegroundColor Green\r 9. **运行** 新脚本！ **单击** 绿色执行按钮。\r 10. 检查结果。 登录到 [Azure 门户][azure-portal]。\r \r    1. 单击左侧面板上的“浏览”<strong></strong>。 </br>\r    2. 单击浏览面板右上角的“全部”<strong></strong>。 </br>\r    3. 找到并单击“Azure Cosmos DB 帐户”<strong></strong>。 </br>\r    4. 接下来，找到“Azure Cosmos DB 帐户”、“Azure Cosmos DB 数据库”以及与 Hive 查询中所指定的输出集合相关联的“Azure Cosmos DB 集合”<strong></strong><strong></strong><strong></strong>。</br>\r    5. 最后，单击“开发人员工具”<strong></strong>下方的“文档资源管理器”<strong></strong>。</br></p>\r \r    会看到 Hive 查询的结果。\r \r    ![Hive 查询结果][image-hive-query-results]\r \r ## <a name=\"RunPig\"></a>步骤 4：使用 Cosmos DB 和 HDInsight 运行 Pig 作业\r > [!IMPORTANT]\r > 必须使用配置设置填写所有由 < > 表示的变量。\r >\r >\r \r 1. 在 PowerShell 脚本窗格中设置以下变量。\r \r         # Provide Azure subscription name.\r         $subscriptionName = \"Azure Subscription Name\"\r \r         # Provide HDInsight cluster name where you want to run the Pig job.\r         $clusterName = \"Azure HDInsight Cluster Name\"\r 2. <p>让我们开始构造查询字符串。 我们将编写 Pig 查询，该查询采用来自 Azure Cosmos DB 集合的所有文档系统生成的时间戳 (_ts) 和唯一 ID (_rid)，按分钟计算所有文档，并将结果存储回新 Azure Cosmos DB 集合。</p>\r     <p>首先，从 Cosmos DB 将文档加载到 HDInsight 中。 将以下代码片段添加到 PowerShell 脚本窗格中从 #1 开始的代码片段<strong>之后</strong>。 请确保添加了 DocumentDB.query 到可选的 DocumentDB 查询参数，以便将我们的文档调整到 just_ts 和 _rid。</p>\r \r    > [!NOTE]\r    > 是，我们允许添加多个集合来作为输入： </br>\r    > '*\\<DocumentDB Input Collection Name 1\\>*,*\\<DocumentDB Input Collection Name 2\\>*'</br> 不使用空格分隔集合名称，仅使用单个逗号。 </b>\r    >\r    >\r \r     文档将为跨多个集合的分布式轮循机制。 一批文档将存储在一个集合中，第二批文档则存储在下一个集合中，如此类推。\r \r         # Load data from Cosmos DB. Pass DocumentDB query to filter transferred data to _rid and _ts.\r         $queryStringPart1 = \"DocumentDB_timestamps = LOAD '<DocumentDB Endpoint>' USING com.microsoft.azure.documentdb.pig.DocumentDBLoader( \" +\r                                                         \"'<DocumentDB Primary Key>', \" +\r                                                         \"'<DocumentDB Database Name>', \" +\r                                                         \"'<DocumentDB Input Collection Name>', \" +\r                                                         \"'SELECT r._rid AS id, r._ts AS ts FROM root r' ); \"\r 3. 接下来，让我们按月、日、小时、分钟和发生次数总数计算文档。\r \r        # GROUP BY minute and COUNT entries for each.\r        $queryStringPart2 = \"timestamp_record = FOREACH DocumentDB_timestamps GENERATE `$0#'id' as id:int, ToDate((long)(`$0#'ts') * 1000) as timestamp:datetime; \" +\r                            \"by_minute = GROUP timestamp_record BY (GetYear(timestamp), GetMonth(timestamp), GetDay(timestamp), GetHour(timestamp), GetMinute(timestamp)); \" +\r                            \"by_minute_count = FOREACH by_minute GENERATE FLATTEN(group) as (Year:int, Month:int, Day:int, Hour:int, Minute:int), COUNT(timestamp_record) as Total:int; \"\r 4. 最后，让我们将结果存储到我们新的输出集合。\r \r    > [!NOTE]\r    > 是，我们允许添加多个集合作为输出： </br>\r    > '\\<DocumentDB Output Collection Name 1\\>,\\<DocumentDB Output Collection Name 2\\>'</br> 不使用空格分隔集合名称，仅使用单个逗号。</br>\r    > 文档将是跨多个集合的分布式轮循机制。 一批文档将存储在一个集合中，第二批文档则存储在下一个集合中，如此类推。\r    >\r    >\r \r         # Store output data to Cosmos DB.\r         $queryStringPart3 = \"STORE by_minute_count INTO '<DocumentDB Endpoint>' \" +\r                             \"USING com.microsoft.azure.documentdb.pig.DocumentDBStorage( \" +\r                                 \"'<DocumentDB Primary Key>', \" +\r                                 \"'<DocumentDB Database Name>', \" +\r                                 \"'<DocumentDB Output Collection Name>'); \"\r 5. 添加以下脚本代码段以从之前的查询创建 Pig 作业定义。\r \r         # Create a Pig job definition.\r         $queryString = $queryStringPart1 + $queryStringPart2 + $queryStringPart3\r         $pigJobDefinition = New-AzureHDInsightPigJobDefinition -Query $queryString -StatusFolder $statusFolder\r \r     也可以使用 -File 开关来指定 HDFS 上的 Pig 脚本文件。\r 6. 添加以下代码段以保存开始时间并提交 Pig 作业。\r \r         # Save the start time and submit the job to the cluster.\r         $startTime = Get-Date\r         Select-AzureSubscription $subscriptionName\r         $pigJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $pigJobDefinition\r 7. 添加以下内容来等待 Pig 作业完成。\r \r         # Wait for the Pig job to complete.\r         Wait-AzureHDInsightJob -Job $pigJob -WaitTimeoutInSeconds 3600\r 8. 添加以下内容以打印标准输出以及开始和结束时间。\r \r         # Print the standard error, the standard output of the Hive job, and the start and end time.\r         $endTime = Get-Date\r         Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $pigJob.JobId -StandardOutput\r         Write-Host \"Start: \" $startTime \", End: \" $endTime -ForegroundColor Green\r 9. **运行** 新脚本！ **单击** 绿色执行按钮。\r 10. 检查结果。 登录到 [Azure 门户][azure-portal]。\r \r     1. 单击左侧面板上的“浏览”<strong></strong>。 </br>\r     2. 单击浏览面板右上角的“全部”<strong></strong>。 </br>\r     3. 找到并单击“Azure Cosmos DB 帐户”<strong></strong>。 </br>\r     4. 接下来，找到“Azure Cosmos DB 帐户”、“Azure Cosmos DB 数据库”以及与 Pig 查询中所指定输出集合相关联的“Azure Cosmos DB 集合”<strong></strong><strong></strong><strong></strong>。</br>\r     5. 最后，单击“开发人员工具”<strong></strong>下方的“文档资源管理器”<strong></strong>。</br></p>\r \r     会看到 Pig 查询的结果。\r \r     ![Pig 查询结果][image-pig-query-results]\r \r ## <a name=\"RunMapReduce\"></a>步骤 5：使用 Azure Cosmos DB 和 HDInsight 运行 MapReduce 作业\r 1. 在 PowerShell 脚本窗格中设置以下变量。\r \r         $subscriptionName = \"<SubscriptionName>\"   # Azure subscription name\r         $clusterName = \"<ClusterName>\"             # HDInsight cluster name\r 2. 我们将执行 MapReduce 作业，该作业从 Azure Cosmos DB 集合计算每个文档属性的发生次数。 在以上片段**之后**添加此脚本片段。\r \r         # Define the MapReduce job.\r         $TallyPropertiesJobDefinition = New-AzureHDInsightMapReduceJobDefinition -JarFile \"wasb:///example/jars/TallyProperties-v01.jar\" -ClassName \"TallyProperties\" -Arguments \"<DocumentDB Endpoint>\",\"<DocumentDB Primary Key>\", \"<DocumentDB Database Name>\",\"<DocumentDB Input Collection Name>\",\"<DocumentDB Output Collection Name>\",\"<[Optional] DocumentDB Query>\"\r \r    > [!NOTE]\r    > Cosmos DB Hadoop 连接器自定义安装中附带了 TallyProperties-v01.jar。\r    >\r    >\r 3. 添加以下命令来提交 MapReduce 作业。\r \r         # Save the start time and submit the job.\r         $startTime = Get-Date\r         Select-AzureSubscription $subscriptionName\r         $TallyPropertiesJob = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $TallyPropertiesJobDefinition | Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600  \r \r     除了 MapReduce 作业定义外，还要提供需运行 MapReduce 作业的 HDInsight 群集名称，以及凭据。 Start-AzureHDInsightJob 是异步调用。 要检查作业是否完成，请使用 *Wait-AzureHDInsightJob* cmdlet。\r 4. 添加以下命令来检查运行 MapReduce 作业时的错误。\r \r         # Get the job output and print the start and end time.\r         $endTime = Get-Date\r         Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $TallyPropertiesJob.JobId -StandardError\r         Write-Host \"Start: \" $startTime \", End: \" $endTime -ForegroundColor Green\r 5. **运行** 新脚本！ **单击** 绿色执行按钮。\r 6. 检查结果。 登录到 [Azure 门户][azure-portal]。\r \r    1. 单击左侧面板上的“浏览”<strong></strong>。\r    2. 单击浏览面板右上角的“全部”<strong></strong>。\r    3. 找到并单击“Azure Cosmos DB 帐户”<strong></strong>。\r    4. 接下来，找到“Azure Cosmos DB 帐户”、“Azure Cosmos DB 数据库”以及与 MapReduce 作业中所指定输出集合相关联的“Azure Cosmos DB 集合”<strong></strong><strong></strong><strong></strong>。\r    5. 最后，单击“开发人员工具”<strong></strong>下方的“文档资源管理器”<strong></strong>。\r \r       会看到 MapReduce 作业的结果。\r \r       ![MapReduce 查询结果][image-mapreduce-query-results]\r \r ## <a name=\"NextSteps\"></a>后续步骤\r 祝贺！ 只需使用 Azure Cosmos DB 和 HDInsight 运行第一个 Hive、Pig 和 MapReduce 作业。\r \r 我们的 Hadoop Connector 是开源的。 如果有兴趣，欢迎在 [GitHub][github] 上供稿。\r \r 若要了解更多信息，请参阅下列文章：\r \r * [使用 Documentdb 开发 Java 应用程序][documentdb-java-application]\r * [为 HDInsight 中的 Hadoop 开发 Java MapReduce 程序][hdinsight-develop-deploy-java-mapreduce]\r * [将 Hadoop 与 HDInsight 中的 Hive 配合使用以分析手机使用情况][hdinsight-get-started]\r * [将 MapReduce 与 HDInsight 配合使用][hdinsight-use-mapreduce]\r * [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]\r * [将 Pig 与 HDInsight 配合使用][hdinsight-use-pig]\r * [使用脚本操作自定义 HDInsight 群集][hdinsight-hadoop-customize-cluster]\r \r [apache-hadoop]: http://hadoop.apache.org/\r [apache-hadoop-doc]: http://hadoop.apache.org/docs/current/\r [apache-hive]: http://hive.apache.org/\r [apache-pig]: http://pig.apache.org/\r [getting-started]: documentdb-get-started.md\r \r [azure-portal]: https://portal.azure.cn/\r [azure-powershell-diagram]: ./media/run-hadoop-with-hdinsight/azurepowershell-diagram-med.png\r \r [hdinsight-samples]: http://portalcontent.blob.core.chinacloudapi.cn/samples/documentdb-hdinsight-samples.zip\r [github]: https://github.com/Azure/azure-documentdb-hadoop\r [documentdb-java-application]: documentdb-java-application.md\r [import-data]: import-data.md\r \r [hdinsight-custom-provision]: ../hdinsight/hdinsight-provision-clusters.md\r [hdinsight-develop-deploy-java-mapreduce]: ../hdinsight/hdinsight-develop-deploy-java-mapreduce-linux.md\r [hdinsight-hadoop-customize-cluster]: ../hdinsight/hdinsight-hadoop-customize-cluster.md\r [hdinsight-get-started]: ../hdinsight/hdinsight-hadoop-tutorial-get-started-windows.md\r [hdinsight-storage]: ../hdinsight/hdinsight-hadoop-use-blob-storage.md\r [hdinsight-use-hive]: ../hdinsight/hdinsight-use-hive.md\r [hdinsight-use-mapreduce]: ../hdinsight/hdinsight-use-mapreduce.md\r [hdinsight-use-pig]: ../hdinsight/hdinsight-use-pig.md\r \r [image-customprovision-page1]: ./media/run-hadoop-with-hdinsight/customprovision-page1.png\r [image-hive-query-results]: ./media/run-hadoop-with-hdinsight/hivequeryresults.PNG\r [image-mapreduce-query-results]: ./media/run-hadoop-with-hdinsight/mapreducequeryresults.PNG\r [image-pig-query-results]: ./media/run-hadoop-with-hdinsight/pigqueryresults.PNG\r \r [powershell-install-configure]: https://docs.microsoft.com/powershell/azure/install-azurerm-ps?view=azurermps-4.0.0\r \r <!--Update_Description: wording update-->"}