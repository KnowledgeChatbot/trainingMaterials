{"Title":"在 HDInsight 中的 Hadoop 上将 C# 与 MapReduce 配合使用 - Azure","Description":"了解如何在 Azure HDInsight 中通过 Hadoop 使用 C# 创建 MapReduce 解决方案。","Content":"# <a name=\"use-c-with-mapreduce-streaming-on-hadoop-in-hdinsight\"></a>在 HDInsight 中的 Hadoop 上将 C# 与 MapReduce 流式处理配合使用\r \r [!INCLUDE [azure-sdk-developer-differences](../../../includes/azure-sdk-developer-differences.md)]\r \r 了解如何在 HDInsight 上使用 C# 创建 MapReduce 解决方案。\r \r > [!IMPORTANT]\r > Linux 是 HDInsight 3.4 或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 组件版本控制](../hdinsight-component-versioning.md)。\r \r Hadoop 流式处理是一个实用工具，通过它可以使用脚本或可执行文件运行 MapReduce 作业。 在本示例中，.NET 用于为单词计数解决方案实现映射器和化简器。\r \r ## <a name=\"net-on-hdinsight\"></a>HDInsight 上的 .NET\r \r __基于 Linux 的 HDInsight__ 群集使用 [Mono (https://mono-project.com)](https://mono-project.com) 运行 .NET 应用程序。 Mono 版本 4.2.1 包含在 HDInsight 版本 3.5 中。 有关包含在 HDInsight 中的 Mono 版本的详细信息，请参阅 [HDInsight 组件版本](../hdinsight-component-versioning.md)。 若要使用 Mono 的特定版本，请参阅[安装或更新 Mono](../hdinsight-hadoop-install-mono.md) 文档。\r \r 有关 Mono 与 .NET Framework 版本的兼容性的详细信息，请参阅 [Mono 兼容性](http://www.mono-project.com/docs/about-mono/compatibility/)。\r \r ## <a name=\"how-hadoop-streaming-works\"></a>Hadoop 流式处理的工作原理\r \r 在本文档中用于流式处理的基本流程如下所示：\r \r 1. Hadoop 在 STDIN 上将数据传递到映射器（在本示例中为 mapper.exe）。\r 2. 映射器处理数据，并向 STDOUT 发出制表符分隔的键/值对。\r 3. 该输出由 Hadoop 读取，随后会传递到 STDIN 上的化简器（在本示例中为 reducer.exe）。\r 4. 化简器将读取制表符分隔的键/值对、处理数据，并将结果作为制表符分隔的键/值对在 STDOUT 上发出。\r 5. 该输出由 Hadoop 读取，并写入输出目录。\r \r 有关流式处理的详细信息，请参阅 [Hadoop 流式处理 (https://hadoop.apache.org/docs/r2.7.1/hadoop-streaming/HadoopStreaming.html)](https://hadoop.apache.org/docs/r2.7.1/hadoop-streaming/HadoopStreaming.html)。\r \r ## <a name=\"prerequisites\"></a>先决条件\r \r * 熟悉编写和生成面向 .NET Framework 4.5 的 C# 代码。 本文档中的各个步骤都使用 Visual Studio 2017。\r \r * 将 .exe 文件上传到群集的方法。 本文档中的各个步骤都使用针对 Visual Studio 的 Data Lake 工具将文件上传到群集的主要存储。\r \r * Azure PowerShell 或 SSH 客户端。\r \r * HDInsight 群集上的 Hadoop。 有关创建群集的详细信息，请参阅[创建 HDInsight 群集](../hdinsight-hadoop-provision-linux-clusters.md)。\r \r ## <a name=\"create-the-mapper\"></a>创建映射器\r \r 在 Visual Studio 中，创建名为 __mapper__ 的新__控制台应用程序__。 针对该应用程序使用以下代码：\r \r ```csharp\r using System;\r using System.Text.RegularExpressions;\r \r namespace mapper\r {\r     class Program\r     {\r         static void Main(string[] args)\r         {\r             string line;\r             //Hadoop passes data to the mapper on STDIN\r             while((line = Console.ReadLine()) != null)\r             {\r                 // We only want words, so strip out punctuation, numbers, etc.\r                 var onlyText = Regex.Replace(line, @\"\\.|;|:|,|[0-9]|'\", \"\");\r                 // Split at whitespace.\r                 var words = Regex.Matches(onlyText, @\"[\\w]+\");\r                 // Loop over the words\r                 foreach(var word in words)\r                 {\r                     //Emit tab-delimited key/value pairs.\r                     //In this case, a word and a count of 1.\r                     Console.WriteLine(\"{0}\\t1\",word);\r                 }\r             }\r         }\r     }\r }\r ```\r \r 创建该应用程序后，生成它以在项目目录中生成 `/bin/Debug/mapper.exe` 文件。\r \r ## <a name=\"create-the-reducer\"></a>创建化简器\r \r 在 Visual Studio 中，创建名为 __reducer__ 的新__控制台应用程序__。 针对该应用程序使用以下代码：\r \r ```csharp\r using System;\r using System.Collections.Generic;\r \r namespace reducer\r {\r     class Program\r     {\r         static void Main(string[] args)\r         {\r             //Dictionary for holding a count of words\r             Dictionary<string, int> words = new Dictionary<string, int>();\r \r             string line;\r             //Read from STDIN\r             while ((line = Console.ReadLine()) != null)\r             {\r                 // Data from Hadoop is tab-delimited key/value pairs\r                 var sArr = line.Split('\\t');\r                 // Get the word\r                 string word = sArr[0];\r                 // Get the count\r                 int count = Convert.ToInt32(sArr[1]);\r \r                 //Do we already have a count for the word?\r                 if(words.ContainsKey(word))\r                 {\r                     //If so, increment the count\r                     words[word] += count;\r                 } else\r                 {\r                     //Add the key to the collection\r                     words.Add(word, count);\r                 }\r             }\r             //Finally, emit each word and count\r             foreach (var word in words)\r             {\r                 //Emit tab-delimited key/value pairs.\r                 //In this case, a word and a count of 1.\r                 Console.WriteLine(\"{0}\\t{1}\", word.Key, word.Value);\r             }\r         }\r     }\r }\r ```\r \r 创建该应用程序后，生成它以在项目目录中生成 `/bin/Debug/reducer.exe` 文件。\r \r ## <a name=\"upload-to-storage\"></a>上传到存储\r \r [!INCLUDE [azure-visual-studio-login-guide](../../../includes/azure-visual-studio-login-guide.md)]\r \r 1. 在 Visual Studio 中，打开“服务器资源管理器”。\r \r 2. 依次展开“Azure”和“HDInsight”。\r \r 3. 如果出现提示，请输入 Azure 订阅凭据，并单击“登录”。\r \r 4. 展开要将此应用程序部署到的 HDInsight 群集。 列出带有文本“（默认存储帐户）”的条目。\r \r     ![显示群集存储帐户的服务器资源管理器](./media/apache-hadoop-dotnet-csharp-mapreduce-streaming/storage.png)\r \r     * 如果此条目可以展开，则在使用 __Azure 存储帐户__作为该群集的默认存储。 如果要查看该群集的默认存储上的文件，请展开该条目，并双击“（默认容器）”。\r \r 5. 若要上传 .exe 文件，请使用以下方法之一：\r \r     单击上传图标，并浏览到 **mapper** 项目的 **bin\\debug** 文件夹。 最后，选择“mapper.exe”文件，并单击“确定”。\r \r         ![upload icon](./media/apache-hadoop-dotnet-csharp-mapreduce-streaming/upload.png)\r \r     上传“mapper.exe”完成后，请为“reducer.exe”文件重复该上传过程。\r \r ## <a name=\"run-a-job-using-an-ssh-session\"></a>运行作业：使用 SSH 会话\r \r 1. 使用 SSH 连接到 HDInsight 群集。 有关详细信息，请参阅 [Use SSH with HDInsight](../hdinsight-hadoop-linux-use-ssh-unix.md)（对 HDInsight 使用 SSH）。\r \r 2. 使用以下命令之一启动 MapReduce 作业：\r \r     使用 __Azure 存储__作为默认存储：\r \r     ```bash\r     yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -files wasb:///mapper.exe,wasb:///reducer.exe -mapper mapper.exe -reducer reducer.exe -input /example/data/gutenberg/davinci.txt -output /example/wordcountout\r     ```\r \r     下表描述每个参数的作用：\r \r     * `hadoop-streaming.jar`：包含流式处理 MapReduce 功能的 jar 文件。\r     * `-files`：将 `mapper.exe` 和 `reducer.exe` 文件添加到此作业。 每个文件前的 `adl:///` 或 `wasb:///` 是指向群集的默认存储根目录的路径。\r     * `-mapper`：指定哪个文件实现映射器。\r     * `-reducer`：指定哪个文件实现化简器。\r     * `-input`：输入数据。\r     * `-output`：输出目录。\r \r 3. 完成 MapReduce 作业后，使用以下命令查看结果：\r \r     ```bash\r     hdfs dfs -text /example/wordcountout/part-00000\r     ```\r \r     以下文本是此命令返回的数据的示例：\r \r         you     1128\r         young   38\r         younger 1\r         youngest        1\r         your    338\r         yours   4\r         yourself        34\r         yourselves      3\r         youth   17\r \r ## <a name=\"run-a-job-using-powershell\"></a>运行作业：使用 PowerShell\r \r 使用以下 PowerShell 脚本运行 MapReduce 作业，并下载结果。\r \r ```powershell\r # Login to your Azure subscription\r # Is there an active Azure subscription?\r $sub = Get-AzureRmSubscription -ErrorAction SilentlyContinue\r if(-not($sub))\r {\r     Add-AzureRmAccount -EnvironmentName AzureChinaCloud\r }\r \r # Get HDInsight info\r $clusterName = Read-Host -Prompt \"Enter the HDInsight cluster name\"\r $creds=Get-Credential -Message \"Enter the login for the cluster\"\r \r # Path for job output\r $outputPath=\"/example/wordcountoutput\"\r \r # Progress indicator\r $activity=\"C# MapReduce example\"\r Write-Progress -Activity $activity -Status \"Getting cluster information...\"\r #Get HDInsight info so we can get the resource group, storage, etc.\r $clusterInfo = Get-AzureRmHDInsightCluster -ClusterName $clusterName\r $resourceGroup = $clusterInfo.ResourceGroup\r $storageActArr=$clusterInfo.DefaultStorageAccount.split('.')\r $storageAccountName=$storageActArr[0]\r $storageType=$storageActArr[1]\r \r # Progress indicator\r #Define the MapReduce job\r # Note: using \"/mapper.exe\" and \"/reducer.exe\" looks in the root\r #       of default storage.\r $jobDef=New-AzureRmHDInsightStreamingMapReduceJobDefinition `\r     -Files \"/mapper.exe\",\"/reducer.exe\" `\r     -Mapper \"mapper.exe\" `\r     -Reducer \"reducer.exe\" `\r     -InputPath \"/example/data/gutenberg/davinci.txt\" `\r     -OutputPath $outputPath\r \r # Start the job\r Write-Progress -Activity $activity -Status \"Starting MapReduce job...\"\r $job=Start-AzureRmHDInsightJob `\r     -ClusterName $clusterName `\r     -JobDefinition $jobDef `\r     -HttpCredential $creds\r \r #Wait for the job to complete\r Write-Progress -Activity $activity -Status \"Waiting for the job to complete...\"\r Wait-AzureRmHDInsightJob `\r     -ClusterName $clusterName `\r     -JobId $job.JobId `\r     -HttpCredential $creds\r \r Write-Progress -Activity $activity -Completed\r \r # Download the output \r # Azure Storage account\r # Get the container\r $container=$clusterInfo.DefaultStorageContainer\r #NOTE: This assumes that the storage account is in the same resource\r #      group as HDInsight. If it is not, change the\r #      --ResourceGroupName parameter to the group that contains storage.\r $storageAccountKey=(Get-AzureRmStorageAccountKey `\r     -Name $storageAccountName `\r     -ResourceGroupName $resourceGroup)[0].Value\r \r #Create a storage context\r $context = New-AzureStorageContext `\r     -StorageAccountName $storageAccountName `\r     -StorageAccountKey $storageAccountKey\r # Download the file\r Get-AzureStorageBlobContent `\r     -Blob 'example/data/WordCountOutput/part-r-00000' `\r     -Container $container `\r     -Destination output.txt `\r     -Context $context\r ```\r \r 此脚本会提示用户提供群集登录的帐户名和密码，以及 HDInsight 群集名称。 作业完成后，输出会下载到名为 `output.txt` 的文件中。 以下文本是 `output.txt` 文件中数据的示例：\r \r     you     1128\r     young   38\r     younger 1\r     youngest        1\r     your    338\r     yours   4\r     yourself        34\r     yourselves      3\r     youth   17\r \r ## <a name=\"next-steps\"></a>后续步骤\r \r 有关将 MapReduce 与 HDInsight 配合使用的详细信息，请参阅[将 MapReduce 与 HDInsight 配合使用](hdinsight-use-mapreduce.md)。\r \r 有关将 C# 与 Hive 和 Pig 配合使用的信息，请参阅[将 C# 用户定义函数与 Hive 和 Pig 配合使用](apache-hadoop-hive-pig-udf-dotnet-csharp.md)。\r \r 有关在 HDInsight 上将 C# 与 Storm 配合使用的信息，请参阅[为 HDInsight 上的 Storm 开发 C# 拓扑](../storm/apache-storm-develop-csharp-visual-studio-topology.md)。\r \r \r <!--Update_Description: update wording and link references-->"}