{"Title":"Apache Kafka 入门 - Azure HDInsight","Description":"了解如何在 Azure HDInsight 上创建 Apache Kafka 群集。 了解如何创建主题、订阅服务器和使用者。","Content":"# <a name=\"start-with-apache-kafka-on-hdinsight\"></a>Apache Kafka on HDInsight 入门\r \r 了解如何在 Azure HDInsight 上创建和使用 [Apache Kafka](https://kafka.apache.org) 群集。 Kafka 是开源分布式流式处理平台，可与 HDInsight 配合使用。 通常将其用作消息代理，因为它可提供类似于发布-订阅消息队列的功能。\r \r > [!NOTE]\r > 目前，有两个 Kafka 版本可用于 HDInsight；0.9.0 (HDInsight 3.4) 和 0.10.0（HDInsight 3.5 和 3.6）。 本文档中的步骤假设使用 Kafka on HDInsight 3.6。\r \r [!INCLUDE [delete-cluster-warning](../../../includes/hdinsight-delete-cluster-warning.md)]\r \r ## <a name=\"create-a-kafka-cluster\"></a>创建 Kafka 群集\r \r 使用以下步骤创建 Kafka on HDInsight 群集：\r \r 1. 从 [Azure 门户](https://portal.azure.cn)，选择“+ 新建”**、“智能 + 分析”**，并选择“HDInsight”。\r \r     ![创建 HDInsight 群集](./media/apache-kafka-get-started/create-hdinsight.png)\r \r 2. 在“基本信息”中输入以下信息：\r \r     * **群集名称**：HDInsight 群集的名称。\r     * **订阅**：选择要使用的订阅。\r     * **群集登录用户名**和**群集登录密码**：通过 HTTPS 访问群集时的登录凭据。 可以使用这些凭据访问 Ambari Web UI 或 REST API 等服务。\r     * **安全外壳 (SSH) 用户名**：通过 SSH 访问群集时使用的登录名。 默认情况下，密码与群集登录密码相同。\r     * **资源组**：要在其中创建群集的资源组。\r     * **位置**：要在其中创建群集的 Azure 区域。\r \r         > [!IMPORTANT]\r         > 为实现数据的高可用性，我们建议选择包含__三个容错域__的位置（区域）。 有关详细信息，请参阅[数据高可用性](#data-high-availability)部分。\r    \r  ![选择订阅](./media/apache-kafka-get-started/hdinsight-basic-configuration.png)\r \r 3. 选择“群集类型”，然后在“群集配置”中设置以下值：\r    \r     * **群集类型**：Kafka\r \r     * 版本：Kafka 0.10.0 (HDI 3.6)\r \r     * **群集层**：标准\r      \r     最后使用“选择”按钮保存设置。\r      \r  ![选择群集类型](./media/apache-kafka-get-started/set-hdinsight-cluster-type.png)\r \r 4. 选择群集类型后，请使用“选择”  按钮设置群集类型。 接下来，使用“下一步”  按钮完成基本配置。\r \r 5. 在“存储”中选择或创建存储帐户。 对于本文档中的步骤，请让其他字段保留默认值。 使用“下一步”  按钮保存存储配置。\r \r     ![设置 HDInsight 的存储帐户设置](./media/apache-kafka-get-started/set-hdinsight-storage-account.png)\r \r 6. 在“应用程序(可选)”中，选择“下一步”继续。 此示例不需要任何应用程序。\r \r 7. 在“群集大小”中，选择“下一步”继续。\r \r     > [!WARNING]\r     > 若要确保 Kafka on HDInsight 的可用性，群集必须至少包含 3 个辅助节点。 有关详细信息，请参阅[数据高可用性](#data-high-availability)部分。\r \r     ![设置 Kafka 群集大小](./media/apache-kafka-get-started/kafka-cluster-size.png)\r \r     > [!IMPORTANT]\r     > “每个辅助角色节点的磁盘数”条目控制 Kafka on HDInsight 的可伸缩性。 Kafka on HDInsight 在群集中使用虚拟机的本地磁盘。 由于 Kafka 的 I/O 很高，因此会使用 [Azure 托管磁盘](../../virtual-machines/windows/managed-disks-overview.md)提供高吞吐量，并为每个节点提供更多存储。 托管磁盘的类型可以为“标准”(HDD) 或“高级”(SSD)。 高级磁盘可与 DS 系列 VM 一起使用。 所有其他的 VM 类型使用“标准”。\r \r 8. 在“高级设置”中，选择“下一步”继续。\r \r 9. 在“摘要”中，查看群集的配置。 使用“编辑”链接更改不正确的设置。 最后，使用“创建”按钮创建群集。\r    \r     ![群集配置摘要](./media/apache-kafka-get-started/hdinsight-configuration-summary.png)\r    \r     > [!NOTE]\r     > 创建群集可能需要 20 分钟。\r \r ## <a name=\"connect-to-the-cluster\"></a>连接至群集\r \r > [!IMPORTANT]\r > 执行以下步骤时，必须使用 SSH 客户端。 有关详细信息，请参阅[将 SSH 与 HDInsight 配合使用](../hdinsight-hadoop-linux-use-ssh-unix.md)文档。\r \r 使用 SSH 从客户端连接到群集：\r \r ```ssh SSHUSER@CLUSTERNAME-ssh.azurehdinsight.cn```\r \r 将 SSHUSER 替换为在群集创建期间提供的 SSH 用户名。 将 **CLUSTERNAME** 替换为群集的名称。\r \r 出现提示时，请输入 SSH 帐户使用的密码。\r \r 有关信息，请参阅[将 SSH 与 HDInsight 配合使用](../hdinsight-hadoop-linux-use-ssh-unix.md)。\r \r ## <a id=\"getkafkainfo\"></a>获取 Zookeeper 和中转站主机信息\r \r 使用 Kafka 时，必须知道两个主机值；*Zookeeper* 主机和*中转站*主机。 Kafka API 以及 Kafka 随附的许多实用工具都使用这些主机。\r \r 使用以下步骤创建包含主机信息的环境变量。 这些环境变量在本文档的步骤中使用。\r \r 1. 与群集建立 SSH 连接后，使用以下命令安装 `jq` 实用工具。 此实用工具用于分析 JSON 文档且有助于检索代理主机的信息：\r \r     ```bash\r     sudo apt -y install jq\r     ```\r \r 2. 若要使用从 Ambari 检索的信息设置环境变量，请执行以下命令：\r \r     ```bash\r     CLUSTERNAME='your cluster name'\r     PASSWORD='your cluster password'\r     export KAFKAZKHOSTS=`curl -sS -u admin:$PASSWORD -G https://$CLUSTERNAME.azurehdinsight.cn/api/v1/clusters/$CLUSTERNAME/services/ZOOKEEPER/components/ZOOKEEPER_SERVER | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2`\r \r     export KAFKABROKERS=`curl -sS -u admin:$PASSWORD -G https://$CLUSTERNAME.azurehdinsight.cn/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2`\r \r     echo '$KAFKAZKHOSTS='$KAFKAZKHOSTS\r     echo '$KAFKABROKERS='$KAFKABROKERS\r     ```\r \r     > [!IMPORTANT]\r     > 将 `CLUSTERNAME=` 设置为 Kafka 群集的名称。 将 `PASSWORD=` 设置为在创建群集时使用的登录（管理员）密码。\r \r     以下文本是 `$KAFKAZKHOSTS`的内容示例：\r    \r     `zk0-kafka.eahjefxxp1netdbyklgqj5y1ud.ex.internal.chinacloudapp.cn:2181,zk2-kafka.eahjefxxp1netdbyklgqj5y1ud.ex.internal.chinacloudapp.cn:2181`\r    \r     以下文本是 `$KAFKABROKERS`的内容示例：\r    \r     `wn1-kafka.eahjefxxp1netdbyklgqj5y1ud.cx.internal.chinacloudapp.cn:9092,wn0-kafka.eahjefxxp1netdbyklgqj5y1ud.cx.internal.chinacloudapp.cn:9092`\r \r     > [!NOTE]\r     > `cut` 命令用于将主机列表剪裁为两个主机条目。 创建 Kafka 使用者或生成者时，不需提供主机的完整列表。\r    \r     > [!WARNING]\r     > 不要期望从此会话中返回的信息始终准确。 如果缩放群集，会相应地新增或删除中转站。 如果发生故障或更换了节点，节点的主机名可能会更改。\r     >\r     > 应在检索 Zookeeper 和中转站主机信息后尽快使用这些信息，确保信息有效。\r \r ## <a name=\"create-a-topic\"></a>创建主题\r \r Kafka 将数据流存储在名为*主题*的类别中。 在与群集头节点建立 SSH 连接后，使用 Kafka 随附的脚本创建主题：\r \r ```bash\r /usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic test --zookeeper $KAFKAZKHOSTS\r ```\r \r 此命令使用 `$KAFKAZKHOSTS`中存储的主机信息连接到 Zookeeper，并创建名为 **test**的 Kafka 主题。 可以通过使用以下脚本列出主题，来验证是否已创建该 test 主题：\r \r ```bash\r /usr/hdp/current/kafka-broker/bin/kafka-topics.sh --list --zookeeper $KAFKAZKHOSTS\r ```\r \r 此命令的输出列出 Kafka 主题，其中包含 **test** 主题。\r \r ## <a name=\"produce-and-consume-records\"></a>生成和使用记录\r \r Kafka 将记录存储在主题中。 记录由*生成者*生成，由*使用者*使用。 生成者从 Kafka *中转站*检索记录。 HDInsight 群集中的每个辅助角色节点都是一个 Kafka 中转站。\r \r 使用以下步骤将记录存储到之前创建的测试主题，并通过使用者对其进行读取：\r \r 1. 从 SSH 会话，使用随 Kafka 提供的脚本将记录写入主题：\r \r     ```bash\r     /usr/hdp/current/kafka-broker/bin/kafka-console-producer.sh --broker-list $KAFKABROKERS --topic test\r     ```\r    \r     完成此命令后，不会返回到提示窗口。 而是键入一些文本消息，并使用 **Ctrl + C** 停止发送到主题。 每行应作为单独的记录发送。\r \r 2. 使用随 Kafka 提供的脚本从主题中读取记录：\r \r     ```bash\r     /usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --bootstrap-server $KAFKABROKERS --topic test --from-beginning\r     ```\r \r     此命令从主题中检索并显示记录。 使用 `--from-beginning` 告知使用者从流的开头开始，以检索所有记录。\r \r 3. 使用 __Ctrl + C__ 停止使用者。\r \r ## <a name=\"producer-and-consumer-api\"></a>生成者和使用者 API\r \r 还可使用 [Kafka API](http://kafka.apache.org/documentation#api) 以编程方式生成和使用记录。 若要构建 Java 生成者和使用者，请在开发环境中执行以下步骤。\r \r > [!IMPORTANT]\r > 必须在开发环境中安装以下组件：\r >\r > * [Java JDK 8](http://www.oracle.com/technetwork/java/javase/downloads/index.html) 或类似程序，如 OpenJDK。\r >\r > * [Apache Maven](http://maven.apache.org/)\r >\r > * SSH 客户端和 `scp` 命令。 有关详细信息，请参阅[将 SSH 与 HDInsight 配合使用](../hdinsight-hadoop-linux-use-ssh-unix.md)文档。\r \r 1. 从 [https://github.com/Azure-Samples/hdinsight-kafka-java-get-started](https://github.com/Azure-Samples/hdinsight-kafka-java-get-started)下载示例。 对于生成者/使用者示例，请使用 `Producer-Consumer` 目录中的项目。 本示例包含以下类：\r    \r     * **运行** - 启动使用者或生成者。\r \r     * **生成者** - 将 1,000,000 个记录存储到主题中。\r \r     * **使用者** - 从主题中读取记录。\r \r 2. 若要创建 jar 包，请将目录更改为 `Producer-Consumer` 目录的位置，然后执行以下命令：\r \r     ```\r     mvn clean package\r     ```\r \r     此命令创建一个名为 `target` 的目录，其中包含名为 `kafka-producer-consumer-1.0-SNAPSHOT.jar` 的文件。\r \r 3. 使用以下命令将 `kafka-producer-consumer-1.0-SNAPSHOT.jar` 文件复制到 HDInsight 群集：\r    \r     ```bash\r     scp ./target/kafka-producer-consumer-1.0-SNAPSHOT.jar SSHUSER@CLUSTERNAME-ssh.azurehdinsight.cn:kafka-producer-consumer.jar\r     ```\r \r     将 **SSHUSER** 替换为群集的 SSH 用户，并将 **CLUSTERNAME** 替换为群集的名称。 出现提示时，请输入 SSH 用户密码。\r \r 4. 等 `scp` 命令复制完文件以后，使用 SSH 连接到群集。 使用以下命令将记录写入测试主题：\r \r     ```bash\r     java -jar kafka-producer-consumer.jar producer $KAFKABROKERS\r     ```\r \r 5. 完成该过程后，使用以下命令从主题中读取：\r \r     ```bash\r     java -jar kafka-producer-consumer.jar consumer $KAFKABROKERS\r     ```\r \r     将显示读取的记录以及记录计数。 可以看到，记录的条目略超过 1,000,000 条，因为在前面的步骤中使用脚本向主题发送了数条记录。\r \r 6. 使用 __Ctrl + C__ 退出使用者。\r \r ### <a name=\"multiple-consumers\"></a>多个使用者\r \r 在读取记录时，Kafka 使用者使用使用者组。 让多个使用者使用同一个组会导致对主题的读取进行负载均衡操作。 组中的每个使用者都会接收一部分记录。 若要在实际操作中了解此过程，请使用以下步骤：\r \r 1. 向群集打开新 SSH 会话，从而具有两个会话。 在每个会话中，使用以下命令通过同一使用者组 ID 启动使用者：\r \r     ```bash\r     java -jar kafka-producer-consumer.jar consumer $KAFKABROKERS mygroup\r     ```\r \r     此命令使用组 ID `mygroup` 启动使用者。\r \r     > [!NOTE]\r     > 使用[获取 Zookeeper 和中转站主机信息](#getkafkainfo)部分的命令为该 SSH 会话设置 `$KAFKABROKERS`。\r \r 2. 观察每个会话对从主题中收到的记录进行计数。 两个会话的总数应与前面从一个使用者收到的数目相同。\r \r 同一个组中客户端的使用方式由主题的分区处理。 前面创建的 `test` 主题有 8 个分区。 若打开 8 个 SSH 会话，并在所有会话中启动一个使用者，每个使用者都将从主题的单个分区中读取记录。\r \r > [!IMPORTANT]\r > 使用者组中存在的使用者实例不能比分区多。 此示例中，一个使用者组最多可包含八个使用者，因为这是本主题中的分区数。 也可拥有多个使用者组，每个组的使用者不能超过八个。\r \r 存储在 Kafka 中的记录都按在分区中接收的顺序进行存储。 若要 *在分区中*实现有序的记录传送，可以创建使用者实例数与分区数相匹配的使用者组。 若要 *在主题中*实现有序的记录传送，可以创建仅包含一个使用者实例的使用者组。\r \r ## <a name=\"streaming-api\"></a>流式处理 API\r \r 流式处理 API 已添加到 Kafka 版本 0.10.0；早期版本依赖于 Apache Spark 或 Storm 进行流式处理。\r \r 1. 如果尚未执行此操作，请从 [https://github.com/Azure-Samples/hdinsight-kafka-java-get-started](https://github.com/Azure-Samples/hdinsight-kafka-java-get-started) 将示例下载到开发环境。 对于流式处理示例，请使用 `streaming` 目录中的项目。\r \r     此项目只包含一个类 (`Stream`)，该类从前面创建的 `test` 主题读取记录。 它会统计读取的字数，并发出每个字，计数到名为 `wordcounts` 的主题。 本部分后面的步骤会创建 `wordcounts` 主题。\r \r 2. 从开发环境中的命令行，将目录更改为 `Streaming` 目录的位置，然后使用以下命令创建 jar 包：\r \r     ```bash\r     mvn clean package\r     ```\r \r     此命令创建名为 `target` 的目录，其中包含名为 `kafka-streaming-1.0-SNAPSHOT.jar` 的文件。\r \r 3. 使用以下命令将 `kafka-streaming-1.0-SNAPSHOT.jar` 文件复制到 HDInsight 群集：\r    \r     ```bash\r     scp ./target/kafka-streaming-1.0-SNAPSHOT.jar SSHUSER@CLUSTERNAME-ssh.azurehdinsight.cn:kafka-streaming.jar\r     ```\r    \r     将 **SSHUSER** 替换为群集的 SSH 用户，并将 **CLUSTERNAME** 替换为群集的名称。 出现提示时，请输入 SSH 用户的密码。\r \r 4. `scp` 命令完成文件复制后，请使用 SSH 连接到群集，然后使用以下命令创建 `wordcounts` 主题：\r \r     ```bash\r     /usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic wordcounts --zookeeper $KAFKAZKHOSTS\r     ```\r \r 5. 接下来，使用以下命令启动流式处理：\r    \r     ```bash\r     java -jar kafka-streaming.jar $KAFKABROKERS $KAFKAZKHOSTS 2>/dev/null &\r     ```\r    \r     此命令在后台启动流式处理。\r \r 6. 使用以下命令将消息发送到 `test` 主题。 这些消息由流式处理示例进行处理：\r \r     ```bash\r     java -jar kafka-producer-consumer.jar producer $KAFKABROKERS &>/dev/null &\r     ```\r \r 7. 使用以下命令查看通过流式处理写入到 `wordcounts` 主题的输出：\r    \r     ```bash\r     /usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --bootstrap-server $KAFKABROKERS --topic wordcounts --from-beginning --formatter kafka.tools.DefaultMessageFormatter --property print.key=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer\r     ```\r    \r     > [!NOTE]\r     > 若要查看数据，必须告诉使用者要列显键以及用于键和值的反序列化程序。 键名称是文字，键值包含计数。\r    \r     输出类似于以下文本：\r    \r         dwarfs  13635\r         ago     13664\r         snow    13636\r         dwarfs  13636\r         ago     13665\r         a       13803\r         ago     13666\r         a       13804\r         ago     13667\r         ago     13668\r         jumped  13640\r         jumped  13641\r         a       13805\r         snow    13637\r    \r     > [!NOTE]\r     > 每遇到一个单词，此计数就会递增。\r \r 7. 使用 __Ctrl + C__ 让使用者退出，然后使用 `fg` 命令将流式处理后台任务恢复到前台。 另外，请使用 __Ctrl + C__ 退出操作。\r \r ## <a name=\"data-high-availability\"></a>数据高可用性\r \r 每个 Azure 区域（位置）均提供_容错域_。 容错域是 Azure 数据中心基础硬件的逻辑分组。 每个容错域共享公用电源和网络交换机。 在 HDInsight 群集中实现节点的虚拟机和托管磁盘跨这些容错域分布。 此体系结构可限制物理硬件故障造成的潜在影响。\r \r 有关区域中容错域数的信息，请参阅 [Linux 虚拟机的可用性](../../virtual-machines/windows/manage-availability.md#use-managed-disks-for-vms-in-an-availability-set)文档。\r \r > [!IMPORTANT]\r > 建议使用包含三个容错域的 Azure 区域，并使用 3 作为复制因子。\r \r 如果必须使用只包含两个容错域的区域，请使用 4 作为复制因子，将副本均衡地分布到两个容错域中。\r \r ### <a name=\"kafka-and-fault-domains\"></a>Kafka 和容错域\r \r Kafka 不识别容错域。 在创建主题的分区副本时，它可能未针对高可用性正确分发副本。 若要确保高可用性，请使用 [Kafka 分区重新均衡工具](https://github.com/hdinsight/hdinsight-kafka-tools)。 必须通过 SSH 会话运行此工具，以便连接到 Kafka 群集的头节点。\r \r 若要确保 Kafka 数据的最高可用性，应该在以下时间为主题重新均衡分区副本：\r \r * 创建新主题或分区时\r \r * 纵向扩展群集时\r \r ## <a name=\"delete-the-cluster\"></a>删除群集\r \r [!INCLUDE [delete-cluster-warning](../../../includes/hdinsight-delete-cluster-warning.md)]\r \r ## <a name=\"troubleshoot\"></a>故障排除\r \r 如果在创建 HDInsight 群集时遇到问题，请参阅[访问控制要求](../hdinsight-administer-use-portal-linux.md#create-clusters)。\r \r ## <a name=\"next-steps\"></a>后续步骤\r \r 本文档已介绍有关使用 Apache Kafka on HDInsight 的基础知识。 请参阅以下资源了解有关使用 Kafka 的详细信息：\r \r * [在 Kafka 群集之间复制数据](apache-kafka-mirroring.md)\r * [将 Apache Spark 流式处理 (DStream) 与 Kafka on HDInsight 配合使用](../hdinsight-apache-spark-with-kafka.md)\r * [将 Apache Spark 结构化流式处理与 Kafka on HDInsight 配合使用](../hdinsight-apache-kafka-spark-structured-streaming.md)\r * [将 Apache Storm 与 Kafka on HDInsight 结合使用](../hdinsight-apache-storm-with-kafka.md)\r * [通过 Azure 虚拟网络连接到 Kafka](apache-kafka-connect-vpn-gateway.md)\r \r \r <!--Update_Description: update wording and link references-->\r "}