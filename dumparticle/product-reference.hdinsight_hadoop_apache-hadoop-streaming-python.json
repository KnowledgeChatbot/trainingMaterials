{"Title":"使用 HDInsight 开发 Python 流式处理 MapReduce 作业 - Azure","Description":"了解如何在流式处理 MapReduce 作业中使用 Python。 Hadoop 为 MapReduce 提供了流式处理 API，以便用 Java 以外的语言编写。","Content":"# <a name=\"develop-python-streaming-mapreduce-programs-for-hdinsight\"></a>为 HDInsight 开发 Python 流式处理 MapReduce 程序\r \r 了解如何在流式处理 MapReduce 操作中使用 Python。 Hadoop 为 MapReduce 提供了一个流式处理 API，这样，除 Java 外，还能使用其他语言编写映射和化简函数。 本文档中的步骤实现 Python 中的映射和化简组件。\r \r ## <a name=\"prerequisites\"></a>先决条件\r \r * 基于 Linux 的 HDInsight 上的 Hadoop 群集\r \r   > [!IMPORTANT]\r   > 本文档中的步骤需要使用 Linux 的 HDInsight 群集。 Linux 是在 HDInsight 3.4 版或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 在 Windows 上停用](../hdinsight-component-versioning.md#hdinsight-windows-retirement)。\r \r * 文本编辑器\r \r   > [!IMPORTANT]\r   > 文本编辑器必须使用 LF 作为行尾。 在基于 Linux 的 HDInsight 群集上运行 MapReduce 作业时，使用 CRLF 的行尾会导致出错。\r \r * `ssh` 和 `scp` 命令，或 [Azure PowerShell](https://docs.microsoft.com/powershell/azure/overview?view=azurermps-3.8.0)\r \r ## <a name=\"word-count\"></a>字数统计\r \r 本示例是使用 python 中的映射器和化简器实现的基本字数统计。 映射器会将句子分解成不同的单词，而化简器会汇总单词并统计字数以生成输出。\r \r 下图演示了在映射和化简阶段发生的情况。\r \r ![mapreduce 进程的说明](./media/apache-hadoop-streaming-python/HDI.WordCountDiagram.png)\r \r ## <a name=\"streaming-mapreduce\"></a>流式处理 MapReduce\r \r Hadoop 允许指定包含作业所用映射和化简逻辑的文件。 映射和化简逻辑的具体要求如下：\r \r * **输入**：映射和化简组件必须从 STDIN 读取输入数据。\r * **输出**：映射和化简组件必须将输出数据写入到 STDOUT。\r * **数据格式**：使用和生成的数据必须是键/值对，并以制表符分隔。\r \r Python 可以使用 `sys` 模块从 STDIN 读取数据，并使用 `print` 输出到 STDOUT，从而轻松应对这些要求。 余下的任务就是在键和值之间以制表符 (`\\t`) 设置数据的格式。\r \r ## <a name=\"create-the-mapper-and-reducer\"></a>创建映射器和化简器\r \r 1. 创建名为 `mapper.py` 的文件并使用以下代码作为内容：\r \r    ```python\r    #!/usr/bin/env python\r \r    # Use the sys module\r    import sys\r \r    # 'file' in this case is STDIN\r    def read_input(file):\r        # Split each line into words\r        for line in file:\r            yield line.split()\r \r    def main(separator='\\t'):\r        # Read the data using read_input\r        data = read_input(sys.stdin)\r        # Process each word returned from read_input\r        for words in data:\r            # Process each word\r            for word in words:\r                # Write to STDOUT\r                print '%s%s%d' % (word, separator, 1)\r \r    if __name__ == \"__main__\":\r        main()\r    ```\r \r 2. 创建名为“reducer.py”的文件并使用以下代码作为内容：\r \r    ```python\r    #!/usr/bin/env python\r \r    # import modules\r    from itertools import groupby\r    from operator import itemgetter\r    import sys\r \r    # 'file' in this case is STDIN\r    def read_mapper_output(file, separator='\\t'):\r        # Go through each line\r        for line in file:\r            # Strip out the separator character\r            yield line.rstrip().split(separator, 1)\r \r    def main(separator='\\t'):\r        # Read the data using read_mapper_output\r        data = read_mapper_output(sys.stdin, separator=separator)\r        # Group words and counts into 'group'\r        #   Since MapReduce is a distributed process, each word\r        #   may have multiple counts. 'group' will have all counts\r        #   which can be retrieved using the word as the key.\r        for current_word, group in groupby(data, itemgetter(0)):\r            try:\r                # For each word, pull the count(s) for the word\r                #   from 'group' and create a total count\r                total_count = sum(int(count) for current_word, count in group)\r                # Write to stdout\r                print \"%s%s%d\" % (current_word, separator, total_count)\r            except ValueError:\r                # Count was not a number, so do nothing\r                pass\r \r    if __name__ == \"__main__\":\r        main()\r    ```\r \r ## <a name=\"run-using-powershell\"></a>使用 PowerShell 运行\r \r 若要确保文件具有适当的行尾，请使用以下 PowerShell 脚本：\r \r ```powershell\r # Set $original_file to the python file path\r $text = [IO.File]::ReadAllText($original_file) -replace \"`r`n\", \"`n\"\r [IO.File]::WriteAllText($original_file, $text)\r ```\r \r 使用以下 PowerShell 脚本上传文件、运行作业以及查看输出：\r \r ```powershell\r # Login to your Azure subscription\r # Is there an active Azure subscription?\r $sub = Get-AzureRmSubscription -ErrorAction SilentlyContinue\r if(-not($sub))\r {\r     Add-AzureRmAccount -EnvironmentName AzureChinaCloud\r }\r \r # Get cluster info\r $clusterName = Read-Host -Prompt \"Enter the HDInsight cluster name\"\r # Get the login (HTTPS) credentials for the cluster\r $creds=Get-Credential -Message \"Enter the login for the cluster\" -UserName \"admin\"\r $clusterInfo = Get-AzureRmHDInsightCluster -ClusterName $clusterName\r $storageInfo = $clusterInfo.DefaultStorageAccount.split('.')\r $defaultStoreageType = $storageInfo[1]\r $defaultStorageName = $storageInfo[0]\r \r # Progress indicator\r $activity=\"Python MapReduce\"\r Write-Progress -Activity $activity -Status \"Uploading mapper and reducer...\"\r \r # Upload the files\r switch ($defaultStoreageType)\r {\r     \"blob\" {\r         # Get the blob storage information for the cluster\r         $resourceGroup = $clusterInfo.ResourceGroup\r         $storageContainer=$clusterInfo.DefaultStorageContainer\r         $storageAccountKey=(Get-AzureRmStorageAccountKey `\r             -Name $defaultStorageName `\r             -ResourceGroupName $resourceGroup)[0].Value\r         # Create a storage context and upload the file\r         $context = New-AzureStorageContext `\r             -StorageAccountName $defaultStorageName `\r             -StorageAccountKey $storageAccountKey\r         # Upload the mapper.py file\r         Set-AzureStorageBlobContent `\r             -File .\\mapper.py `\r             -Blob \"mapper.py\" `\r             -Container $storageContainer `\r             -Context $context\r         # Upload the reducer.py file\r         Set-AzureStorageBlobContent `\r             -File .\\reducer.py `\r             -Blob \"reducer.py\" `\r             -Container $storageContainer `\r             -Context $context `\r     }\r     default {\r         Throw \"Unknown storage type: $defaultStoreageType\"\r     }\r }\r \r # Create the streaming job definition\r # Note: This assumes that the mapper.py and reducer.py\r #       are in the root of default storage. If you put them in a\r #       subdirectory, change the -Files parameter to the correct path.\r $jobDefinition = New-AzureRmHDInsightStreamingMapReduceJobDefinition `\r     -Files \"/mapper.py\", \"/reducer.py\" `\r     -Mapper \"mapper.py\" `\r     -Reducer \"reducer.py\" `\r     -InputPath \"/example/data/gutenberg/davinci.txt\" `\r     -OutputPath \"/example/wordcountout\"\r \r # Start the job\r Write-Progress -Activity $activity -Status \"Starting the MapReduce job...\"\r $job = Start-AzureRmHDInsightJob `\r     -ClusterName $clusterName `\r     -JobDefinition $jobDefinition `\r     -HttpCredential $creds\r \r # Wait for the job to complete\r Write-Progress -Activity $activity -Status \"Waiting for the job to complete...\"\r Wait-AzureRmHDInsightJob `\r     -JobId $job.JobId `\r     -ClusterName $clusterName `\r     -HttpCredential $creds\r \r # Display the results of the job\r Write-Progress -Activity $activity -Status \"Downloading job output...\"\r switch ($defaultStoreageType)\r {\r     \"blob\" {\r         # Get the blob storage information for the cluster\r         $resourceGroup = $clusterInfo.ResourceGroup\r         $storageContainer=$clusterInfo.DefaultStorageContainer\r         $storageAccountKey=(Get-AzureRmStorageAccountKey `\r             -Name $defaultStorageName `\r             -ResourceGroupName $resourceGroup)[0].Value\r         # Create a storage context and download the file\r         $context = New-AzureStorageContext `\r             -StorageAccountName $defaultStorageName `\r             -StorageAccountKey $storageAccountKey\r         # Download the file\r         Get-AzureStorageBlobContent `\r             -Container $storageContainer `\r             -Blob \"example/wordcountout/part-00000\" `\r             -Context $context `\r             -Destination \"./output.txt\"\r         # Display the output\r         Get-Content \"./output.txt\"\r     }\r     default {\r         Throw \"Unknown storage type: $defaultStoreageType\"\r     }\r }\r ```\r \r ## <a name=\"run-from-an-ssh-session\"></a>从 SSH 会话运行\r \r 1. 在开发环境中，在与 `mapper.py` 和 `reducer.py` 文件相同的目录中，使用以下命令：\r \r     ```bash\r     scp mapper.py reducer.py username@clustername-ssh.azurehdinsight.cn:\r     ```\r \r     将 `username` 替换为群集的 SSH 用户名，并将 `clustername` 替换为群集的名称。\r \r     此命令会将两个文件从本地系统复制到头节点。\r \r     > [!NOTE]\r     > 如果使用了密码来保护 SSH 帐户，系统会提示输入密码。 如果使用了 SSH 密钥，可能必须使用 `-i` 参数和私钥的路径。 例如，`scp -i /path/to/private/key mapper.py reducer.py username@clustername-ssh.azurehdinsight.cn:`。\r \r 2. 通过使用 SSH 连接到群集：\r \r     ```bash\r     ssh username@clustername-ssh.azurehdinsight.cn`\r     ```\r \r     有关详细信息，请参阅[将 SSH 与 HDInsight 配合使用](../hdinsight-hadoop-linux-use-ssh-unix.md)。\r \r 3. 若要确保 mapper.py 和 reducer.py 具有正确的行尾，请使用以下命令：\r \r     ```bash\r     perl -pi -e 's/\\r\\n/\\n/g' mapper.py\r     perl -pi -e 's/\\r\\n/\\n/g' reducer.py\r     ```\r \r 4. 使用以下命令启动 MapReduce 作业。\r \r     ```bash\r     yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input /example/data/gutenberg/davinci.txt -output /example/wordcountout\r     ```\r \r     此命令包括以下几个部分：\r \r    * **hadoop-streaming.jar**：运行流式处理 MapReduce 操作时使用。 它可以将 Hadoop 和你提供的外部 MapReduce 代码连接起来。\r \r    * **-files**：将指定的文件添加到 MapReduce 作业。\r \r    * **-mapper**：告诉 Hadoop 要用作映射器的文件。\r \r    * **-reducer**：告诉 Hadoop 要用作化简器的文件。\r \r    * **-input**：要从中统计字数的输入文件。\r \r    * **-output**：输出将写入到的目录。\r \r     当 MapReduce 作业运行时，将以百分比形式显示进程。\r \r     ```\r     15/02/05 19:01:04 INFO mapreduce.Job:  map 0% reduce 0%\r     15/02/05 19:01:16 INFO mapreduce.Job:  map 100% reduce 0%\r     15/02/05 19:01:27 INFO mapreduce.Job:  map 100% reduce 100%\r     ```\r \r 5. 若要查看输出，请使用以下命令：\r \r     ```bash\r     hdfs dfs -text /example/wordcountout/part-00000\r     ```\r \r     此命令会显示单词和单词出现次数的列表。\r \r ## <a name=\"next-steps\"></a>后续步骤\r \r 了解如何将流式处理 MapRedcue 作业用于 HDInsight 后，就可以使用以下链接来学习 Azure HDInsight 的其他用法。\r \r * [将 Hive 与 HDInsight 配合使用](hdinsight-use-hive.md)\r * [将 Pig 与 HDInsight 配合使用](hdinsight-use-pig.md)\r * [将 MapReduce 作业与 HDInsight 配合使用](hdinsight-use-mapreduce.md)\r \r \r <!--Update_Description: update wording and link references-->"}