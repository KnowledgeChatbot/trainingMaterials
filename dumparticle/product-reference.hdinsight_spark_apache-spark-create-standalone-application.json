{"Title":"创建要在 Spark 群集上运行的 Scala 应用 - Azure HDInsight","Description":"将 Apache Maven 作为生成系统和 IntelliJ IDEA 提供的 Scala 的现有 Maven 原型，创建用 Scala 编写的 Spark 应用程序。","Content":"# <a name=\"create-a-scala-maven-application-to-run-on-apache-spark-cluster-on-hdinsight\"></a>创建要在 HDInsight 上的 Apache Spark 群集中运行的 Scala Maven 应用程序\r \r 了解如何结合使用 Maven 和 IntelliJ IDEA 创建用 Scala 编写的 Spark 应用程序。 本文将 Apache Maven 用作生成系统，并从 IntelliJ IDEA 提供的 Scala 的现有 Maven 原型入手。  在 IntelliJ IDEA 中创建 Scala 应用程序需要以下步骤：\r \r * 将 Maven 用作生成系统。\r * 更新项目对象模型 (POM) 文件以解析 Spark 模块依赖项。\r * 在 Scala 中编写应用程序。\r * 生成可提交到 HDInsight Spark 群集的 jar 文件。\r * 使用 Livy 在 Spark 群集上运行应用程序。\r \r > [!NOTE]\r > HDInsight 还提供了一个 IntelliJ IDEA 插件工具，用于简化在 Linux 上创建应用程序并提交到 HDInsight Spark 群集的过程。 有关详细信息，请参阅[使用适用于 IntelliJ IDEA 的 HDInsight 工具插件创建和提交 Spark 应用程序](apache-spark-intellij-tool-plugin.md)。\r > \r > \r \r ## <a name=\"prerequisites\"></a>先决条件\r \r * Azure 订阅。 请参阅[获取 Azure 试用版](https://www.azure.cn/pricing/1rmb-trial/)。\r * HDInsight 上的 Apache Spark 群集。 有关说明，请参阅[在 Azure HDInsight 中创建 Apache Spark 群集](apache-spark-jupyter-spark-sql.md)。\r * Oracle Java 开发工具包。 可以从 [此处](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)安装它。\r * Java IDE。 本文使用 IntelliJ IDEA 15.0.1。 可以从 [此处](https://www.jetbrains.com/idea/download/)安装它。\r \r ## <a name=\"install-scala-plugin-for-intellij-idea\"></a>安装适用于 IntelliJ IDEA 的 Scala 插件\r 如果 IntelliJ IDEA 安装未提示启用 Scala 插件，请启动 IntelliJ IDEA 并完成以下步骤来安装该插件：\r \r 1. 启动 IntelliJ IDEA，在欢迎屏幕上单击“配置”，并单击“插件”。\r    \r     ![启用 scala 插件](./media/apache-spark-create-standalone-application/enable-scala-plugin.png)\r 2. 在下一屏幕中，单击左下角的“安装 JetBrains 插件”。 在打开的“浏览 JetBrains 插件”对话框中搜索 Scala，并单击“安装”。\r    \r     ![安装 scala 插件](./media/apache-spark-create-standalone-application/install-scala-plugin.png)\r 3. 插件安装成功后，请单击“重启 IntelliJ IDEA”  按钮重启 IDE。\r \r ## <a name=\"create-a-standalone-scala-project\"></a>创建独立 Scala 项目\r 1. 启动 IntelliJ IDEA 并创建一个新项目。 在“新建项目”对话框中做出以下选择，并单击“下一步”。\r    \r     ![创建 Maven 项目](./media/apache-spark-create-standalone-application/create-maven-project.png)\r \r    * 选择“Maven”  作为项目类型。\r    * 指定“项目 SDK” 。 单击“新建”并导航到 Java 安装目录，通常是 `C:\\Program Files\\Java\\jdk1.8.0_66`。\r    * 选择“从原型创建”  选项。\r    * 从原型列表中，选择“org.scala-tools.archetypes:scala-archetype-simple” 。 这会创建适当的目录结构，并下载所需的默认依赖项来编写 Scala 程序。\r 2. 提供 **GroupId**、**ArtifactId** 和 **Version** 的相关值。 单机“下一步”\r 3. 在下一个对话框中（在其中指定 Maven 主目录和其他用户设置的位置），接受默认值，并单击“下一步” 。\r 4. 在最后一个对话框中，指定项目名称和位置，并单击“完成” 。\r 5. 删除位于 **src\\test\\scala\\com\\microsoft\\spark\\example** 的 **MySpec.Scala** 文件。 应用程序不需要此文件。\r 6. 如有必要，对默认源和测试文件进行重命名。 从 IntelliJ IDEA 的左窗格中，导航到 **src\\main\\scala\\com.microsoft.spark.example**。 右键单击“App.scala”，单击“重构”、“重命名”文件，并在对话框中，提供应用程序的新名称，并单击“重构”。\r    \r     ![重命名文件](./media/apache-spark-create-standalone-application/rename-scala-files.png)  \r     \r 7. 后续步骤将更新 pom.xml 以定义 Spark Scala 应用程序的依赖项。 若要自动下载并解析这些依赖项，必须相应地配置 Maven。\r    \r     ![配置 Maven 以进行自动下载](./media/apache-spark-create-standalone-application/configure-maven.png)\r    \r    1. 在“文件”菜单中，单击“设置”。\r    2. 在“设置”对话框中，导航到“生成、执行、部署” > “生成工具” > “Maven” > “导入”。\r    3. 选择“自动导入 Maven 项目”选项。\r    4. 单击“应用”，并单击“确定”。\r 8. 更新 Scala 源文件以包含应用程序代码。 打开现有示例代码，并将其替换为以下代码，保存所做更改。 此代码从 HVAC.csv（所有 HDInsight Spark 群集均有该文件）中读取数据，检索第六列中只有一个数字的行，并将输出写入群集的默认存储容器下的 **/HVACOut**。\r \r         package com.microsoft.spark.example\r \r         import org.apache.spark.SparkConf\r         import org.apache.spark.SparkContext\r \r         /**\r           * Test IO to wasb\r           */\r         object WasbIOTest {\r           def main (arg: Array[String]): Unit = {\r             val conf = new SparkConf().setAppName(\"WASBIOTest\")\r             val sc = new SparkContext(conf)\r \r             val rdd = sc.textFile(\"wasb:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\")\r \r             //find the rows which have only one digit in the 7th column in the CSV\r             val rdd1 = rdd.filter(s => s.split(\",\")(6).length() == 1)\r \r             rdd1.saveAsTextFile(\"wasb:///HVACout\")\r           }\r         }\r 9. 更新 pom.xml。\r    \r    1. 在 `<project>\\<properties>` 中添加以下内容：\r       \r           <scala.version>2.10.4</scala.version>\r           <scala.compat.version>2.10.4</scala.compat.version>\r           <scala.binary.version>2.10</scala.binary.version>\r    2. 在 `<project>\\<dependencies>` 中添加以下内容：\r       \r            <dependency>\r              <groupId>org.apache.spark</groupId>\r              <artifactId>spark-core_${scala.binary.version}</artifactId>\r              <version>1.4.1</version>\r            </dependency>\r       \r       将更改保存到 pom.xml。\r 10. 创建 .jar 文件。 IntelliJ IDEA 允许创建 JAR，作为项目的一个项目 (artifact)。 执行以下步骤。\r \r     1. 在“文件”菜单中，单击“项目结构”。\r     2. 在“项目结构”对话框中，单击“项目”，并单击加号。 在弹出的对话框中，单击“JAR”，并单击“从包含依赖项的模块”。\r        \r         ![创建 JAR](./media/apache-spark-create-standalone-application/create-jar-1.png)\r \t\r     3. 在“从模块创建 JAR”对话框中，单击“主类”旁边的省略号 (![ellipsis](./media/apache-spark-create-standalone-application/ellipsis.png))。\r     4. 在“选择主类”对话框中，选择默认显示的类，并单击“确定”。\r        \r         ![创建 JAR](./media/apache-spark-create-standalone-application/create-jar-2.png)\r \t\r     5. 在“从模块创建 JAR”对话框中，确保已选择“提取到目标 JAR”选项，并单击“确定”。 这会创建包含所有依赖项的单个 JAR。\r        \r         ![创建 JAR](./media/apache-spark-create-standalone-application/create-jar-3.png)\r \t\r     6. “输出布局”选项卡列出了所有包含为 Maven 项目一部分的 jar。 可以选择并删除 Scala 应用程序不直接依赖的 jar。 对于此处创建的应用程序，可以删除最后一个（**SparkSimpleApp 编译输出**）以外的所有 jar。 选择要删除的 jar，并单击“删除”  图标。\r        \r         ![创建 JAR](./media/apache-spark-create-standalone-application/delete-output-jars.png)\r        \r         请务必选中“在创建时生成”框，以确保每次生成或更新项目时都创建 jar。 单击“应用”，并单击“确定”。\r \t\r     7. 在菜单栏中单击“生成”，并单击“创建项目”。 也可以单击“生成项目”以创建 jar。 输出 jar 会在 **\\out\\artifacts** 下创建。\r        \r         ![创建 JAR](./media/apache-spark-create-standalone-application/output.png)\r \r ## <a name=\"run-the-application-on-the-spark-cluster\"></a>在 Spark 群集上运行应用程序\r 若要在群集上运行应用程序，必须执行以下操作：\r \r * **将应用程序 jar 复制到群集关联的 Azure 存储 blob**。 可以使用命令行实用工具 [**AzCopy**](../../storage/common/storage-use-azcopy.md) 来执行此操作。 也可以使用许多其他客户端来上传数据。 有关详细信息，请参阅[在 HDInsight 中上传 Hadoop 作业的数据](../hdinsight-upload-data.md)。\r * **使用 Livy 将应用程序作业远程提交**到 Spark 群集。 HDInsight 上的 Spark 群集包括 Livy，可公开 REST 终结点以远程提交 Spark 作业。 有关详细信息，请参阅[将 Livy 与 HDInsight 上的 Spark 群集配合使用以远程提交 Spark 作业](apache-spark-livy-rest-interface.md)。\r \r ## <a name=\"next-step\"></a>后续步骤\r \r 本文介绍如何创建 Spark scala 应用程序。 转到下一文章，了解如何使用 Livy 在 HDInsight Spark 群集上运行此应用程序。\r \r > [!div class=\"nextstepaction\"]\r >[使用 Livy 在 Spark 群集中远程运行作业](apache-spark-livy-rest-interface.md)\r \r \r <!--Update_Description: update wording and link references-->"}