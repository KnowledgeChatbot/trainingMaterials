{"Title":"在 HDInsight 中将 Hadoop Pig 与远程桌面配合使用 - Azure","Description":"了解如何使用 Pig 命令从到基于 Windows 的 HDInsight Hadoop 群集的远程桌面连接运行 Pig Latin 语句。","Content":"# <a name=\"run-pig-jobs-from-a-remote-desktop-connection\"></a>从远程桌面连接运行 Pig 作业\r [!INCLUDE [pig-selector](../../../includes/hdinsight-selector-use-pig.md)]\r \r 本文档演练了如何使用 Pig 命令从到基于 Windows 的 HDInsight 群集的远程桌面连接运行 Pig Latin 语句。 Pig Latin 允许通过描述数据转换创建 MapReduce 应用程序，而不是创建映射和化简函数。\r \r > [!IMPORTANT]\r > 远程桌面只能在使用 Windows 作为操作系统的 HDInsight 群集上使用。 Linux 是在 HDInsight 3.4 版或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 在 Windows 上停用](../hdinsight-component-versioning.md#hdinsight-windows-retirement)。\r >\r > 有关 HDInsight 3.4 或更高版本，请参阅[将 Pig 与 HDInsight 和 SSH 配合使用](apache-hadoop-use-pig-ssh.md)，了解如何通过命令行直接在群集上以交互方式运行 Pig 作业。\r \r ## <a id=\"prereq\"></a>先决条件\r 要完成本文中的步骤，需要：\r \r * 基于 Windows 的 HDInsight（HDInsight 上的 Hadoop）群集\r * 运行 Windows 10、Windows 8 或 Windows 7 的客户端计算机\r \r ## <a id=\"connect\"></a>使用远程桌面进行连接\r 为 HDInsight 群集启用远程桌面，然后根据[使用 RDP 连接到 HDInsight 群集](../hdinsight-administer-use-management-portal.md#connect-to-clusters-using-rdp)中的说明连接到该群集。\r \r ## <a id=\"pig\"></a>使用 Pig 命令\r 1. 在建立远程桌面连接后，通过使用桌面上的图标启动 **Hadoop 命令行** 。\r 2. 使用以下方法启动 Pig 命令：\r \r         %pig_home%\\bin\\pig\r \r     系统将提供 `grunt>` 提示符。\r 3. 输入以下语句：\r \r         LOGS = LOAD 'wasb:///example/data/sample.log';\r \r     此命令会将 sample.log 文件的内容加载到 LOGS 文件中。 可以通过使用以下命令查看该文件的内容：\r \r         DUMP LOGS;\r 4. 通过应用正则表达式从每个记录中仅提取日志记录级别来转换数据：\r \r         LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\r \r     转换后，可以使用 **DUMP** 查看数据。 在本例中为 `DUMP LEVELS;`。\r 5. 使用以下语句继续应用转换。 使用 `DUMP` 查看每个步骤后的转换结果。\r \r     <table>\r     <tr>\r     <th>语句</th><th>作用</th>\r     </tr>\r     <tr>\r     <td>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</td><td>删除包含日志级别 Null 值的行，并将结果存储到 FILTEREDLEVELS。</td>\r     </tr>\r     <tr>\r     <td>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</td><td>按日志级别对行进行分组，并将结果存储到 GROUPEDLEVELS。</td>\r     </tr>\r     <tr>\r     <td>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</td><td>创建一组新的数据，其中包含每个唯一日志级别值和其发生次数。 此数据将存储到 FREQUENCIES</td>\r     </tr>\r     <tr>\r     <td>RESULT = order FREQUENCIES by COUNT desc;</td><td>按计数为日志级别排序（降序），并存储到 RESULT</td>\r     </tr>\r     </table>\r 6.还可使用 `STORE` 语句保存转换结果。 例如，以下命令将 `RESULT` 保存到群集的默认存储容器中的 **/example/data/pigout**目录：\r \r         STORE RESULT into 'wasb:///example/data/pigout'\r \r    > [!NOTE]\r    > 数据将存储到指定目录中名为 **part-nnnnn** 的文件中。 如果该目录已存在，会收到错误消息。\r    >\r    >\r 7. 若要退出 grunt 提示符，请输入以下语句。\r \r         QUIT;\r \r ### <a name=\"pig-latin-batch-files\"></a>Pig Latin 批处理文件\r 也可以使用 Pig 命令运行文件中包含的 Pig Latin。\r \r 1. 退出 grunt 提示符之后，请打开“记事本”，并在 %PIG_HOME% 目录中创建名为 pigbatch.pig 的新文件。\r 2. 在 **pigbatch.pig** 文件中键入或粘贴以下行，并保存它：\r \r         LOGS = LOAD 'wasb:///example/data/sample.log';\r         LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\r         FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;\r         GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;\r         FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;\r         RESULT = order FREQUENCIES by COUNT desc;\r         DUMP RESULT;\r 3. 使用以下命令，使用 pig 命令运行 **pigbatch.pig** 文件。\r \r         pig %PIG_HOME%\\pigbatch.pig\r \r     在批处理作业完成后，应该会看到以下输出，该输出应该与先前步骤中使用 `DUMP RESULT;` 时相同：\r \r         (TRACE,816)\r         (DEBUG,434)\r         (INFO,96)\r         (WARN,11)\r         (ERROR,6)\r         (FATAL,2)\r \r ## <a id=\"summary\"></a>摘要\r 如你所见，Pig 命令允许以交互方式运行 MapReduce 操作，或运行存储在批处理文件中的 Pig Latin 作业。\r \r ## <a id=\"nextsteps\"></a>后续步骤\r 有关 HDInsight 中的 Pig 的一般信息：\r \r * [将 Pig 与 Hadoop on HDInsight 配合使用](hdinsight-use-pig.md)\r \r 有关 HDInsight 上的 Hadoop 的其他使用方法的信息：\r \r * [将 Hive 与 Hadoop on HDInsight 配合使用](hdinsight-use-hive.md)\r * [将 MapReduce 与 HDInsight 上的 Hadoop 配合使用](hdinsight-use-mapreduce.md)\r \r \r <!--Update_Description: update wording and link references-->"}