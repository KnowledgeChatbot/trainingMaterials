{"Title":"将 MapReduce 与 Hadoop on HDInsight 配合使用","Description":"学习如何在 HDInsight 群集中的 Hadoop 上运行 MapReduce 作业。","Content":"# <a name=\"use-mapreduce-in-hadoop-on-hdinsight\"></a>在 Hadoop on HDInsight 中使用 MapReduce\r \r 了解如何在 HDInsight 群集上运行 MapReduce 作业。 使用下表找到可将 MapReduce 与 HDInsight 配合使用的各种方法：\r \r | **请使用以下方法**... | **...实现此目的** | ...使用此 **群集操作系统** | ...从此 **客户端操作系统** |\r |:--- |:--- |:--- |:--- |\r | [SSH](apache-hadoop-use-mapreduce-ssh.md) |通过 **SSH** |Linux |Linux、Unix、Mac OS X 或 Windows |\r | [REST](apache-hadoop-use-mapreduce-curl.md) |使用 **REST**（示例使用 cURL）远程提交作业 |Linux 或 Windows |Linux、Unix、Mac OS X 或 Windows |\r | [Windows PowerShell](apache-hadoop-use-mapreduce-powershell.md) |使用 **Windows PowerShell** |Linux 或 Windows |Windows |\r | [远程桌面](apache-hadoop-use-mapreduce-remote-desktop.md)（HDInsight 3.2 和 3.3） |通过**远程桌面**使用 Hadoop 命令 |Windows |Windows |\r \r > [!IMPORTANT]\r > Linux 是在 HDInsight 3.4 版或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 在 Windows 上停用](../hdinsight-component-versioning.md#hdinsight-windows-retirement)。\r \r ## <a id=\"whatis\"></a>什么是 MapReduce\r \r Hadoop MapReduce 是一个软件框架，用于编写处理海量数据的作业。 输入的数据将拆分为独立的区块。 每个区块跨群集中的节点并行进行处理。 MapReduce 作业包括两个函数：\r \r * **映射器**：使用输入数据，对数据进行分析（通常使用筛选器和排序操作），并发出元组（键/值对）\r \r * **化简器**：使用映射器发出的元组并执行汇总运算，以基于映射器数据创建更小的合并结果\r \r 下图演示了一个基本的单词计数 MapReduce 作业示例：\r \r ![HDI.WordCountDiagram][image-hdi-wordcountdiagram]\r \r 此作业的输出是文本中每个单词出现次数的计数。\r \r * mapper 将输入文本中的每一行作为一个输入并将其拆分为多个单词。 每当一个单词出现时，mapper 发出一个键/值对，其中在该单词后跟一个 1。 输出在发送到化简器之前经过排序。\r * 随后，化简器会计算每个单词的计数的和并发出一个键/值对（包含单词，后跟该单词的总出现次数）。\r \r MapReduce 可使用多种语言实现。 Java 是最常见的实现，本文档中使用该语言进行演示。\r \r ## <a name=\"development-languages\"></a>开发语言\r \r 基于 Java 和 Java 虚拟机的语言或框架可作为 MapReduce 作业直接运行。 在本文档中使用的示例是 Java MapReduce 应用程序。 C#、Python 等非 Java 语言或独立可执行文件必须使用 **Hadoop 流式处理**。\r \r Hadoop 流式处理通过 STDIN 和 STDOUT 与映射器和化简器通信。 映射器和化简器从 STDIN 中一次读取一行数据，并将输出写入 STDOUT。 映射器和化简器读取或发出的每行必须采用制表符分隔的键/值对格式：\r \r     [key]/t[value]\r \r 有关详细信息，请参阅 [Hadoop Streaming](http://hadoop.apache.org/docs/r1.2.1/streaming.html)（Hadoop 流式处理）。\r \r 有关将 Hadoop 流式处理与 HDInsight 配合使用的示例，请参阅以下文档：\r \r * [开发 C# MapReduce 作业](apache-hadoop-dotnet-csharp-mapreduce-streaming.md)\r \r * [开发 Python MapReduce 作业](apache-hadoop-streaming-python.md)\r \r ## <a id=\"data\"></a>示例数据\r \r HDInsight 提供存储在 `/example/data` 和 `/HdiSamples` 目录中的各种示例数据集。 这些目录位于群集的默认存储中。 在本文档中，我们使用 `/example/data/gutenberg/davinci.txt` 文件。 此文件包含 Leonardo Da Vinci 的笔记本。\r \r ## <a id=\"job\"></a>MapReduce 示例\r \r MapReduce 单词计数应用程序示例包含在 HDInsight 群集中。 此示例位于群集默认存储的 `/example/jars/hadoop-mapreduce-examples.jar` 中。\r \r 以下 Java 代码是包含在 `hadoop-mapreduce-examples.jar` 文件中的 MapReduce 应用程序的源代码：\r \r ```java\r package org.apache.hadoop.examples;\r \r import java.io.IOException;\r import java.util.StringTokenizer;\r \r import org.apache.hadoop.conf.Configuration;\r import org.apache.hadoop.fs.Path;\r import org.apache.hadoop.io.IntWritable;\r import org.apache.hadoop.io.Text;\r import org.apache.hadoop.mapreduce.Job;\r import org.apache.hadoop.mapreduce.Mapper;\r import org.apache.hadoop.mapreduce.Reducer;\r import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r import org.apache.hadoop.util.GenericOptionsParser;\r \r public class WordCount {\r \r     public static class TokenizerMapper\r         extends Mapper<Object, Text, Text, IntWritable>{\r \r     private final static IntWritable one = new IntWritable(1);\r     private Text word = new Text();\r \r     public void map(Object key, Text value, Context context\r                     ) throws IOException, InterruptedException {\r         StringTokenizer itr = new StringTokenizer(value.toString());\r         while (itr.hasMoreTokens()) {\r         word.set(itr.nextToken());\r         context.write(word, one);\r         }\r     }\r     }\r \r     public static class IntSumReducer\r         extends Reducer<Text,IntWritable,Text,IntWritable> {\r     private IntWritable result = new IntWritable();\r \r     public void reduce(Text key, Iterable<IntWritable> values,\r                         Context context\r                         ) throws IOException, InterruptedException {\r         int sum = 0;\r         for (IntWritable val : values) {\r         sum += val.get();\r         }\r         result.set(sum);\r         context.write(key, result);\r     }\r     }\r \r     public static void main(String[] args) throws Exception {\r     Configuration conf = new Configuration();\r     String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\r     if (otherArgs.length != 2) {\r         System.err.println(\"Usage: wordcount <in> <out>\");\r         System.exit(2);\r     }\r     Job job = new Job(conf, \"word count\");\r     job.setJarByClass(WordCount.class);\r     job.setMapperClass(TokenizerMapper.class);\r     job.setCombinerClass(IntSumReducer.class);\r     job.setReducerClass(IntSumReducer.class);\r     job.setOutputKeyClass(Text.class);\r     job.setOutputValueClass(IntWritable.class);\r     FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\r     FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\r     System.exit(job.waitForCompletion(true) ? 0 : 1);\r     }\r }\r ```\r \r 有关编写自己的 MapReduce 应用程序的说明，请参阅以下文档：\r \r * [为 HDInsight 开发 Java MapReduce 应用程序](apache-hadoop-develop-deploy-java-mapreduce-linux.md)\r \r * [为 HDInsight 开发 Python MapReduce 应用程序](apache-hadoop-streaming-python.md)\r \r ## <a id=\"run\"></a>运行 MapReduce\r \r HDInsight 可以使用各种方法运行 HiveQL 作业。 使用下表来确定哪种方法最适合用户，并访问此链接进行演练。\r \r | **使用此方法**... | **...实现此目的** | ...使用此 **群集操作系统** | ...从此 **客户端操作系统** |\r |:--- |:--- |:--- |:--- |\r | [SSH](apache-hadoop-use-mapreduce-ssh.md) |通过 **SSH** |Linux |Linux、Unix、Mac OS X 或 Windows |\r | [Curl](apache-hadoop-use-mapreduce-curl.md) |使用 **REST** |Linux 或 Windows |Linux、Unix、Mac OS X 或 Windows |\r | [Windows PowerShell](apache-hadoop-use-mapreduce-powershell.md) |使用 **Windows PowerShell** |Linux 或 Windows |Windows |\r | [远程桌面](apache-hadoop-use-mapreduce-remote-desktop.md)（HDInsight 3.2 和 3.3） |通过**远程桌面**使用 Hadoop 命令 |Windows |Windows |\r \r [!INCLUDE [hdinsight-linux-acn-version.md](../../../includes/hdinsight-linux-acn-version.md)]\r \r > [!IMPORTANT]\r > Linux 是在 HDInsight 3.4 版或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 在 Windows 上停用](../hdinsight-component-versioning.md#hdinsight-windows-retirement)。\r \r ## <a id=\"nextsteps\"></a>后续步骤\r \r 若要了解如何使用 HDInsight 中的数据的详细信息，请参阅以下文档：\r \r * [为 HDInsight 开发 Java MapReduce 程序](apache-hadoop-develop-deploy-java-mapreduce-linux.md)\r \r * [为 HDInsight 开发 Python 流式处理 MapReduce 程序](apache-hadoop-streaming-python.md)\r \r * [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]\r \r * [将 Pig 与 HDInsight 配合使用][hdinsight-use-pig]\r \r \r [hdinsight-upload-data]: ../hdinsight-upload-data.md\r [hdinsight-get-started]:apache-hadoop-linux-tutorial-get-started.md\r [hdinsight-develop-mapreduce-jobs]: apache-hadoop-develop-deploy-java-mapreduce-linux.md\r [hdinsight-use-hive]:../hdinsight-use-hive.md\r [hdinsight-use-pig]:hdinsight-use-pig.md\r \r [powershell-install-configure]: https://docs.microsoft.com/powershell/azureps-cmdlets-docs\r \r [image-hdi-wordcountdiagram]: ./media/hdinsight-use-mapreduce/HDI.WordCountDiagram.gif\r \r \r <!--Update_Description: update wording and link references-->"}