{"Title":"Apache Storm 示例 Java 拓扑 - Azure HDInsight","Description":"了解如何通过创建一个示例单词计数拓扑，来以 Java 语言创建 Apache Storm 拓扑。","Content":"# <a name=\"create-an-apache-storm-topology-in-java\"></a>以 Java 语言创建 Apache Storm 拓扑\r \r 了解如何为 Apache Storm 创建基于 Java 的拓扑。 将创建一个实现单词计数应用程序的 Storm 拓扑。 将使用 Maven 生成并打包项目。 然后，了解如何使用 Flux 框架定义拓扑。\r \r [!INCLUDE [hdinsight-linux-acn-version.md](../../../includes/hdinsight-linux-acn-version.md)]\r \r > [!NOTE]\r > Storm 0.10.0 或更高版本中提供了 Flux 框架。 HDInsight 3.3 和 3.4 提供了 Storm 0.10.0。\r \r 完成本文档中的步骤之后，可将拓扑部署到 Apache Storm on HDInsight。\r \r > [!NOTE]\r > [https://github.com/Azure-Samples/hdinsight-java-storm-wordcount](https://github.com/Azure-Samples/hdinsight-java-storm-wordcount) 上提供了本文档中创建的 Storm 拓扑示例的完整版本。\r \r ## <a name=\"prerequisites\"></a>先决条件\r \r * [Java 开发人员工具包 (JDK) 版本 8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)\r \r * [Maven (https://maven.apache.org/download.cgi)](https://maven.apache.org/download.cgi)：Maven 是 Java 项目的项目生成系统。\r \r * 文本编辑器或 IDE。\r \r ## <a name=\"configure-environment-variables\"></a>配置环境变量\r \r 可以在安装 Java 和 JDK 时设置以下环境变量。 但应检查其是否存在并且包含相关系统的适当值。\r \r * **JAVA_HOME** - 应该指向已安装 Java 运行时环境 (JRE) 的目录。 例如，在 Unix 或 Linux 分发版中，它的值应该类似于 `/usr/lib/jvm/java-8-oracle`。 在 Windows 中，它的值类似于 `c:\\Program Files (x86)\\Java\\jre1.8`\r \r * **PATH** - 应该包含以下路径：\r \r   * **JAVA_HOME**（或等效的路径）\r \r   * **JAVA_HOME\\bin**（或等效的路径）\r \r   * 安装 Maven 的目录\r \r ## <a name=\"create-a-maven-project\"></a>创建 Maven 项目\r \r 在命令行中，使用以下命令创建名为 **WordCount** 的 Maven 项目：\r \r ```bash\r mvn archetype:generate -DarchetypeArtifactId=maven-archetype-quickstart -DgroupId=com.microsoft.example -DartifactId=WordCount -DinteractiveMode=false\r ```\r \r > [!NOTE]\r > 如果使用 PowerShell，必须将 `-D` 参数用双引号引起来。\r >\r > `mvn archetype:generate \"-DarchetypeArtifactId=maven-archetype-quickstart\" \"-DgroupId=com.microsoft.example\" \"-DartifactId=WordCount\" \"-DinteractiveMode=false\"`\r \r 该命令会在当前位置创建名为 `WordCount` 的目录，其中包含基本 Maven 项目。 `WordCount` 目录包含以下项：\r \r * `pom.xml`：包含 Maven 项目的设置。\r * `src\\main\\java\\com\\microsoft\\example`：包含应用程序代码。\r * `src\\test\\java\\com\\microsoft\\example`：包含应用程序的测试。 \r \r ### <a name=\"remove-the-generated-example-code\"></a>删除生成的示例代码\r \r 删除生成的测试和应用程序文件：\r \r * **src\\test\\java\\com\\microsoft\\example\\AppTest.java**\r * **src\\main\\java\\com\\microsoft\\example\\App.java**\r \r ## <a name=\"add-maven-repositories\"></a>添加 Maven 存储库\r \r 由于 HDInsight 基于 Hortonworks Data Platform (HDP)，因此我们建议使用 Hortonworks 存储库来下载 Apache Storm 项目的依赖项。 在 __pom.xml__ 文件中，在 `<url>http://maven.apache.org</url>` 行后添加以下 XML：\r \r ```xml\r <repositories>\r     <repository>\r         <releases>\r             <enabled>true</enabled>\r             <updatePolicy>always</updatePolicy>\r             <checksumPolicy>warn</checksumPolicy>\r         </releases>\r         <snapshots>\r             <enabled>false</enabled>\r             <updatePolicy>never</updatePolicy>\r             <checksumPolicy>fail</checksumPolicy>\r         </snapshots>\r         <id>HDPReleases</id>\r         <name>HDP Releases</name>\r         <url>http://repo.hortonworks.com/content/repositories/releases/</url>\r         <layout>default</layout>\r     </repository>\r     <repository>\r         <releases>\r             <enabled>true</enabled>\r             <updatePolicy>always</updatePolicy>\r             <checksumPolicy>warn</checksumPolicy>\r         </releases>\r         <snapshots>\r             <enabled>false</enabled>\r             <updatePolicy>never</updatePolicy>\r             <checksumPolicy>fail</checksumPolicy>\r         </snapshots>\r         <id>HDPJetty</id>\r         <name>Hadoop Jetty</name>\r         <url>http://repo.hortonworks.com/content/repositories/jetty-hadoop/</url>\r         <layout>default</layout>\r     </repository>\r </repositories>\r ```\r \r ## <a name=\"add-properties\"></a>添加属性\r \r Maven 允许定义项目级的值，称为属性。 在 __pom.xml__ 中，在 `</repositories>` 行后添加以下容：\r \r ```xml\r <properties>\r     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\r     <!--\r     This is a version of Storm from the Hortonworks repository that is compatible with HDInsight 3.6.\r     -->\r     <storm.version>1.1.0.2.6.1.9-1</storm.version>\r </properties>\r ```\r \r 现在，可以在 `pom.xml` 的其他部分中使用此值。 例如，在指定 Storm 组件的版本时，可以使用 `${storm.version}` 而无需将值硬编码。\r \r ## <a name=\"add-dependencies\"></a>添加依赖项\r \r 添加 Storm 组件的依赖项。 打开 `pom.xml` 文件，并在 `<dependencies>` 部分添加以下代码：\r \r ```xml\r <dependency>\r     <groupId>org.apache.storm</groupId>\r     <artifactId>storm-core</artifactId>\r     <version>${storm.version}</version>\r     <!-- keep storm out of the jar-with-dependencies -->\r     <scope>provided</scope>\r </dependency>\r ```\r \r 在编译时，Maven 会使用此信息在 Maven 存储库中查找 `storm-core`。 它会先查找本地计算机上的存储库。 如果文件不存在，Maven 会从公共 Maven 存储库下载这些文件，并将其存储在本地存储库中。\r \r > [!NOTE]\r > 请注意该部分中的 `<scope>provided</scope>` 行。 此设置会告诉 Maven 从创建的任何 JAR 文件中排除 **storm-core**，因为系统会提供它。\r \r ## <a name=\"build-configuration\"></a>生成配置\r \r Maven 插件可用于自定义项目的生成阶段。 例如，如何编译项目或者如何将其打包到 JAR 文件中。 打开 `pom.xml` 文件，并紧靠在 `</project>` 行的上方添加以下代码。\r \r ```xml\r <build>\r     <plugins>\r     </plugins>\r     <resources>\r     </resources>\r </build>\r ```\r \r 此节用于添加插件、资源和其他生成配置选项。 有关 **pom.xml** 文件的完整参考信息，请参阅 [http://maven.apache.org/pom.html](http://maven.apache.org/pom.html)。\r \r ### <a name=\"add-plug-ins\"></a>添加插件\r \r 对于以 Java 语言实现的 Apache Storm 拓扑，[Exec Maven 插件](http://www.mojohaus.org/exec-maven-plugin/)十分有用，因为它可让你轻松地在开发环境本地运行拓扑。 在 `pom.xml` 文件的 `<plugins>` 部分中添加以下内容，以包括 Exec Maven 插件：\r \r ```xml\r <plugin>\r     <groupId>org.codehaus.mojo</groupId>\r     <artifactId>exec-maven-plugin</artifactId>\r     <version>1.5.0</version>\r     <executions>\r     <execution>\r     <goals>\r         <goal>exec</goal>\r     </goals>\r     </execution>\r     </executions>\r     <configuration>\r     <executable>java</executable>\r     <includeProjectDependencies>true</includeProjectDependencies>\r     <includePluginDependencies>false</includePluginDependencies>\r     <classpathScope>compile</classpathScope>\r     <mainClass>${storm.topology}</mainClass>\r     <cleanupDaemonThreads>false</cleanupDaemonThreads> \r     </configuration>\r </plugin>\r ```\r \r 另一个有用的插件是用于更改编译选项的 [Apache Maven Compiler 插件](http://maven.apache.org/plugins/maven-compiler-plugin/)。 这会更改 Maven 用作应用程序源和目标的 Java 版本。\r \r * 对于 __HDInsight 3.4 或更早的版本__，请将源和目标 Java 版本设置为 __1.7__。\r \r * 对于 HDInsight __3.5__，请将源和目标 Java 版本设置为 __1.8__。\r \r 在 `pom.xml` 文件的 `<plugins>` 部分添加以下文本，以包括 Apache Maven Compiler 插件。 此示例指定 1.8，因此目标 HDInsight 版本为 3.5。\r \r ```xml\r <plugin>\r     <groupId>org.apache.maven.plugins</groupId>\r     <artifactId>maven-compiler-plugin</artifactId>\r     <version>3.3</version>\r     <configuration>\r     <source>1.8</source>\r     <target>1.8</target>\r     </configuration>\r </plugin>\r ```\r \r ### <a name=\"configure-resources\"></a>配置资源\r \r 使用 resources 节可以包含非代码资源，例如拓扑中组件所需的配置文件。 本示例会在 `pom.xml 文件的 `<resources>` 节中添加以下文本。\r \r ```xml\r <resource>\r     <directory>${basedir}/resources</directory>\r     <filtering>false</filtering>\r     <includes>\r         <include>log4j2.xml</include>\r     </includes>\r </resource>\r ```\r \r 本示例会将项目根目录 (`${basedir}`) 中的 resources 目录添加为包含资源的位置，并包含名为 `log4j2.xml` 的文件。 此文件用于配置拓扑所要记录的信息。\r \r ## <a name=\"create-the-topology\"></a>创建拓扑\r \r 基于 Java 的 Apache Storm 拓扑包含必须编写（或引用）为依赖项的三个组件。\r \r * **Spouts**：读取外部源中的数据，并发出进入拓扑的数据流。\r \r * **Bolt**：对 Spout 或其他 Bolt 所发出的数据流执行处理，并发出一个或多个数据流。\r \r * **拓扑**：定义如何排列 Spout 和 Bolt，并提供拓扑的入口点。\r \r ### <a name=\"create-the-spout\"></a>创建 Spout\r \r 为了降低设置外部数据源的要求，以下 Spout 只会发出随机句子。 它是 [Storm-Starter 示例](https://github.com/apache/storm/blob/0.10.x-branch/examples/storm-starter/src/jvm/storm/starter)随附的 Spout 的修改版本。\r \r > [!NOTE]\r > 有关从外部数据源读取的 Spout 的示例，请参阅以下示例之一：\r >\r > * [TwitterSampleSpout](https://github.com/apache/storm/blob/0.10.x-branch/examples/storm-starter/src/jvm/storm/starter/spout/TwitterSampleSpout.java)：从Twitter 读取数据的示例 Spout\r > * [Storm-Kafka](https://github.com/apache/storm/tree/0.10.x-branch/external/storm-kafka)：从 Kafka 读取数据的 Spout\r \r 对于 Spout，请在 `src\\main\\java\\com\\microsoft\\example` 目录中创建名为 `RandomSentenceSpout.java` 的文件，并将以下 Java 代码用作其内容：\r \r ```java\r package com.microsoft.example;\r \r import org.apache.storm.spout.SpoutOutputCollector;\r import org.apache.storm.task.TopologyContext;\r import org.apache.storm.topology.OutputFieldsDeclarer;\r import org.apache.storm.topology.base.BaseRichSpout;\r import org.apache.storm.tuple.Fields;\r import org.apache.storm.tuple.Values;\r import org.apache.storm.utils.Utils;\r \r import java.util.Map;\r import java.util.Random;\r \r //This spout randomly emits sentences\r public class RandomSentenceSpout extends BaseRichSpout {\r   //Collector used to emit output\r   SpoutOutputCollector _collector;\r   //Used to generate a random number\r   Random _rand;\r \r   //Open is called when an instance of the class is created\r   @Override\r   public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {\r   //Set the instance collector to the one passed in\r     _collector = collector;\r     //For randomness\r     _rand = new Random();\r   }\r \r   //Emit data to the stream\r   @Override\r   public void nextTuple() {\r   //Sleep for a bit\r     Utils.sleep(100);\r     //The sentences that are randomly emitted\r     String[] sentences = new String[]{ \"the cow jumped over the moon\", \"an apple a day keeps the doctor away\",\r         \"four score and seven years ago\", \"snow white and the seven dwarfs\", \"i am at two with nature\" };\r     //Randomly pick a sentence\r     String sentence = sentences[_rand.nextInt(sentences.length)];\r     //Emit the sentence\r     _collector.emit(new Values(sentence));\r   }\r \r   //Ack is not implemented since this is a basic example\r   @Override\r   public void ack(Object id) {\r   }\r \r   //Fail is not implemented since this is a basic example\r   @Override\r   public void fail(Object id) {\r   }\r \r   //Declare the output fields. In this case, an sentence\r   @Override\r   public void declareOutputFields(OutputFieldsDeclarer declarer) {\r     declarer.declare(new Fields(\"sentence\"));\r   }\r }\r ```\r \r > [!NOTE]\r > 虽然此拓扑只使用一个 Spout，但其他拓扑可能存在将数据从不同源送入拓扑的多个 Spout。\r \r ### <a name=\"create-the-bolts\"></a>创建 Bolt\r \r Bolt 用于处理数据。 此拓扑使用两个 Bolt：\r \r * **SplitSentence**：将 **RandomSentenceSpout** 发出的句子分割成不同的单词。\r \r * **WordCount**：统计每个单词的出现次数。\r \r > [!NOTE]\r > Bolt 可以执行任何操作，例如，计算、保存，或者与外部组件通信。\r \r 在 `src\\main\\java\\com\\microsoft\\example` 目录中创建两个新文件：`SplitSentence.java` 和 `WordCount.java`。 将以下文本用作这些文件的内容：\r \r #### <a name=\"splitsentence\"></a>SplitSentence\r \r ```java\r package com.microsoft.example;\r \r import java.text.BreakIterator;\r \r import org.apache.storm.topology.BasicOutputCollector;\r import org.apache.storm.topology.OutputFieldsDeclarer;\r import org.apache.storm.topology.base.BaseBasicBolt;\r import org.apache.storm.tuple.Fields;\r import org.apache.storm.tuple.Tuple;\r import org.apache.storm.tuple.Values;\r \r //There are a variety of bolt types. In this case, use BaseBasicBolt\r public class SplitSentence extends BaseBasicBolt {\r \r   //Execute is called to process tuples\r   @Override\r   public void execute(Tuple tuple, BasicOutputCollector collector) {\r     //Get the sentence content from the tuple\r     String sentence = tuple.getString(0);\r     //An iterator to get each word\r     BreakIterator boundary=BreakIterator.getWordInstance();\r     //Give the iterator the sentence\r     boundary.setText(sentence);\r     //Find the beginning first word\r     int start=boundary.first();\r     //Iterate over each word and emit it to the output stream\r     for (int end=boundary.next(); end != BreakIterator.DONE; start=end, end=boundary.next()) {\r       //get the word\r       String word=sentence.substring(start,end);\r       //If a word is whitespace characters, replace it with empty\r       word=word.replaceAll(\"\\\\s+\",\"\");\r       //if it's an actual word, emit it\r       if (!word.equals(\"\")) {\r         collector.emit(new Values(word));\r       }\r     }\r   }\r \r   //Declare that emitted tuples contain a word field\r   @Override\r   public void declareOutputFields(OutputFieldsDeclarer declarer) {\r     declarer.declare(new Fields(\"word\"));\r   }\r }\r ```\r \r #### <a name=\"wordcount\"></a>WordCount\r \r ```java\r package com.microsoft.example;\r \r import java.util.HashMap;\r import java.util.Map;\r import java.util.Iterator;\r \r import org.apache.storm.Constants;\r import org.apache.storm.topology.BasicOutputCollector;\r import org.apache.storm.topology.OutputFieldsDeclarer;\r import org.apache.storm.topology.base.BaseBasicBolt;\r import org.apache.storm.tuple.Fields;\r import org.apache.storm.tuple.Tuple;\r import org.apache.storm.tuple.Values;\r import org.apache.storm.Config;\r \r // For logging\r import org.apache.logging.log4j.Logger;\r import org.apache.logging.log4j.LogManager;\r \r //There are a variety of bolt types. In this case, use BaseBasicBolt\r public class WordCount extends BaseBasicBolt {\r   //Create logger for this class\r   private static final Logger logger = LogManager.getLogger(WordCount.class);\r   //For holding words and counts\r   Map<String, Integer> counts = new HashMap<String, Integer>();\r   //How often to emit a count of words\r   private Integer emitFrequency;\r \r   // Default constructor\r   public WordCount() {\r       emitFrequency=5; // Default to 60 seconds\r   }\r \r   // Constructor that sets emit frequency\r   public WordCount(Integer frequency) {\r       emitFrequency=frequency;\r   }\r \r   //Configure frequency of tick tuples for this bolt\r   //This delivers a 'tick' tuple on a specific interval,\r   //which is used to trigger certain actions\r   @Override\r   public Map<String, Object> getComponentConfiguration() {\r       Config conf = new Config();\r       conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, emitFrequency);\r       return conf;\r   }\r \r   //execute is called to process tuples\r   @Override\r   public void execute(Tuple tuple, BasicOutputCollector collector) {\r     //If it's a tick tuple, emit all words and counts\r     if(tuple.getSourceComponent().equals(Constants.SYSTEM_COMPONENT_ID)\r             && tuple.getSourceStreamId().equals(Constants.SYSTEM_TICK_STREAM_ID)) {\r       for(String word : counts.keySet()) {\r         Integer count = counts.get(word);\r         collector.emit(new Values(word, count));\r         logger.info(\"Emitting a count of \" + count + \" for word \" + word);\r       }\r     } else {\r       //Get the word contents from the tuple\r       String word = tuple.getString(0);\r       //Have we counted any already?\r       Integer count = counts.get(word);\r       if (count == null)\r         count = 0;\r       //Increment the count and store it\r       count++;\r       counts.put(word, count);\r     }\r   }\r \r   //Declare that this emits a tuple containing two fields; word and count\r   @Override\r   public void declareOutputFields(OutputFieldsDeclarer declarer) {\r     declarer.declare(new Fields(\"word\", \"count\"));\r   }\r }\r ```\r \r ### <a name=\"define-the-topology\"></a>定义拓扑\r \r 拓扑将 Spout 和 Bolt 一起绑定到图形，该图形定义了组件之间的数据流动方式。 它还提供 Storm 在群集内创建组件的实例时使用的并行度提示。\r \r 下图是此拓扑的组件的基本原理图。\r \r ![显示 Spout 和 Bolt 排列方式的示意图](./media/apache-storm-develop-java-topology/wordcount-topology.png)\r \r 若要实现该拓扑，请在 `WordCountTopology.java` in the `src\\main\\java\\com\\microsoft\\example` 。 将以下 Java 代码用作该文件的内容：\r \r ```java\r package com.microsoft.example;\r \r import org.apache.storm.Config;\r import org.apache.storm.LocalCluster;\r import org.apache.storm.StormSubmitter;\r import org.apache.storm.topology.TopologyBuilder;\r import org.apache.storm.tuple.Fields;\r \r import com.microsoft.example.RandomSentenceSpout;\r \r public class WordCountTopology {\r \r   //Entry point for the topology\r   public static void main(String[] args) throws Exception {\r   //Used to build the topology\r     TopologyBuilder builder = new TopologyBuilder();\r     //Add the spout, with a name of 'spout'\r     //and parallelism hint of 5 executors\r     builder.setSpout(\"spout\", new RandomSentenceSpout(), 5);\r     //Add the SplitSentence bolt, with a name of 'split'\r     //and parallelism hint of 8 executors\r     //shufflegrouping subscribes to the spout, and equally distributes\r     //tuples (sentences) across instances of the SplitSentence bolt\r     builder.setBolt(\"split\", new SplitSentence(), 8).shuffleGrouping(\"spout\");\r     //Add the counter, with a name of 'count'\r     //and parallelism hint of 12 executors\r     //fieldsgrouping subscribes to the split bolt, and\r     //ensures that the same word is sent to the same instance (group by field 'word')\r     builder.setBolt(\"count\", new WordCount(), 12).fieldsGrouping(\"split\", new Fields(\"word\"));\r \r     //new configuration\r     Config conf = new Config();\r     //Set to false to disable debug information when\r     // running in production on a cluster\r     conf.setDebug(false);\r \r     //If there are arguments, we are running on a cluster\r     if (args != null && args.length > 0) {\r       //parallelism hint to set the number of workers\r       conf.setNumWorkers(3);\r       //submit the topology\r       StormSubmitter.submitTopology(args[0], conf, builder.createTopology());\r     }\r     //Otherwise, we are running locally\r     else {\r       //Cap the maximum number of executors that can be spawned\r       //for a component to 3\r       conf.setMaxTaskParallelism(3);\r       //LocalCluster is used to run locally\r       LocalCluster cluster = new LocalCluster();\r       //submit the topology\r       cluster.submitTopology(\"word-count\", conf, builder.createTopology());\r       //sleep\r       Thread.sleep(10000);\r       //shut down the cluster\r       cluster.shutdown();\r     }\r   }\r }\r ```\r \r ### <a name=\"configure-logging\"></a>配置日志记录\r \r Storm 使用 Apache Log4j 来记录信息。 如果未配置日志记录，拓扑将发出诊断信息。 若要控制所记录的信息，请在 `log4j2.xml` in the `resources` 。 将以下 XML 用作该文件的内容。\r \r ```xml\r <?xml version=\"1.0\" encoding=\"UTF-8\"?>\r <Configuration>\r <Appenders>\r     <Console name=\"STDOUT\" target=\"SYSTEM_OUT\">\r         <PatternLayout pattern=\"%d{HH:mm:ss} [%t] %-5level %logger{36} - %msg%n\"/>\r     </Console>\r </Appenders>\r <Loggers>\r     <Logger name=\"com.microsoft.example\" level=\"trace\" additivity=\"false\">\r         <AppenderRef ref=\"STDOUT\"/>\r     </Logger>\r     <Root level=\"error\">\r         <Appender-Ref ref=\"STDOUT\"/>\r     </Root>\r </Loggers>\r </Configuration>\r ```\r \r 此 XML 为 `com.microsoft.example` 类（其中包含本示例拓扑中的组件）配置一个新记录器。 此记录器的级别设置为“跟踪”，可以捕获此拓扑中的组件发出的任何日志记录信息。\r \r `<Root level=\"error\">` 部分将日志记录的根级别（不在 `com.microsoft.example` 中的所有内容）配置为只记录错误信息。\r \r 有关为 Log4j 配置日志记录的详细信息，请参阅 [http://logging.apache.org/log4j/2.x/manual/configuration.html](http://logging.apache.org/log4j/2.x/manual/configuration.html)。\r \r > [!NOTE]\r > Storm 0.10.0 版及更高版本使用 Log4j 2.x。 早期版本的 Storm 使用 Log4j 1.x（为日志配置使用的格式不同）。 有关旧配置的信息，请参阅 [http://wiki.apache.org/logging-log4j/Log4jXmlFormat](http://wiki.apache.org/logging-log4j/Log4jXmlFormat)。\r \r ## <a name=\"test-the-topology-locally\"></a>在本地测试拓扑\r \r 保存文件之后，请使用以下命令在本地测试拓扑。\r \r ```bash\r mvn compile exec:java -Dstorm.topology=com.microsoft.example.WordCountTopology\r ```\r \r 运行该命令时，拓扑显示启动信息。 以下文本是单词计数输出的示例：\r \r     17:33:27 [Thread-12-count] INFO  com.microsoft.example.WordCount - Emitting a count of 56 for word snow\r     17:33:27 [Thread-12-count] INFO  com.microsoft.example.WordCount - Emitting a count of 56 for word white\r     17:33:27 [Thread-12-count] INFO  com.microsoft.example.WordCount - Emitting a count of 112 for word seven\r     17:33:27 [Thread-16-count] INFO  com.microsoft.example.WordCount - Emitting a count of 195 for word the\r     17:33:27 [Thread-30-count] INFO  com.microsoft.example.WordCount - Emitting a count of 113 for word and\r     17:33:27 [Thread-30-count] INFO  com.microsoft.example.WordCount - Emitting a count of 57 for word dwarfs\r     17:33:27 [Thread-12-count] INFO  com.microsoft.example.WordCount - Emitting a count of 57 for word snow\r \r 此示例日志指示单词“and”已发出了 113 次。 只要拓扑运行，计数就会持续增加，因为 Spout 会连续发出相同的句子。\r \r 每两次发出单词和句子的间隔为 5 秒。 **WordCount** 组件配置为仅当 tick 元组到达时才发出信息。 它要求仅每五秒钟传送一次 tick 元组。\r \r ## <a name=\"convert-the-topology-to-flux\"></a>将拓扑转换为 Flux\r \r Flux 是 Storm 0.10.0 及更高版本随附的一个新框架，可以将配置和实现分离开来。 组件仍然是以 Java 语言定义的，但拓扑是使用 YAML 文件定义的。 可以随项目一起打包默认的拓扑定义，也可以在提交拓扑时使用独立的文件。 将拓扑提交到 Storm 时，可以使用环境变量或配置文件来填充 YAML 拓扑定义中的值。\r \r YAML 文件定义了要用于拓扑的组件以及它们之间的数据流。 可以包括一个 YAML 文件（作为 jar 文件的一部分），也可以使用外部 YAML 文件。\r \r \r > [!WARNING]\r > 由于 Storm 1.0.1 的 [bug (https://issues.apache.org/jira/browse/STORM-2055)](https://issues.apache.org/jira/browse/STORM-2055)，可能需要安装 Storm 开发环境，在本地运行 Flux 拓扑。\r \r 1. 将 `WordCountTopology.java` 文件移出项目。 以前，此文件定义了拓扑，但有了 Flux，就不需要此文件了。\r \r 2. 在 `resources` 目录中，创建一个名为 `topology.yaml` 的文件。 将以下文本用作此文件的内容。\r \r     ```yaml\r     name: \"wordcount\"       # friendly name for the topology\r \r     config:                 # Topology configuration\r       topology.workers: 1     # Hint for the number of workers to create\r \r     spouts:                 # Spout definitions\r     - id: \"sentence-spout\"\r       className: \"com.microsoft.example.RandomSentenceSpout\"\r       parallelism: 1      # parallelism hint\r \r     bolts:                  # Bolt definitions\r     - id: \"splitter-bolt\"\r       className: \"com.microsoft.example.SplitSentence\"\r       parallelism: 1\r         \r     - id: \"counter-bolt\"\r       className: \"com.microsoft.example.WordCount\"\r       constructorArgs:\r         - 10\r       parallelism: 1\r \r     streams:                # Stream definitions\r     - name: \"Spout --> Splitter\" # name isn't used (placeholder for logging, UI, etc.)\r       from: \"sentence-spout\"       # The stream emitter\r       to: \"splitter-bolt\"          # The stream consumer\r       grouping:                    # Grouping type\r         type: SHUFFLE\r     \r     - name: \"Splitter -> Counter\"\r       from: \"splitter-bolt\"\r       to: \"counter-bolt\"\r       grouping:\r         type: FIELDS\r         args: [\"word\"]           # field(s) to group on\r     ```\r \r 3. 对 `pom.xml` 文件进行以下更改。\r \r    * 在 `<dependencies>` 节中添加以下新依赖关系：\r \r         ```xml\r         <!-- Add a dependency on the Flux framework -->\r         <dependency>\r             <groupId>org.apache.storm</groupId>\r             <artifactId>flux-core</artifactId>\r             <version>${storm.version}</version>\r         </dependency>\r         ```\r    * 将以下插件添加到 `<plugins>` 节。 此插件处理项目包（jar 文件）的创建，并在创建包时应用一些特定于 Flux 的转换。\r \r         ```xml\r         <!-- build an uber jar -->\r         <plugin>\r             <groupId>org.apache.maven.plugins</groupId>\r             <artifactId>maven-shade-plugin</artifactId>\r             <version>2.3</version>\r             <configuration>\r                 <transformers>\r                     <!-- Keep us from getting a \"can't overwrite file error\" -->\r                     <transformer implementation=\"org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer\" />\r                     <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\" />\r                     <!-- We're using Flux, so refer to it as main -->\r                     <transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\r                         <mainClass>org.apache.storm.flux.Flux</mainClass>\r                     </transformer>\r                 </transformers>\r                 <!-- Keep us from getting a bad signature error -->\r                 <filters>\r                     <filter>\r                         <artifact>*:*</artifact>\r                         <excludes>\r                             <exclude>META-INF/*.SF</exclude>\r                             <exclude>META-INF/*.DSA</exclude>\r                             <exclude>META-INF/*.RSA</exclude>\r                         </excludes>\r                     </filter>\r                 </filters>\r             </configuration>\r             <executions>\r                 <execution>\r                     <phase>package</phase>\r                     <goals>\r                         <goal>shade</goal>\r                     </goals>\r                 </execution>\r             </executions>\r         </plugin>\r         ```\r \r    * 在 **exec-maven-plugin** `<configuration>` 节中，将 `<mainClass>` 的值更改为 `org.apache.storm.flux.Flux`。 在开发环境中本地运行拓扑时，Flux 可以使用此设置处理拓扑运行。\r \r    * 将以下内容添加到 `<resources>` 节中的 `<includes>`。 此 XML 包括了将拓扑定义为项目一部分的 YAML 文件。\r \r         ```xml\r         <include>topology.yaml</include>\r         ```\r \r ## <a name=\"test-the-flux-topology-locally\"></a>在本地测试 Flux 拓扑\r \r 1. 使用以下命令通过 Maven 编译并执行 Flux 拓扑：\r \r     ```bash\r     mvn compile exec:java -Dexec.args=\"--local -R /topology.yaml\"\r     ```\r \r     如果使用的是 PowerShell，请使用以下命令：\r \r     ```bash\r     mvn compile exec:java \"-Dexec.args=--local -R /topology.yaml\"\r     ```\r \r     > [!WARNING]\r     > 如果拓扑使用 Storm 1.0.1 位，此命令会失败。 此失败是由 [https://issues.apache.org/jira/browse/STORM-2055](https://issues.apache.org/jira/browse/STORM-2055) 引起的。 相反，在开发环境中安装 Storm ，并按照以下步骤操作：\r     >\r     > 如果已在开发环境中安装 Storm，则可以改用以下命令：\r     >\r     > ```bash\r     > mvn compile package\r     > storm jar target/WordCount-1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local -R /topology.yaml\r     > ```\r \r     `--local` 参数在开发环境中以本地模式运行拓扑。 `-R /topology.yaml` 参数使用 jar 文件中的 `topology.yaml` 文件资源来定义拓扑。\r \r     运行该命令时，拓扑显示启动信息。 以下文本是输出的示例：\r \r         17:33:27 [Thread-12-count] INFO  com.microsoft.example.WordCount - Emitting a count of 56 for word snow\r         17:33:27 [Thread-12-count] INFO  com.microsoft.example.WordCount - Emitting a count of 56 for word white\r         17:33:27 [Thread-12-count] INFO  com.microsoft.example.WordCount - Emitting a count of 112 for word seven\r         17:33:27 [Thread-16-count] INFO  com.microsoft.example.WordCount - Emitting a count of 195 for word the\r         17:33:27 [Thread-30-count] INFO  com.microsoft.example.WordCount - Emitting a count of 113 for word and\r         17:33:27 [Thread-30-count] INFO  com.microsoft.example.WordCount - Emitting a count of 57 for word dwarfs\r \r     不同批次的日志记录信息之间存在 10 秒的延迟。\r \r 2. 从项目创建 `topology.yaml` 文件的副本。 将新文件命名为 `newtopology.yaml`。 在 `newtopology.yaml` 文件中，找到以下节，将 `10` 的值更改为 `5`。 此修改会将发出单词计数批的间隔时间从 10 秒更改为 5 秒。\r \r     ```yaml\r     - id: \"counter-bolt\"\r     className: \"com.microsoft.example.WordCount\"\r     constructorArgs:\r     - 5\r     parallelism: 1\r     ```\r \r 3. To run the topology, use the following command:\r \r     ```bash\r     mvn exec:java -Dexec.args=\"--local /path/to/newtopology.yaml\"\r     ```\r \r     或者，如果开发环境中有 Storm，则执行以下操作：\r \r     ```bash\r     storm jar target/WordCount-1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local /path/to/newtopology.yaml\r     ```\r \r     将 `/path/to/newtopology.yaml` 更改为前一步骤中创建的 newtopology.yaml 文件的路径。 此命令使用 newtopology.yaml 作为拓扑定义。 由于没有包含 `compile` 参数，Maven 使用前面步骤中生成的项目的版本。\r \r     启动拓扑后，应会发现发出批的间隔时间已发生更改，反映 newtopology.yaml 中的值。 因此可以看到，无需重新编译拓扑即可通过 YAML 文件更改配置。\r \r ## <a name=\"trident\"></a>Trident\r \r Trident 是 Storm 提供的高级抽象。 它支持有状态处理。 Trident 的主要优点在于，它可以保证进入拓扑的每个消息只会处理一次。 如果不使用 Trident，则拓扑只能保证至少将消息处理一次。 两者还有其他方面的差异，例如，可以使用内置组件，而无需创建 Bolt。 事实上，可以使用低泛型组件（例如筛选、投影和函数）来取代 Bolt。\r \r 可以使用 Maven 项目来创建 Trident 应用程序。 使用本文前面所述的相同基本步骤 - 只有代码不同。 Trident（目前）还不能与 Flux 框架配合使用。\r \r 有关 Trident 的详细信息，请参阅 [Trident API 概述](http://storm.apache.org/documentation/Trident-API-Overview.html)。\r \r ## <a name=\"next-steps\"></a>后续步骤\r \r 已学习如何使用 Java 创建 Storm 拓扑。 接下来，请学习如何：\r \r * [在 HDInsight 上部署和管理 Apache Storm 拓扑](apache-storm-deploy-monitor-topology.md)\r \r * [使用 Visual Studio 开发 Apache Storm on HDInsight 的 C# 拓扑](apache-storm-develop-csharp-visual-studio-topology.md)\r \r 如需更多 Storm 拓扑示例，请访问 [Storm on HDInsight 示例拓扑](apache-storm-example-topology.md)。\r \r <!--Update_Description: update meta data-->"}