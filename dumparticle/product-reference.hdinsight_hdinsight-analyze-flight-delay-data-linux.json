{"Title":"在 HDInsight 上使用 Hive 分析航班延误数据 - Azure","Description":"了解如何在基于 Linux 的 HDInsight 上使用 Hive 分析航班数据，然后使用 Sqoop 将数据导出到 SQL 数据库。","Content":"# <a name=\"analyze-flight-delay-data-by-using-hive-on-linux-based-hdinsight\"></a>在基于 Linux 的 HDInsight 上使用 Hive 分析航班延误数据\r \r 了解如何在基于 Linux 的 HDInsight 上使用 Hive 分析航班延误数据，以及如何使用 Sqoop 将数据导出到 Azure SQL 数据库。\r \r > [!IMPORTANT]\r > 本文档中的步骤需要使用 Linux 的 HDInsight 群集。 Linux 是 Azure HDInsight 3.4 或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 在 Windows 上停用](hdinsight-component-versioning.md#hdinsight-windows-retirement)。\r \r ## <a name=\"prerequisites\"></a>先决条件\r \r * **HDInsight 群集**。 有关创建新的基于 Linux 的 HDInsight 群集的步骤，请参阅[开始在 HDInsight 中使用 Hadoop](hadoop/apache-hadoop-linux-tutorial-get-started.md)。\r \r * **Azure SQL 数据库**。 使用 Azure SQL 数据库作为目标数据存储。 如果没有 SQL 数据库，请参阅[在 Azure 门户中创建 Azure SQL 数据库](../sql-database/sql-database-get-started.md)。\r \r * **Azure CLI**。 如果尚未安装 Azure CLI，请参阅[安装 Azure CLI 1.0](../cli-install-nodejs.md)了解详细步骤。\r \r ## <a name=\"download-the-flight-data\"></a>下载航班数据\r \r 1. 浏览到 [美国研究与技术创新管理部门 - 运输统计局][rita-website]。\r \r 2. 在该页面上，选择以下值：\r \r    | 名称 | 值 |\r    | --- | --- |\r    | 筛选年份 |2013 |\r    | 筛选期间 |1 月 |\r    | 字段 |Year、FlightDate、UniqueCarrier、Carrier、FlightNum、OriginAirportID、Origin、OriginCityName、OriginState、DestAirportID、Dest、DestCityName、DestState、DepDelayMinutes、ArrDelay、ArrDelayMinutes、CarrierDelay、WeatherDelay、NASDelay、SecurityDelay、LateAircraftDelay。 |\r    清除所有其他字段。 \r \r 3. 选择“下载”。\r \r ## <a name=\"upload-the-data\"></a>上传数据\r \r 1. 使用以下命令将 zip 文件上传到 HDInsight 群集头节点：\r \r     ```\r     scp FILENAME.zip USERNAME@CLUSTERNAME-ssh.azurehdinsight.cn:\r     ```\r \r     将“FILENAME”替换为 zip 文件的名称。 将 *USERNAME* 替换为 HDInsight 群集的 SSH 登录名。 将 *CLUSTERNAME* 替换为 HDInsight 群集的名称。\r \r    > [!NOTE]\r    > 如果使用密码对 SSH 登录名进行身份验证，系统会提示输入密码。 如果使用公钥，可能需要使用 `-i` 参数并指定匹配私钥的路径。 例如，`scp -i ~/.ssh/id_rsa FILENAME.zip USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:`。\r \r 2. 上传完成后，使用 SSH 连接到群集：\r \r     ```ssh USERNAME@CLUSTERNAME-ssh.azurehdinsight.cn```\r \r     有关详细信息，请参阅[使用 SSH 连接到 HDInsight (Hadoop)](hdinsight-hadoop-linux-use-ssh-unix.md)。\r \r 3. 使用以下命令解压缩 .zip 文件：\r \r     ```\r     unzip FILENAME.zip\r     ```\r \r     此命令会提取约为 60 MB 的 .csv 文件。\r \r 4. 使用以下命令在 HDInsight 存储上创建一个目录，并将该文件复制到此目录：\r \r     ```\r     hdfs dfs -mkdir -p /tutorials/flightdelays/data\r     hdfs dfs -put FILENAME.csv /tutorials/flightdelays/data/\r     ```\r \r ## <a name=\"create-and-run-the-hiveql\"></a>创建并运行 HiveQL\r \r 使用以下步骤将 .csv 文件中的数据导入到名为“Delays”的 Hive 表中。\r \r 1. 使用以下命令创建名为 **flightdelays.hql**的新文件并编辑它：\r \r     ```\r     nano flightdelays.hql\r     ```\r \r     将以下文本用作此文件的内容：\r \r     ```hiveql\r     DROP TABLE delays_raw;\r     -- Creates an external table over the csv file\r     CREATE EXTERNAL TABLE delays_raw (\r         YEAR string,\r         FL_DATE string,\r         UNIQUE_CARRIER string,\r         CARRIER string,\r         FL_NUM string,\r         ORIGIN_AIRPORT_ID string,\r         ORIGIN string,\r         ORIGIN_CITY_NAME string,\r         ORIGIN_CITY_NAME_TEMP string,\r         ORIGIN_STATE_ABR string,\r         DEST_AIRPORT_ID string,\r         DEST string,\r         DEST_CITY_NAME string,\r         DEST_CITY_NAME_TEMP string,\r         DEST_STATE_ABR string,\r         DEP_DELAY_NEW float,\r         ARR_DELAY_NEW float,\r         CARRIER_DELAY float,\r         WEATHER_DELAY float,\r         NAS_DELAY float,\r         SECURITY_DELAY float,\r         LATE_AIRCRAFT_DELAY float)\r     -- The following lines describe the format and location of the file\r     ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\r     LINES TERMINATED BY '\\n'\r     STORED AS TEXTFILE\r     LOCATION '/tutorials/flightdelays/data';\r \r     -- Drop the delays table if it exists\r     DROP TABLE delays;\r     -- Create the delays table and populate it with data\r     -- pulled in from the CSV file (via the external table defined previously)\r     CREATE TABLE delays AS\r     SELECT YEAR AS year,\r         FL_DATE AS flight_date,\r         substring(UNIQUE_CARRIER, 2, length(UNIQUE_CARRIER) -1) AS unique_carrier,\r         substring(CARRIER, 2, length(CARRIER) -1) AS carrier,\r         substring(FL_NUM, 2, length(FL_NUM) -1) AS flight_num,\r         ORIGIN_AIRPORT_ID AS origin_airport_id,\r         substring(ORIGIN, 2, length(ORIGIN) -1) AS origin_airport_code,\r         substring(ORIGIN_CITY_NAME, 2) AS origin_city_name,\r         substring(ORIGIN_STATE_ABR, 2, length(ORIGIN_STATE_ABR) -1)  AS origin_state_abr,\r         DEST_AIRPORT_ID AS dest_airport_id,\r         substring(DEST, 2, length(DEST) -1) AS dest_airport_code,\r         substring(DEST_CITY_NAME,2) AS dest_city_name,\r         substring(DEST_STATE_ABR, 2, length(DEST_STATE_ABR) -1) AS dest_state_abr,\r         DEP_DELAY_NEW AS dep_delay_new,\r         ARR_DELAY_NEW AS arr_delay_new,\r         CARRIER_DELAY AS carrier_delay,\r         WEATHER_DELAY AS weather_delay,\r         NAS_DELAY AS nas_delay,\r         SECURITY_DELAY AS security_delay,\r         LATE_AIRCRAFT_DELAY AS late_aircraft_delay\r     FROM delays_raw;\r     ```\r \r 2. 若要保存该文件，请按 Ctrl+X、Y。\r \r 3. 若要启动 Hive 并运行 **flightdelays.hql** 文件，请使用以下命令：\r \r     ```\r     beeline -u 'jdbc:hive2://localhost:10001/;transportMode=http' -f flightdelays.hql\r     ```\r \r 4. flightdelays.hql 脚本完成运行后，使用以下命令打开交互式 Beeline 会话：\r \r     ```\r     beeline -u 'jdbc:hive2://localhost:10001/;transportMode=http'\r     ```\r \r 5. 收到 `jdbc:hive2://localhost:10001/>` 提示时，使用以下查询从导入的航班延误数据中检索数据：\r \r     ```hiveql\r     INSERT OVERWRITE DIRECTORY '/tutorials/flightdelays/output'\r     ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\r     SELECT regexp_replace(origin_city_name, '''', ''),\r         avg(weather_delay)\r     FROM delays\r     WHERE weather_delay IS NOT NULL\r     GROUP BY origin_city_name;\r     ```\r \r     此查询会检索遇到天气延迟的城市的列表以及平均延迟时间，并将其保存到 `/tutorials/flightdelays/output` 中。 稍后，Sqoop 会从该位置读取数据并将其导出到 Azure SQL 数据库。\r \r 6. 若要退出 Beeline，请在提示符处输入 `!quit` 。\r \r ## <a name=\"create-a-sql-database\"></a>创建 SQL 数据库\r \r 如果已拥有 SQL 数据库，必须获取服务器的名称。 要在 [Azure 门户](https://portal.azure.cn)中查找服务器名称，请选择“SQL 数据库”，然后筛选要使用的数据库的名称。 服务器名称在“SERVER”  列中列出。\r \r 如果没有 SQL 数据库，请使用[在 Azure 门户中创建 Azure SQL 数据库](../sql-database/sql-database-get-started.md)中的信息进行创建。 保存数据库所使用的服务器名称。\r \r ## <a name=\"create-a-sql-database-table\"></a>创建 SQL 数据库表\r \r > [!NOTE]\r > 可通过多种方式连接到 SQL 数据库并创建表。 以下步骤从 HDInsight 群集使用 [FreeTDS](http://www.freetds.org/) 。\r \r 1. 使用 SSH 连接到基于 Linux 的 HDInsight 群集，并从 SSH 会话运行以下步骤。\r \r 2. 使用以下命令安装 FreeTDS：\r \r     ```\r     sudo apt-get --assume-yes install freetds-dev freetds-bin\r     ```\r \r 3. 安装完成后，使用以下命令连接到 SQL 数据库服务器。 使用 SQL 数据库服务器名称替换 **serverName**。 使用 SQL 数据库登录信息替换 **adminLogin** 和 **adminPassword**。 使用数据库名称替换 **databaseName**。\r \r     ```\r     TDSVER=8.0 tsql -H <serverName>.database.chinacloudapi.cn -U <adminLogin> -P <adminPassword> -p 1433 -D <databaseName>\r     ```\r \r     你会收到类似于以下文本的输出：\r \r     ```\r     locale is \"en_US.UTF-8\"\r     locale charset is \"UTF-8\"\r     using default charset \"UTF-8\"\r     Default database being set to sqooptest\r     1>\r     ```\r \r 4. 在 `1>` 提示符下，输入以下行：\r \r     ```\r     CREATE TABLE [dbo].[delays](\r     [origin_city_name] [nvarchar](50) NOT NULL,\r     [weather_delay] float,\r     CONSTRAINT [PK_delays] PRIMARY KEY CLUSTERED   \r     ([origin_city_name] ASC))\r     GO\r     ```\r \r     输入 `GO` 语句后，将评估前面的语句。 此查询会创建一个名为 **delays** 且具有聚集索引的表。\r \r     使用以下查询验证是否已创建该表：\r \r     ```\r     SELECT * FROM information_schema.tables\r     GO\r     ```\r \r     输出类似于以下文本：\r \r     ```\r     TABLE_CATALOG   TABLE_SCHEMA    TABLE_NAME      TABLE_TYPE\r     databaseName       dbo     delays      BASE TABLE\r     ```\r \r 5. 在 `exit` at the `1>` 以退出 tsql 实用工具。\r \r ## <a name=\"export-data-with-sqoop\"></a>使用 Sqoop 导出数据\r \r 1. 使用以下命令验证 Sqoop 是否可以看到 SQL 数据库：\r \r     ```\r     sqoop list-databases --connect jdbc:sqlserver://<serverName>.database.chinacloudapi.cn:1433 --username <adminLogin> --password <adminPassword>\r     ```\r \r     此命令会返回数据库列表，其中包括此前创建的 delays 表所在的数据库。\r \r 2. 使用以下命令将 hivesampletable 中的数据导出到 delays 表：\r \r     ```\r     sqoop export --connect 'jdbc:sqlserver://<serverName>.database.chinacloudapi.cn:1433;database=<databaseName>' --username <adminLogin> --password <adminPassword> --table 'delays' --export-dir '/tutorials/flightdelays/output' --fields-terminated-by '\\t' -m 1\r     ```\r \r     Sqoop 连接到包含 delays 表的数据库，并将数据从 `/tutorials/flightdelays/output` 目录导出到 delays 表。\r \r 3. Sqoop 命令完成后，使用 tsql 实用工具连接到数据库：\r \r     ```\r     TDSVER=8.0 tsql -H <serverName>.database.chinacloudapi.cn -U <adminLogin> -P <adminPassword> -p 1433 -D <databaseName>\r     ```\r \r     使用以下语句验证数据是否已导出到 delays 表：\r \r     ```\r     SELECT * FROM delays\r     GO\r     ```\r \r     会在表中看到一系列数据。 键入 `exit` 退出 tsql 实用程序。\r \r ## <a name=\"next-steps\"></a>后续步骤\r \r 若要了解使用 HDInsight 中的数据的更多方式，请参阅以下文章：\r \r * [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]\r * [将 Oozie 与 HDInsight 配合使用][hdinsight-use-oozie]\r * [将 Sqoop 与 HDInsight 配合使用][hdinsight-use-sqoop]\r * [将 Pig 与 HDInsight 配合使用][hdinsight-use-pig]\r * [为 HDInsight 上的 Hadoop 开发 Java MapReduce 程序][hdinsight-develop-mapreduce]\r * [为 HDInsight 开发 Python 流式处理 MapReduce 程序][hdinsight-develop-streaming]\r \r [azure-purchase-options]: https://www.azure.cn/pricing/overview/\r [azure-member-offers]: https://www.azure.cn/pricing/member-offers/\r [azure-trial]: https://www.azure.cn/pricing/1rmb-trial/\r \r [rita-website]: http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time\r [cindygross-hive-tables]: http://blogs.msdn.com/b/cindygross/archive/2013/02/06/hdinsight-hive-internal-and-external-tables-intro.aspx\r \r [hdinsight-use-oozie]: hdinsight-use-oozie-linux-mac.md\r [hdinsight-use-hive]:hadoop/hdinsight-use-hive.md\r [hdinsight-provision]: hdinsight-hadoop-provision-linux-clusters.md\r [hdinsight-storage]: hdinsight-hadoop-use-blob-storage.md\r [hdinsight-upload-data]: hdinsight-upload-data.md\r [hdinsight-get-started]: hadoop/apache-hadoop-linux-tutorial-get-started.md\r [hdinsight-use-sqoop]:hadoop/apache-hadoop-use-sqoop-mac-linux.md\r [hdinsight-use-pig]:hadoop/hdinsight-use-pig.md\r [hdinsight-develop-streaming]:hadoop/apache-hadoop-streaming-python.md\r [hdinsight-develop-mapreduce]:hadoop/apache-hadoop-develop-deploy-java-mapreduce-linux.md\r \r [hadoop-hiveql]: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL\r \r [technetwiki-hive-error]: http://social.technet.microsoft.com/wiki/contents/articles/23047.hdinsight-hive-error-unable-to-rename.aspx\r <!--Update_Description: update code-->"}