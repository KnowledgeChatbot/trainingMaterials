{"Title":"使用 HDInsight 中的 Hadoop 分析航班延误数据 - Azure","Description":"了解如何使用单个 Windows PowerShell 脚本来创建 HDInsight 群集、运行 Hive 作业、运行 Sqoop 作业和删除群集。","Content":"# <a name=\"analyze-flight-delay-data-by-using-hive-in-hdinsight\"></a>使用 HDInsight 中的 Hive 分析航班延误数据\r \r [!INCLUDE [azure-sdk-developer-differences](../../includes/azure-sdk-developer-differences.md)]\r \r Hive 提供了通过类似 SQL 的脚本语言（称为 [HiveQL][hadoop-hiveql]）运行 Hadoop MapReduce 作业的方法，此方法可用于对大量数据进行汇总、查询和分析。\r \r > [!IMPORTANT]\r > 本文档中的步骤要求使用基于 Windows 的 HDInsight 群集。 Linux 是在 HDInsight 3.4 版或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 在 Windows 上停用](hdinsight-component-versioning.md#hdinsight-windows-retirement)。 有关适用于基于 Linux 的群集的步骤，请参阅[在 HDInsight (Linux) 中使用 Hive 分析航班延误数据](hdinsight-analyze-flight-delay-data-linux.md)。\r \r Azure HDInsight 的主要优势之一就是隔离数据存储和计算。 HDInsight 将 Azure Blob 存储用于数据存储。 典型的作业包含三部分：\r \r 1. **将数据存储在 Azure Blob 存储中。**  例如，将天气数据、传感器数据、Web 日志以及此示例中的航班延误数据保存到 Azure Blob 存储中。\r 2. **运行作业。** 要处理数据时，可以运行 Windows PowerShell 脚本（或客户端应用程序）创建 HDInsight 群集、运行作业，然后删除该群集。 作业将输出数据保存到 Azure Blob 存储。 甚至在删除该群集后，输出数据也会保留。 这样，仅为已使用的内容付费。\r 3. **从 Azure Blob 存储检索输出**，或在此教程中将数据导出到 Azure SQL 数据库。\r \r 下图演示了本教程的方案和结构：\r \r ![HDI.FlightDelays.flow][img-hdi-flightdelays-flow]\r \r 请注意，图中的编号对应于章节标题。 **M** 代表主进程。 **A** 代表附录中的内容。\r \r 本教程的主要部分说明如何使用单个 Windows PowerShell 脚本执行以下任务：\r \r * 创建 HDInsight 群集。\r * 在群集上运行 Hive 作业，以计算机场的平均延迟。 航班延误数据会存储在 Azure Blob 存储帐户中。\r * 运行 Sqoop 作业将 Hive 作业输出导出至 Azure SQL 数据库。\r * 删除 HDInsight 群集。\r \r 在附录中，你可以找到有关上传航班延误数据、创建/上传 Hive 查询字符串和针对 Sqoop 作业准备 Azure SQL 数据库的说明。\r \r > [!NOTE]\r > 本文档中的步骤特定于基于 Windows 的 HDInsight 群集。 有关适用于基于 Linux 的群集的步骤，请参阅[在 HDInsight (Linux) 中使用 Hive 分析航班延误数据](hdinsight-analyze-flight-delay-data-linux.md)\r \r ### <a name=\"prerequisites\"></a>先决条件\r 要阅读本教程，必须具备以下项：\r \r * **一个 Azure 订阅**。 请参阅[获取 Azure 试用版](https://www.azure.cn/pricing/1rmb-trial/)。\r * **配备 Azure PowerShell 的工作站**。\r \r     > [!IMPORTANT]\r     > 使用 Azure Service Manager 管理 HDInsight 资源的 Azure PowerShell 支持**已弃用**，已在 2017 年 1 月 1 日删除。 本文档中的步骤使用的是与 Azure Resource Manager 兼容的新 HDInsight cmdlet。\r     >\r     > 请按照 [Install and configure Azure PowerShell](https://docs.microsoft.com/powershell/azureps-cmdlets-docs) （安装和配置 Azure PowerShell）中的步骤安装最新版本的 Azure PowerShell。 如果脚本需要修改才能使用与 Azure Resource Manager 兼容的新 cmdlet，请参阅[迁移到适用于 HDInsight 群集的基于 Azure Resource Manager 的开发工具](hdinsight-hadoop-development-using-azure-resource-manager.md)，了解详细信息。\r \r **本教程中使用的文件**\r \r 本教程使用来自 [美国研究与技术创新管理部门 - 运输统计局或 RITA][rita-website]的航班准时表现数据。\r 数据的副本已上传至具有公共 Blob 访问权限的 Azure Blob 存储容器。\r PowerShell 脚本的一部分将数据从公共 blob 容器复制到群集的默认 blob 容器。 HiveQL 脚本也会复制到同一 Blob 容器。\r 如果想要了解如何将数据获取/上传到自己的存储帐户，以及如何创建/上传 HiveQL 脚本文件，请参阅[附录 A](#appendix-a) 和[附录 B](#appendix-b)。\r \r 下表列出了本教程中使用的文件：\r \r <table border=\"1\">\r <tr><th>文件</th><th>说明</th></tr>\r <tr><td>wasb://flightdelay@hditutorialdata.blob.core.windows.net/flightdelays.hql</td><td>Hive 作业所用的 HiveQL 脚本文件。 此脚本已上传到具有公共访问权限的 Azure Blob 存储帐户。 <a href=\"#appendix-b\">附录 B</a> 提供了有关准备此文件以及将其上传到用户的 Azure Blob 存储帐户的说明。</td></tr>\r <tr><td>wasb://flightdelay@hditutorialdata.blob.core.windows.net/2013Data</td><td>Hive 作业的输入数据。 这些数据已上传到具有公共访问权限的 Azure Blob 存储帐户。 <a href=\"#appendix-a\">附录 A</a> 提供了有关获取数据以及将数据上传到用户的 Azure Blob 存储帐户的说明。</td></tr>\r <tr><td>\\tutorials\\flightdelays\\output</td><td>Hive 作业的输出路径。 默认容器用于存储输出数据。</td></tr>\r <tr><td>\\tutorials\\flightdelays\\jobstatus</td><td>默认容器上的 Hive 作业状态文件夹。</td></tr>\r </table>\r \r ## <a name=\"create-cluster-and-run-hivesqoop-jobs\"></a>创建群集并运行 Hive/Sqoop 作业\r Hadoop MapReduce 属于批处理。 运行 Hive 作业时，最具成本效益的方法是为作业创建群集，并在作业完成之后删除作业。 以下脚本覆盖了整个过程。\r 有关创建 HDInsight 群集和运行 Hive 作业的详细信息，请参阅[在 HDInsight 中创建 Hadoop 群集][hdinsight-provision]和[将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]。\r \r **使用 Azure PowerShell 运行 Hive 查询**\r \r 1. 按照 [附录 C](#appendix-c)中的说明，为 Sqoop 作业输出创建 Azure SQL 数据库和表。\r 2. 打开 Windows PowerShell ISE 并运行以下脚本：\r \r     ```powershell\r     $subscriptionID = \"<Azure Subscription ID>\"\r     $nameToken = \"<Enter an Alias>\"\r \r     ###########################################\r     # You must configure the follwing variables\r     # for an existing Azure SQL Database\r     ###########################################\r     $existingSqlDatabaseServerName = \"<Azure SQL Database Server>\"\r     $existingSqlDatabaseLogin = \"<Azure SQL Database Server Login>\"\r     $existingSqlDatabasePassword = \"<Azure SQL Database Server login password>\"\r     $existingSqlDatabaseName = \"<Azure SQL Database name>\"\r \r     $localFolder = \"E:\\Tutorials\\Downloads\\\" # A temp location for copying files.\r     $azcopyPath = \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\" # depends on the version, the folder can be different\r \r     ###########################################\r     # (Optional) configure the following variables\r     ###########################################\r \r     $namePrefix = $nameToken.ToLower() + (Get-Date -Format \"MMdd\")\r \r     $resourceGroupName = $namePrefix + \"rg\"\r     $location = \"CHINA EAST\"\r \r     $HDInsightClusterName = $namePrefix + \"hdi\"\r     $httpUserName = \"admin\"\r     $httpPassword = \"<Enter the Password>\"\r \r     $defaultStorageAccountName = $namePrefix + \"store\"\r     $defaultBlobContainerName = $HDInsightClusterName # use the cluster name\r \r     $existingSqlDatabaseTableName = \"AvgDelays\"\r     $sqlDatabaseConnectionString = \"jdbc:sqlserver://$existingSqlDatabaseServerName.database.chinacloudapi.cn;user=$existingSqlDatabaseLogin@$existingSqlDatabaseServerName;password=$existingSqlDatabaseLogin;database=$existingSqlDatabaseName\"\r \r     $hqlScriptFile = \"/tutorials/flightdelays/flightdelays.hql\"\r \r     $jobStatusFolder = \"/tutorials/flightdelays/jobstatus\"\r \r     ###########################################\r     # Login\r     ###########################################\r     try{\r         $acct = Get-AzureRmSubscription\r     }\r     catch{\r         Login-AzureRmAccount -EnvironmentName AzureChinaCloud\r     }\r     Select-AzureRmSubscription -SubscriptionID $subscriptionID\r \r     ###########################################\r     # Create a new HDInsight cluster\r     ###########################################\r \r     # Create ARM group\r     New-AzureRmResourceGroup -Name $resourceGroupName -Location $location\r \r     # Create the default storage account\r     New-AzureRmStorageAccount -ResourceGroupName $resourceGroupName -Name $defaultStorageAccountName -Location $location -Type Standard_LRS\r \r     # Create the default Blob container\r     $defaultStorageAccountKey = (Get-AzureRmStorageAccountKey -ResourceGroupName $resourceGroupName -Name $defaultStorageAccountName)[0].Value\r     $defaultStorageAccountContext = New-AzureStorageContext -StorageAccountName $defaultStorageAccountName -StorageAccountKey $defaultStorageAccountKey\r     New-AzureStorageContainer -Name $defaultBlobContainerName -Context $defaultStorageAccountContext\r \r     # Create the HDInsight cluster\r     $pw = ConvertTo-SecureString -String $httpPassword -AsPlainText -Force\r     $httpCredential = New-Object System.Management.Automation.PSCredential($httpUserName,$pw)\r \r     New-AzureRmHDInsightCluster `\r         -ResourceGroupName $resourceGroupName `\r         -ClusterName $HDInsightClusterName `\r         -Location $location `\r         -ClusterType Hadoop `\r         -OSType Windows `\r         -ClusterSizeInNodes 2 `\r         -HttpCredential $httpCredential `\r         -DefaultStorageAccountName \"$defaultStorageAccountName.blob.core.chinacloudapi.cn\" `\r         -DefaultStorageAccountKey $defaultStorageAccountKey `\r         -DefaultStorageContainer $existingDefaultBlobContainerName\r \r     ###########################################\r     # Prepare the HiveQL script and source data\r     ###########################################\r \r     # Create the temp location\r     New-Item -Path $localFolder -ItemType Directory -Force\r \r     # Download the sample file from Azure Blob storage\r     $context = New-AzureStorageContext -StorageAccountName \"hditutorialdata\" -Anonymous\r     $blobs = Get-AzureStorageBlob -Container \"flightdelay\" -Context $context\r     #$blobs | Get-AzureStorageBlobContent -Context $context -Destination $localFolder\r \r     # Upload data to default container\r \r     $azcopycmd = \"cmd.exe /C '$azcopyPath\\azcopy.exe' /S /Source:'$localFolder' /Dest:'https://$defaultStorageAccountName.blob.core.chinacloudapi.cn/$defaultBlobContainerName/tutorials/flightdelays' /DestKey:$defaultStorageAccountKey\"\r \r     Invoke-Expression -Command:$azcopycmd\r \r     ###########################################\r     # Submit the Hive job\r     ###########################################\r     Use-AzureRmHDInsightCluster -ClusterName $HDInsightClusterName -HttpCredential $httpCredential\r     $response = Invoke-AzureRmHDInsightHiveJob `\r                     -Files $hqlScriptFile `\r                     -DefaultContainer $defaultBlobContainerName `\r                     -DefaultStorageAccountName $defaultStorageAccountName `\r                     -DefaultStorageAccountKey $defaultStorageAccountKey `\r                     -StatusFolder $jobStatusFolder\r \r     write-Host $response\r \r     ###########################################\r     # Submit the Sqoop job\r     ###########################################\r     $exportDir = \"wasb://$defaultBlobContainerName@$defaultStorageAccountName.blob.core.chinacloudapi.cn/tutorials/flightdelays/output\"\r \r     $sqoopDef = New-AzureRmHDInsightSqoopJobDefinition `\r                     -Command \"export --connect $sqlDatabaseConnectionString --table $sqlDatabaseTableName --export-dir $exportDir --fields-terminated-by \\001 \"\r     $sqoopJob = Start-AzureRmHDInsightJob `\r                     -ResourceGroupName $resourceGroupName `\r                     -ClusterName $hdinsightClusterName `\r                     -HttpCredential $httpCredential `\r                     -JobDefinition $sqoopDef #-Debug -Verbose\r \r     Wait-AzureRmHDInsightJob `\r         -ResourceGroupName $resourceGroupName `\r         -ClusterName $HDInsightClusterName `\r         -HttpCredential $httpCredential `\r         -WaitTimeoutInSeconds 3600 `\r         -Job $sqoopJob.JobId\r \r     Get-AzureRmHDInsightJobOutput `\r         -ResourceGroupName $resourceGroupName `\r         -ClusterName $hdinsightClusterName `\r         -HttpCredential $httpCredential `\r         -DefaultContainer $existingDefaultBlobContainerName `\r         -DefaultStorageAccountName $defaultStorageAccountName `\r         -DefaultStorageAccountKey $defaultStorageAccountKey `\r         -JobId $sqoopJob.JobId `\r         -DisplayOutputType StandardError\r \r     ###########################################\r     # Delete the cluster\r     ###########################################\r     Remove-AzureRmHDInsightCluster -ResourceGroupName $resourceGroupName -ClusterName $hdinsightClusterName\r     ```\r 3. 连接到 SQL 数据库，并在 AvgDelays 表中按城市查看平均航班延迟：\r \r     ![HDI.FlightDelays.AvgDelays.Dataset][image-hdi-flightdelays-avgdelays-dataset]\r \r - - -\r \r ## <a id=\"appendix-a\"></a>附录 A - 将航班延迟数据上传到 Azure Blob 存储\r 上传数据文件和 HiveQL 脚本文件（请参阅[附录 B](#appendix-b)）需要进行规划。 思路是在创建 HDInsight 群集和运行 Hive 作业之前存储数据文件和 HiveQL 文件。 可以使用两个选项：\r \r * \r             **使用由 HDInsight 群集用作默认文件系统的同一 Azure 存储帐户。** 由于 HDInsight 群集将具有存储帐户访问密钥，因此你无需进行任何其他更改。\r * **使用与 HDInsight 群集默认文件系统不同的 Azure 存储帐户。** 如果选择了此项，必须修改 [创建 HDInsight 群集和运行 Hive/Sqoop 作业](#runjob) 中的 Windows PowerShell 脚本的创建部分，以链接该存储帐户作为额外的存储帐户。 有关说明，请参阅 [在 HDInsight 中创建 Hadoop 群集][hdinsight-provision]。 这样，HDInsight 群集就会知道存储帐户的访问密钥。\r \r > [!NOTE]\r > 数据文件的 WASB 路径会在 HiveQL 脚本文件中进行硬编码。 必须相应地进行更新。\r \r **下载航班数据**\r \r 1. 浏览到 [美国研究与技术创新管理部门 - 运输统计局][rita-website]。\r 2. 在该页面上，选择以下值：\r \r     <table border=\"1\">\r     <tr><th>名称</th><th>值</th></tr>\r     <tr><td>筛选年份</td><td>2013 </td></tr>\r     <tr><td>筛选期间</td><td>1 月</td></tr>\r     <tr><td>字段</td><td>Year、FlightDate、UniqueCarrier、Carrier、FlightNum、OriginAirportID、Origin、OriginCityName、OriginState、DestAirportID、Dest、DestCityName、DestState、DepDelayMinutes、ArrDelay、ArrDelayMinutes、CarrierDelay、WeatherDelay、NASDelay、SecurityDelay 、LateAircraftDelay（清除其他所有字段）</td></tr>\r     </table>\r 3.单击“下载”****。\r 4. 将文件解压缩到 C:\\Tutorials\\FlightDelay\\2013Data 文件夹。 每个文件均为 CSV 文件且大小约为 60GB。\r 5. 将文件重命名为其包含的数据所对应的月份的名称。 例如，将包含 1 月份数据的文件命名为 *January.csv*。\r 6. 重复步骤 2 和步骤 5 为 2013 年中的 12 个月分别下载一个对应的文件。 完成本教程至少需要一个文件。\r \r **将航班延迟数据上传到 Azure Blob 存储**\r \r 1. 准备参数：\r \r     <table border=\"1\">\r     <tr><th>变量名</th><th>说明</th></tr>\r     <tr><td>$storageAccountName</td><td>数据上传的目标 Azure 存储帐户。</td></tr>\r     <tr><td>$blobContainerName</td><td>数据上传的目标 Blob 容器。</td></tr>\r     </table>\r 2. 打开 Azure PowerShell ISE。\r 3. 将以下脚本粘贴到脚本窗格中：\r \r     ```powershell\r     [CmdletBinding()]\r     Param(\r \r         [Parameter(Mandatory=$True,\r                     HelpMessage=\"Enter the Azure storage account name for creating a new HDInsight cluster. If the account doesn't exist, the script will create one.\")]\r         [String]$storageAccountName,\r \r         [Parameter(Mandatory=$True,\r                     HelpMessage=\"Enter the Azure blob container name for creating a new HDInsight cluster. If not specified, the HDInsight cluster name will be used.\")]\r         [String]$blobContainerName\r     )\r \r     #Region - Variables\r     $localFolder = \"C:\\Tutorials\\FlightDelay\\2013Data\"  # The source folder\r     $destFolder = \"tutorials/flightdelay/2013data\"     #The blob name prefix for the files to be uploaded\r     #EndRegion\r \r     #Region - Connect to Azure subscription\r     Write-Host \"`nConnecting to your Azure subscription ...\" -ForegroundColor Green\r     try{Get-AzureRmContext}\r     catch{Login-AzureRmAccount -EnvironmentName AzureChinaCloud}\r     #EndRegion\r \r     #Region - Validate user input\r     Write-Host \"`nValidating the Azure Storage account and the Blob container...\" -ForegroundColor Green\r     # Validate the Storage account\r     if (-not (Get-AzureRmStorageAccount|Where-Object{$_.StorageAccountName -eq $storageAccountName}))\r     {\r         Write-Host \"The storage account, $storageAccountName, doesn't exist.\" -ForegroundColor Red\r         exit\r     }\r     else{\r         $resourceGroupName = (Get-AzureRmStorageAccount|Where-Object{$_.StorageAccountName -eq $storageAccountName}).ResourceGroupName\r     }\r \r     # Validate the container\r     $storageAccountKey = (Get-AzureRmStorageAccountKey -StorageAccountName $storageAccountName -ResourceGroupName $resourceGroupName)[0].Value\r     $storageContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey\r \r     if (-not (Get-AzureStorageContainer -Context $storageContext |Where-Object{$_.Name -eq $blobContainerName}))\r     {\r         Write-Host \"The Blob container, $blobContainerName, doesn't exist\" -ForegroundColor Red\r         Exit\r     }\r     #EngRegion\r \r     #Region - Copy the file from local workstation to Azure Blob storage\r     if (test-path -Path $localFolder)\r     {\r         foreach ($item in Get-ChildItem -Path $localFolder){\r             $fileName = \"$localFolder\\$item\"\r             $blobName = \"$destFolder/$item\"\r \r             Write-Host \"Copying $fileName to $blobName\" -ForegroundColor Green\r \r             Set-AzureStorageBlobContent -File $fileName -Container $blobContainerName -Blob $blobName -Context $storageContext\r         }\r     }\r     else\r     {\r         Write-Host \"The source folder on the workstation doesn't exist\" -ForegroundColor Red\r     }\r \r     # List the uploaded files on HDInsight\r     Get-AzureStorageBlob -Container $blobContainerName  -Context $storageContext -Prefix $destFolder\r     #EndRegion\r     ```\r 4. 按 **F5** 运行脚本。\r \r 如果选择使用其他方法上传文件，请确保文件路径是 tutorials/flightdelay/data。 用于访问文件的语法是：\r \r     wasb://<ContainerName>@<StorageAccountName>.blob.core.chinacloudapi.cn/tutorials/flightdelay/data\r \r 路径 tutorials/flightdelay/data 是上传文件时创建的虚拟文件夹。 验证是否有 12 个文件，每个月对应一个文件。\r \r > [!NOTE]\r > 必须更新 Hive 查询，才能从新位置进行读取。\r >\r > 必须配置容器访问权限，使其成为公用，或者将存储帐户绑定到 HDInsight 群集。 否则，Hive 查询字符串将无法访问数据文件。\r \r - - -\r \r ## <a id=\"appendix-b\"></a>附录 B - 创建并上传 HiveQL 脚本\r 使用 Azure PowerShell，可一次运行多个 HiveQL 语句，或者将 HiveQL 语句打包到一个脚本文件中。 本部分说明如何创建 HiveQL 脚本，以及使用 Azure PowerShell 将脚本上传到 Azure Blob 存储。 Hive 要求 HiveQL 脚本必须存储在 Azure Blob 存储中。\r \r HiveQL 脚本将执行以下操作：\r \r 1. 删除 delays_raw 表（如果该表已存在）。\r 2. 创建 delays_raw 外部 Hive 表，并将该表指向航班延误文件所在的 Blob 存储位置。 此查询指定用“,”分隔字段并用“\\n”终止行。 这在字段值包含逗号时将导致出现问题，因为 Hive 无法区分逗号是字段分隔符还是字段值的一部分（在 ORIGIN\\_CITY\\_NAME 和 DEST\\_CITY\\_NAME 的字段值中属于此情况）。 为了解决此问题，此查询将创建 TEMP 列来保存未正确拆分到列中的数据。\r 3. **删除 delays 表**（如果该表已存在）。\r 4. **创建 delays 表**。 这适用于在进一步处理前清理数据。 此查询将从 delays_raw 表创建一个新表 delays。 请注意，将不会复制 TEMP 列（如前所述），并且将使用 substring 函数从数据中删除引号标记。\r 5. **计算平均天气延迟，并按城市名对结果进行分组。** 它还会将结果输出到 Blob 存储。 请注意，查询将从数据中删除撇号，并且将排除 weather_delay 的值为 null 的行。 由于本教程中稍后使用的 Sqoop 在默认情况下无法适当地处理这些值，因此这是必要的。\r \r 如需 HiveQL 命令的完整列表，请参阅 [Hive 数据定义语言][hadoop-hiveql]。 每条 HiveQL 命令必须以分号结尾。\r \r **创建 HiveQL 脚本文件**\r \r 1. 准备参数：\r \r     <table border=\"1\">\r     <tr><th>变量名</th><th>说明</th></tr>\r     <tr><td>$storageAccountName</td><td>HiveQL 脚本上传的目标 Azure 存储帐户。</td></tr>\r     <tr><td>$blobContainerName</td><td>HiveQL 脚本上传的目标 Blob 容器。</td></tr>\r     </table>\r 2. 打开 Azure PowerShell ISE。\r 3. 将以下脚本复制并粘贴到脚本窗格中：\r \r     ```powershell\r     [CmdletBinding()]\r     Param(\r \r         # Azure Blob storage variables\r         [Parameter(Mandatory=$True,\r                     HelpMessage=\"Enter the Azure storage account name for creating a new HDInsight cluster. If the account doesn't exist, the script will create one.\")]\r         [String]$storageAccountName,\r \r         [Parameter(Mandatory=$True,\r                     HelpMessage=\"Enter the Azure blob container name for creating a new HDInsight cluster. If not specified, the HDInsight cluster name will be used.\")]\r         [String]$blobContainerName\r     )\r \r     #region - Define variables\r     # Treat all errors as terminating\r     $ErrorActionPreference = \"Stop\"\r \r     # The HiveQL script file is exported as this file before it's uploaded to Blob storage\r     $hqlLocalFileName = \"e:\\tutorials\\flightdelay\\flightdelays.hql\"\r \r     # The HiveQL script file will be uploaded to Blob storage as this blob name\r     $hqlBlobName = \"tutorials/flightdelay/flightdelays.hql\"\r \r     # These two constants are used by the HiveQL script file\r     #$srcDataFolder = \"tutorials/flightdelay/data\"\r     $dstDataFolder = \"/tutorials/flightdelay/output\"\r     #endregion\r \r     #Region - Connect to Azure subscription\r     Write-Host \"`nConnecting to your Azure subscription ...\" -ForegroundColor Green\r     try{Get-AzureRmContext}\r     catch{Login-AzureRmAccount -EnvironmentName AzureChinaCloud}\r     #EndRegion\r \r     #Region - Validate user input\r     Write-Host \"`nValidating the Azure Storage account and the Blob container...\" -ForegroundColor Green\r     # Validate the Storage account\r     if (-not (Get-AzureRmStorageAccount|Where-Object{$_.StorageAccountName -eq $storageAccountName}))\r     {\r         Write-Host \"The storage account, $storageAccountName, doesn't exist.\" -ForegroundColor Red\r         exit\r     }\r     else{\r         $resourceGroupName = (Get-AzureRmStorageAccount|Where-Object{$_.StorageAccountName -eq $storageAccountName}).ResourceGroupName\r     }\r \r     # Validate the container\r     $storageAccountKey = (Get-AzureRmStorageAccountKey -StorageAccountName $storageAccountName -ResourceGroupName $resourceGroupName)[0].Value\r     $storageContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey\r \r     if (-not (Get-AzureStorageContainer -Context $storageContext |Where-Object{$_.Name -eq $blobContainerName}))\r     {\r         Write-Host \"The Blob container, $blobContainerName, doesn't exist\" -ForegroundColor Red\r         Exit\r     }\r     #EngRegion\r \r     #region - Validate the file and file path\r \r     # Check if a file with the same file name already exists on the workstation\r     Write-Host \"`nvalidating the folder structure on the workstation for saving the HQL script file ...\"  -ForegroundColor Green\r     if (test-path $hqlLocalFileName){\r \r         $isDelete = Read-Host 'The file, ' $hqlLocalFileName ', exists.  Do you want to overwirte it? (Y/N)'\r \r         if ($isDelete.ToLower() -ne \"y\")\r         {\r             Exit\r         }\r     }\r \r     # Create the folder if it doesn't exist\r     $folder = split-path $hqlLocalFileName\r     if (-not (test-path $folder))\r     {\r         Write-Host \"`nCreating folder, $folder ...\" -ForegroundColor Green\r \r         new-item $folder -ItemType directory\r     }\r     #end region\r \r     #region - Write the Hive script into a local file\r     Write-Host \"`nWriting the Hive script into a file on your workstation ...\" `\r                 -ForegroundColor Green\r \r     $hqlDropDelaysRaw = \"DROP TABLE delays_raw;\"\r \r     $hqlCreateDelaysRaw = \"CREATE EXTERNAL TABLE delays_raw (\" +\r             \"YEAR string, \" +\r             \"FL_DATE string, \" +\r             \"UNIQUE_CARRIER string, \" +\r             \"CARRIER string, \" +\r             \"FL_NUM string, \" +\r             \"ORIGIN_AIRPORT_ID string, \" +\r             \"ORIGIN string, \" +\r             \"ORIGIN_CITY_NAME string, \" +\r             \"ORIGIN_CITY_NAME_TEMP string, \" +\r             \"ORIGIN_STATE_ABR string, \" +\r             \"DEST_AIRPORT_ID string, \" +\r             \"DEST string, \" +\r             \"DEST_CITY_NAME string, \" +\r             \"DEST_CITY_NAME_TEMP string, \" +\r             \"DEST_STATE_ABR string, \" +\r             \"DEP_DELAY_NEW float, \" +\r             \"ARR_DELAY_NEW float, \" +\r             \"CARRIER_DELAY float, \" +\r             \"WEATHER_DELAY float, \" +\r             \"NAS_DELAY float, \" +\r             \"SECURITY_DELAY float, \" +\r             \"LATE_AIRCRAFT_DELAY float) \" +\r         \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \" +\r         \"LINES TERMINATED BY '\\n' \" +\r         \"STORED AS TEXTFILE \" +\r         \"LOCATION 'wasb://flightdelay@hditutorialdata.blob.core.chinacloudapi.cn/2013Data';\"\r \r     $hqlDropDelays = \"DROP TABLE delays;\"\r \r     $hqlCreateDelays = \"CREATE TABLE delays AS \" +\r         \"SELECT YEAR AS year, \" +\r             \"FL_DATE AS flight_date, \" +\r             \"substring(UNIQUE_CARRIER, 2, length(UNIQUE_CARRIER) -1) AS unique_carrier, \" +\r             \"substring(CARRIER, 2, length(CARRIER) -1) AS carrier, \" +\r             \"substring(FL_NUM, 2, length(FL_NUM) -1) AS flight_num, \" +\r             \"ORIGIN_AIRPORT_ID AS origin_airport_id, \" +\r             \"substring(ORIGIN, 2, length(ORIGIN) -1) AS origin_airport_code, \" +\r             \"substring(ORIGIN_CITY_NAME, 2) AS origin_city_name, \" +\r             \"substring(ORIGIN_STATE_ABR, 2, length(ORIGIN_STATE_ABR) -1)  AS origin_state_abr, \" +\r             \"DEST_AIRPORT_ID AS dest_airport_id, \" +\r             \"substring(DEST, 2, length(DEST) -1) AS dest_airport_code, \" +\r             \"substring(DEST_CITY_NAME,2) AS dest_city_name, \" +\r             \"substring(DEST_STATE_ABR, 2, length(DEST_STATE_ABR) -1) AS dest_state_abr, \" +\r             \"DEP_DELAY_NEW AS dep_delay_new, \" +\r             \"ARR_DELAY_NEW AS arr_delay_new, \" +\r             \"CARRIER_DELAY AS carrier_delay, \" +\r             \"WEATHER_DELAY AS weather_delay, \" +\r             \"NAS_DELAY AS nas_delay, \" +\r             \"SECURITY_DELAY AS security_delay, \" +\r             \"LATE_AIRCRAFT_DELAY AS late_aircraft_delay \" +\r         \"FROM delays_raw;\"\r \r     $hqlInsertLocal = \"INSERT OVERWRITE DIRECTORY '$dstDataFolder' \" +\r         \"SELECT regexp_replace(origin_city_name, '''', ''), \" +\r             \"avg(weather_delay) \" +\r         \"FROM delays \" +\r         \"WHERE weather_delay IS NOT NULL \" +\r         \"GROUP BY origin_city_name;\"\r \r     $hqlScript = $hqlDropDelaysRaw + $hqlCreateDelaysRaw + $hqlDropDelays + $hqlCreateDelays + $hqlInsertLocal\r \r     $hqlScript | Out-File $hqlLocalFileName -Encoding ascii -Force\r     #endregion\r \r     #region - Upload the Hive script to the default Blob container\r     Write-Host \"`nUploading the Hive script to the default Blob container ...\" -ForegroundColor Green\r \r     # Create a storage context object\r     $storageAccountKey = (Get-AzureRmStorageAccountKey -StorageAccountName $storageAccountName -ResourceGroupName $resourceGroupName)[0].Value\r     $destContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey\r \r     # Upload the file from local workstation to Blob storage\r     Set-AzureStorageBlobContent -File $hqlLocalFileName -Container $blobContainerName -Blob $hqlBlobName -Context $destContext\r     #endregion\r \r     Write-host \"`nEnd of the PowerShell script\" -ForegroundColor Green\r     ```\r \r     该脚本中使用了以下变量：\r \r    * **$hqlLocalFileName** - 该脚本会先将 HiveQL 脚本文件保存在本地，然后才上传到 Blob 存储。 这是文件名。 默认值是 <u>C:\\tutorials\\flightdelay\\flightdelays.hql</u>。\r    * **$hqlBlobName** - 这是 Azure Blob 存储中使用的 HiveQL 脚本文件 Blob 名称。 默认值是 tutorials/flightdelay/flightdelays.hql。 因为文件会直接写入 Azure Blob 存储，所以 Blob 名称的开头不是“/”。 如果要从 Blob 存储访问文件，必须在文件名的开头添加“/”。\r    * $srcDataFolder 和 $dstDataFolder - = \"tutorials/flightdelay/data\" = \"tutorials/flightdelay/output\"\r \r - - -\r ## <a id=\"appendix-c\"></a>附录 C - 针对 Sqoop 作业输出准备 Azure SQL 数据库\r **准备 SQL 数据库（将此部分与 Sqoop 脚本合并）**\r \r 1. 准备参数：\r \r     <table border=\"1\">\r     <tr><th>变量名</th><th>说明</th></tr>\r     <tr><td>$sqlDatabaseServerName</td><td>Azure SQL 数据库服务器的名称。 不输入任何值会创建新的服务器。</td></tr>\r     <tr><td>$sqlDatabaseUsername</td><td>Azure SQL 数据库服务器登录名。 如果 $sqlDatabaseServerName 是现有的服务器，登录名和登录密码将用来向服务器进行身份验证。 否则将其用于创建新的服务器。</td></tr>\r     <tr><td>$sqlDatabasePassword</td><td>Azure SQL 数据库服务器登录密码。</td></tr>\r     <tr><td>$sqlDatabaseLocation</td><td>只有在创建新的 Azure 数据库服务器时才会使用此值。</td></tr>\r     <tr><td>$sqlDatabaseName</td><td>Sqoop 作业的 AvgDelays 表的 SQL 数据库。 保留空白会创建名为 HDISqoop 的数据库。 Sqooop 作业输出的表名称为 AvgDelays。 </td></tr>\r     </table>\r 2. 打开 Azure PowerShell ISE。\r 3. 将以下脚本复制并粘贴到脚本窗格中：\r \r     ```powershell\r     [CmdletBinding()]\r     Param(\r \r         # Azure Resource group variables\r         [Parameter(Mandatory=$True,\r                 HelpMessage=\"Enter the Azure resource group name. It will be created if it doesn't exist.\")]\r         [String]$resourceGroupName,\r \r         # SQL database server variables\r         [Parameter(Mandatory=$True,\r                 HelpMessage=\"Enter the Azure SQL Database Server Name. It will be created if it doesn't exist.\")]\r         [String]$sqlDatabaseServer,\r \r         [Parameter(Mandatory=$True,\r                 HelpMessage=\"Enter the Azure SQL Database admin user.\")]\r         [String]$sqlDatabaseLogin,\r \r         [Parameter(Mandatory=$True,\r                 HelpMessage=\"Enter the Azure SQL Database admin user password.\")]\r         [String]$sqlDatabasePassword,\r \r         [Parameter(Mandatory=$True,\r                 HelpMessage=\"Enter the region to create the Database in.\")]\r         [String]$sqlDatabaseLocation,   #For example, China North.\r \r         # SQL database variables\r         [Parameter(Mandatory=$True,\r                 HelpMessage=\"Enter the database name. It will be created if it doesn't exist.\")]\r         [String]$sqlDatabaseName # specify the database name if you have one created. Otherwise use \"\" to have the script create one for you.\r     )\r \r     # Treat all errors as terminating\r     $ErrorActionPreference = \"Stop\"\r \r     #region - Constants and variables\r \r     # IP address REST service used for retrieving external IP address and creating firewall rules\r     [String]$ipAddressRestService = \"http://bot.whatismyipaddress.com\"\r     [String]$fireWallRuleName = \"FlightDelay\"\r \r     # SQL database variables\r     [String]$sqlDatabaseMaxSizeGB = 10\r \r     #SQL query string for creating AvgDelays table\r     [String]$sqlDatabaseTableName = \"AvgDelays\"\r     [String]$sqlCreateAvgDelaysTable = \" CREATE TABLE [dbo].[$sqlDatabaseTableName](\r                 [origin_city_name] [nvarchar](50) NOT NULL,\r                 [weather_delay] float,\r             CONSTRAINT [PK_$sqlDatabaseTableName] PRIMARY KEY CLUSTERED\r             (\r                 [origin_city_name] ASC\r             )\r             )\"\r     #endregion\r \r     #Region - Connect to Azure subscription\r     Write-Host \"`nConnecting to your Azure subscription ...\" -ForegroundColor Green\r     try{Get-AzureRmContext}\r     catch{Login-AzureRmAccount -EnvironmentName AzureChinaCloud}\r     #EndRegion\r \r     #region - Create and validate Azure resouce group\r     try{\r         Get-AzureRmResourceGroup -Name $resourceGroupName\r     }\r     catch{\r         New-AzureRmResourceGroup -Name $resourceGroupName -Location $sqlDatabaseLocation\r     }\r \r     #EndRegion\r \r     #region - Create and validate Azure SQL database server\r     try{\r         Get-AzureRmSqlServer -ServerName $sqlDatabaseServer -ResourceGroupName $resourceGroupName}\r     catch{\r         Write-Host \"`nCreating SQL Database server ...\"  -ForegroundColor Green\r \r         $sqlDatabasePW = ConvertTo-SecureString -String $sqlDatabasePassword -AsPlainText -Force\r         $credential = New-Object System.Management.Automation.PSCredential($sqlDatabaseLogin,$sqlDatabasePW)\r \r         $sqlDatabaseServer = (New-AzureRmSqlServer -ResourceGroupName $resourceGroupName -ServerName $sqlDatabaseServer -SqlAdministratorCredentials $credential -Location $sqlDatabaseLocation).ServerName\r         Write-Host \"`tThe new SQL database server name is $sqlDatabaseServer.\" -ForegroundColor Cyan\r \r         Write-Host \"`nCreating firewall rule, $fireWallRuleName ...\" -ForegroundColor Green\r         $workstationIPAddress = Invoke-RestMethod $ipAddressRestService\r         New-AzureRmSqlServerFirewallRule -ResourceGroupName $resourceGroupName -ServerName $sqlDatabaseServer -FirewallRuleName \"$fireWallRuleName-workstation\" -StartIpAddress $workstationIPAddress -EndIpAddress $workstationIPAddress\r \r         #To allow other Azure services to access the server add a firewall rule and set both the StartIpAddress and EndIpAddress to 0.0.0.0. Note that this allows Azure traffic from any Azure subscription to access the server.\r         New-AzureRmSqlServerFirewallRule -ResourceGroupName $resourceGroupName -ServerName $sqlDatabaseServer -FirewallRuleName \"$fireWallRuleName-Azureservices\" -StartIpAddress \"0.0.0.0\" -EndIpAddress \"0.0.0.0\"\r     }\r \r     #endregion\r \r     #region - Create and validate Azure SQL database\r \r     try {\r         Get-AzureRmSqlDatabase -ResourceGroupName $resourceGroupName -ServerName $sqlDatabaseServer -DatabaseName $sqlDatabaseName\r     }\r     catch {\r         Write-Host \"`nCreating SQL Database, $sqlDatabaseName ...\"  -ForegroundColor Green\r         New-AzureRMSqlDatabase -ResourceGroupName $resourceGroupName -ServerName $sqlDatabaseServer -DatabaseName $sqlDatabaseName -Edition \"Standard\" -RequestedServiceObjectiveName \"S1\"\r     }\r \r     #endregion\r \r     #region -  Execute an SQL command to create the AvgDelays table\r \r     Write-Host \"`nCreating SQL Database table ...\"  -ForegroundColor Green\r     $conn = New-Object System.Data.SqlClient.SqlConnection\r     $conn.ConnectionString = \"Data Source=$sqlDatabaseServer.database.chinacloudapi.cn;Initial Catalog=$sqlDatabaseName;User ID=$sqlDatabaseLogin;Password=$sqlDatabasePassword;Encrypt=true;Trusted_Connection=false;\"\r     $conn.open()\r     $cmd = New-Object System.Data.SqlClient.SqlCommand\r     $cmd.connection = $conn\r     $cmd.commandtext = $sqlCreateAvgDelaysTable\r     $cmd.executenonquery()\r \r     $conn.close()\r \r     Write-host \"`nEnd of the PowerShell script\" -ForegroundColor Green\r     ```\r \r    > [!NOTE]\r    > 该脚本使用表述性状态转移 (REST) 服务 (http://bot.whatismyipaddress.com) 来检索外部 IP 地址。 IP 地址用于创建 SQL 数据库服务器的防火墙规则。\r \r     该脚本中使用的某些变量：\r \r    * $ipAddressRestService - 默认值是 http://bot.whatismyipaddress.com。这是用来获取外部 IP 地址的公共 IP 地址 REST 服务。 可根据需要使用其他服务。 使用此服务检索的外部 IP 地址将用于创建 Azure SQL 数据库服务器的防火墙规则，使用户能够从工作站访问数据库（通过 Windows PowerShell 脚本）。\r    * **$fireWallRuleName** - 这是 Azure SQL 数据库服务器的防火墙规则名称。 默认名称为 <u>FlightDelay</u>。 可根据需要对其进行重命名。\r    * **$sqlDatabaseMaxSizeGB** - 只有在创建新的 Azure SQL 数据库服务器时才会使用此值。 默认值为 10GB。 10GB 对于本教程来说已足够。\r    * **$sqlDatabaseName** - 只有在创建新的 Azure SQL 数据库时才会使用此值。 默认值为 HDISqoop。 如果将它重命名，则必须相应地更新 Sqoop Windows PowerShell 脚本。\r 4. 按 **F5** 运行脚本。\r 5. 验证脚本输出。 确保已成功运行脚本。\r \r ## <a id=\"nextsteps\"></a> 后续步骤\r 现在你已了解如何执行以下操作：将文件上传到 Azure Blob 存储、使用 Azure Blob 存储中的数据填充 Hive 表、运行 Hive 查询以及使用 Sqoop 将数据从 HDFS 导出到 Azure SQL 数据库。 若要了解更多信息，请参阅下列文章：\r \r * [开始使用 HDInsight][hdinsight-get-started]\r * [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]\r * [将 Oozie 与 HDInsight 配合使用][hdinsight-use-oozie]\r * [将 Sqoop 与 HDInsight 配合使用][hdinsight-use-sqoop]\r * [将 Pig 与 HDInsight 配合使用][hdinsight-use-pig]\r * [为 HDInsight 开发 Java MapReduce 程序][hdinsight-develop-mapreduce]\r \r [azure-purchase-options]: https://www.azure.cn/pricing/overview/\r [azure-member-offers]: https://www.azure.cn/pricing/member-offers/\r [azure-trial]: https://www.azure.cn/pricing/1rmb-trial/\r \r [rita-website]: http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time\r [powershell-install-configure]: https://docs.microsoft.com/powershell/azureps-cmdlets-docs\r \r [hdinsight-use-oozie]: hdinsight-use-oozie.md\r [hdinsight-use-hive]:hadoop/hdinsight-use-hive.md\r [hdinsight-provision]: hdinsight-hadoop-provision-linux-clusters.md\r [hdinsight-storage]: hdinsight-hadoop-use-blob-storage.md\r [hdinsight-upload-data]: hdinsight-upload-data.md\r [hdinsight-get-started]:hadoop/apache-hadoop-linux-tutorial-get-started.md\r [hdinsight-use-sqoop]:hadoop/hdinsight-use-sqoop.md\r [hdinsight-use-pig]:hadoop/hdinsight-use-pig.md\r [hdinsight-develop-mapreduce]:hadoop/apache-hadoop-develop-deploy-java-mapreduce-linux.md\r \r [hadoop-hiveql]: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL\r \r [technetwiki-hive-error]: http://social.technet.microsoft.com/wiki/contents/articles/23047.hdinsight-hive-error-unable-to-rename.aspx\r \r [image-hdi-flightdelays-avgdelays-dataset]: ./media/hdinsight-analyze-flight-delay-data/HDI.FlightDelays.AvgDelays.DataSet.png\r [img-hdi-flightdelays-run-hive-job-output]: ./media/hdinsight-analyze-flight-delay-data/HDI.FlightDelays.RunHiveJob.Output.png\r [img-hdi-flightdelays-flow]: ./media/hdinsight-analyze-flight-delay-data/HDI.FlightDelays.Flow.png\r <!--Update_Description: update wording and link references-->"}