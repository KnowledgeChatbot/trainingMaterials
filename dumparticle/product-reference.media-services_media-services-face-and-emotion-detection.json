{"Title":"使用 Azure 媒体分析检测面部和情绪","Description":"本主题演示如何使用 Azure 媒体分析检测人脸和情感。","Content":"# <a name=\"detect-face-and-emotion-with-azure-media-analytics\"></a>使用 Azure 媒体分析检测面部和情绪\r \r ## <a name=\"overview\"></a>概述\r \r 借助 **Azure Media Face Detector** 媒体处理器 (MP)，可通过面部表情来统计、跟踪动作，甚至计量受众的参与和反应。 此服务包含两项功能：\r \r * **面部检测**\r \r     面部检测能够找出并跟踪视频中的人脸。 可以检测多个面部，随后随着对象移动进行跟踪，并将时间和位置的元数据以 JSON 文件的形式返回。 跟踪期间，该服务会在人员于屏幕上四处移动时，尝试为他们的面部赋予相同的 ID，即使他们被挡住或暂时离帧。\r \r   > [!NOTE]\r   > 此服务并不执行面部识别。 面部离帧或被挡住太久的人员，会在回来时赋予新的 ID。\r \r * **情绪检测**\r \r     情绪检测是面部检测媒体处理器的可选组件，它根据检测到的面部返回多个情绪属性的分析，包括快乐、悲伤、恐惧、愤怒等等。\r \r **Azure 媒体面部检测器** MP 目前以预览版提供。\r \r 本主题提供有关 Azure Media Face Detector 的详细信息，并演示如何通过适用于 .NET 的媒体服务 SDK 使用它。\r \r ## <a name=\"face-detector-input-files\"></a>面部检测器输入文件\r \r 视频文件。 目前支持以下格式：MP4、MOV 和 WMV。\r \r ## <a name=\"face-detector-output-files\"></a>面部检测器输出文件\r \r 面部检测器和跟踪 API 可提供高精确度的面部位置检测和跟踪功能，并在单个视频中检测到最多 64 个人脸。 正面的面部可提供最佳效果，而侧面的面部和较小的面部（小于或等于 24x24 像素）可能就无法获得相同的精确度。\r \r 已检测到并已跟踪的面部会在坐标（左侧、顶部、宽度和高度）中返回，其中会在以像素为单位的图像中指明面部的位置，以及表示正在跟踪该人员的面部 ID 编号。 在正面面部长时间于帧中消失或重叠的情况下，面部 ID 编号很容易重置，导致某些人员被分配多个 ID。\r \r ## <a id=\"output_elements\"></a>输出 JSON 文件中的元素\r \r [!INCLUDE [media-services-analytics-output-json](../../includes/media-services-analytics-output-json.md)]\r \r 面部检测器使用分片（元数据可以分解为基于时间的区块，可以只下载需要的部分）和分段（可以在事件数过于庞大的情况下对事件进行分解）技术。 一些简单的计算可帮助转换数据。 例如，如果事件从 6300（刻度）开始，其时间刻度为 2997（刻度/秒），帧速率为 29.97（帧/秒），那么：\r \r * 开始时间/时间刻度 = 2.1 秒\r * 秒数 x 帧速率 = 63 帧\r \r ## <a name=\"face-detection-input-and-output-example\"></a>面部检测输入和输出示例\r \r ### <a name=\"input-video\"></a>输入视频\r \r [输入视频](http://ampdemo.azureedge.net/azuremediaplayer.html?url=https%3A%2F%2Freferencestream-samplestream.streaming.mediaservices.windows.net%2Fc8834d9f-0b49-4b38-bcaf-ece2746f1972%2FMicrosoft%20Convergence%202015%20%20Keynote%20Highlights.ism%2Fmanifest&amp;autoplay=false)\r \r ### <a name=\"task-configuration-preset\"></a>任务配置（预设）\r \r 在使用 **Azure 媒体面部检测器**创建任务时，必须指定配置预设。 以下配置预设仅适用于面部检测。\r \r     {\r       \"version\":\"1.0\",\r       \"options\":{\r           \"TrackingMode\": \"Fast\"\r       }\r     }\r \r #### <a name=\"attribute-descriptions\"></a>属性说明\r \r | 属性名称 | 说明 |\r | --- | --- |\r | Mode |快速 - 处理速度快，但准确度较低（默认）。|\r \r ### <a name=\"json-output\"></a>JSON 输出\r \r 下面是 JSON 输出被截断的示例。\r \r     {\r     \"version\": 1,\r     \"timescale\": 30000,\r     \"offset\": 0,\r     \"framerate\": 29.97,\r     \"width\": 1280,\r     \"height\": 720,\r     \"fragments\": [\r         {\r         \"start\": 0,\r         \"duration\": 60060\r         },\r         {\r         \"start\": 60060,\r         \"duration\": 60060,\r         \"interval\": 1001,\r         \"events\": [\r             [\r             {\r                 \"id\": 0,\r                 \"x\": 0.519531,\r                 \"y\": 0.180556,\r                 \"width\": 0.0867188,\r                 \"height\": 0.154167\r             }\r             ],\r             [\r             {\r                 \"id\": 0,\r                 \"x\": 0.517969,\r                 \"y\": 0.181944,\r                 \"width\": 0.0867188,\r                 \"height\": 0.154167\r             }\r             ],\r             [\r             {\r                 \"id\": 0,\r                 \"x\": 0.517187,\r                 \"y\": 0.183333,\r                 \"width\": 0.0851562,\r                 \"height\": 0.151389\r             }\r             ],\r \r         . . .\r \r ## <a name=\"emotion-detection-input-and-output-example\"></a>情绪检测输入和输出示例\r \r ### <a name=\"input-video\"></a>输入视频\r \r [输入视频](http://ampdemo.azureedge.net/azuremediaplayer.html?url=https%3A%2F%2Freferencestream-samplestream.streaming.mediaservices.windows.net%2Fc8834d9f-0b49-4b38-bcaf-ece2746f1972%2FMicrosoft%20Convergence%202015%20%20Keynote%20Highlights.ism%2Fmanifest&amp;autoplay=false)\r \r ### <a name=\"task-configuration-preset\"></a>任务配置（预设）\r \r 在使用 **Azure 媒体面部检测器**创建任务时，必须指定配置预设。 以下配置预设指定基于情绪检测创建 JSON。\r \r     {\r       \"version\": \"1.0\",\r       \"options\": {\r         \"aggregateEmotionWindowMs\": \"987\",\r         \"mode\": \"aggregateEmotion\",\r         \"aggregateEmotionIntervalMs\": \"342\"\r       }\r     }\r \r #### <a name=\"attribute-descriptions\"></a>属性说明\r \r | 属性名称 | 说明 |\r | --- | --- |\r | Mode |Faces：仅人脸检测。<br/>PerFaceEmotion：独立返回每个人脸检测的情绪。<br/>AggregateEmotion：返回帧中所有面部的平均情绪值。 |\r | AggregateEmotionWindowMs |在已选择 AggregateEmotion 模式时使用。 指定用于生成每个聚合结果的视频的长度，以毫秒为单位。 |\r | AggregateEmotionIntervalMs |在已选择 AggregateEmotion 模式时使用。 指定生成聚合结果的频率。 |\r \r #### <a name=\"aggregate-defaults\"></a>聚合默认值\r \r 下面是聚合窗口和间隔设置的建议值。 AggregateEmotionWindowMs 应该超过 AggregateEmotionIntervalMs。\r \r || 默认值 | 最大值 | 最小值 |\r |--- | --- | --- | --- |\r | AggregateEmotionWindowMs |0.5 |2 |0.25|\r | AggregateEmotionIntervalMs |0.5 |1 |0.25|\r \r ### <a name=\"json-output\"></a>JSON 输出\r \r 聚合情绪的 JSON 输出（已截断）：\r \r     {\r      \"version\": 1,\r      \"timescale\": 30000,\r      \"offset\": 0,\r      \"framerate\": 29.97,\r      \"width\": 1280,\r      \"height\": 720,\r      \"fragments\": [\r        {\r          \"start\": 0,\r          \"duration\": 60060,\r          \"interval\": 15015,\r          \"events\": [\r            [\r              {\r                \"windowFaceDistribution\": {\r                  \"neutral\": 0,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                },\r                \"windowMeanScores\": {\r                  \"neutral\": 0,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                }\r              }\r            ],\r            [\r              {\r                \"windowFaceDistribution\": {\r                  \"neutral\": 0,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                },\r                \"windowMeanScores\": {\r                  \"neutral\": 0,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                }\r              }\r            ],\r            [\r              {\r                \"windowFaceDistribution\": {\r                  \"neutral\": 0,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                },\r                \"windowMeanScores\": {\r                  \"neutral\": 0,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                }\r              }\r            ],\r            [\r              {\r                \"windowFaceDistribution\": {\r                  \"neutral\": 0,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                },\r                \"windowMeanScores\": {\r                  \"neutral\": 0,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                }\r              }\r            ]\r          ]\r        },\r        {\r          \"start\": 60060,\r          \"duration\": 60060,\r          \"interval\": 15015,\r          \"events\": [\r            [\r              {\r                \"windowFaceDistribution\": {\r                  \"neutral\": 1,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r                  \"contempt\": 0\r                },\r                \"windowMeanScores\": {\r                  \"neutral\": 0.688541,\r                  \"happiness\": 0.0586323,\r                  \"surprise\": 0.227184,\r                  \"sadness\": 0.00945675,\r                  \"anger\": 0.00592107,\r                  \"disgust\": 0.00154993,\r                  \"fear\": 0.00450447,\r                  \"contempt\": 0.0042109\r                }\r              }\r            ],\r            [\r              {\r                \"windowFaceDistribution\": {\r                  \"neutral\": 1,\r                  \"happiness\": 0,\r                  \"surprise\": 0,\r                  \"sadness\": 0,\r                  \"anger\": 0,\r                  \"disgust\": 0,\r                  \"fear\": 0,\r \r ## <a name=\"limitations\"></a>限制\r \r * 支持的输入视频格式包括 MP4、MOV 和 WMV。\r * 可检测的面部大小范围为 24x24 到 2048x2048 像素。 无法检测此范围以外的面部。\r * 对于每个视频，返回的面部数上限为 64。\r * 某些面部可能因技术难题而无法检测，例如非常大的面部角度（头部姿势），以及较大的阻挡物。 正面和接近正面的面部可提供最佳效果。\r \r ## <a name=\"net-sample-code\"></a>.NET 示例代码\r \r 以下程序演示如何：\r \r 1. 创建资产并将媒体文件上传到资产。\r 2. 使用人脸检测任务创建一个作业，所根据的配置文件包含以下 json 预设。 \r    \r         {\r             \"version\": \"1.0\"\r         }\r 3. 下载输出 JSON 文件。 \r \r #### <a name=\"create-and-configure-a-visual-studio-project\"></a>创建和配置 Visual Studio 项目\r \r 设置开发环境，并根据[使用 .NET 进行媒体服务开发](media-services-dotnet-how-to-use.md)中所述，在 app.config 文件中填充连接信息。 \r \r #### <a name=\"example\"></a>示例\r \r     using System;\r     using System.Configuration;\r     using System.IO;\r     using System.Linq;\r     using Microsoft.WindowsAzure.MediaServices.Client;\r     using System.Threading;\r     using System.Threading.Tasks;\r \r     namespace FaceDetection\r     {\r         class Program\r         {\r             private static readonly string _AADTenantDomain =\r                       ConfigurationManager.AppSettings[\"AADTenantDomain\"];\r             private static readonly string _RESTAPIEndpoint =\r                       ConfigurationManager.AppSettings[\"MediaServiceRESTAPIEndpoint\"];\r \r             // Field for service context.\r             private static CloudMediaContext _context = null;\r \r             static void Main(string[] args)\r             {\r                 var tokenCredentials = new AzureAdTokenCredentials(_AADTenantDomain, AzureEnvironments.AzureChinaCloudEnvironment);\r                 var tokenProvider = new AzureAdTokenProvider(tokenCredentials);\r \r                 _context = new CloudMediaContext(new Uri(_RESTAPIEndpoint), tokenProvider);\r \r                 // Run the FaceDetection job.\r                 var asset = RunFaceDetectionJob(@\"C:\\supportFiles\\FaceDetection\\BigBuckBunny.mp4\",\r                                             @\"C:\\supportFiles\\FaceDetection\\config.json\");\r \r                 // Download the job output asset.\r                 DownloadAsset(asset, @\"C:\\supportFiles\\FaceDetection\\Output\");\r             }\r \r             static IAsset RunFaceDetectionJob(string inputMediaFilePath, string configurationFile)\r             {\r                 // Create an asset and upload the input media file to storage.\r                 IAsset asset = CreateAssetAndUploadSingleFile(inputMediaFilePath,\r                     \"My Face Detection Input Asset\",\r                     AssetCreationOptions.None);\r \r                 // Declare a new job.\r                 IJob job = _context.Jobs.Create(\"My Face Detection Job\");\r \r                 // Get a reference to Azure Media Face Detector.\r                 string MediaProcessorName = \"Azure Media Face Detector\";\r \r                 var processor = GetLatestMediaProcessorByName(MediaProcessorName);\r \r                 // Read configuration from the specified file.\r                 string configuration = File.ReadAllText(configurationFile);\r \r                 // Create a task with the encoding details, using a string preset.\r                 ITask task = job.Tasks.AddNew(\"My Face Detection Task\",\r                     processor,\r                     configuration,\r                     TaskOptions.None);\r \r                 // Specify the input asset.\r                 task.InputAssets.Add(asset);\r \r                 // Add an output asset to contain the results of the job.\r                 task.OutputAssets.AddNew(\"My Face Detectoion Output Asset\", AssetCreationOptions.None);\r \r                 // Use the following event handler to check job progress.  \r                 job.StateChanged += new EventHandler<JobStateChangedEventArgs>(StateChanged);\r \r                 // Launch the job.\r                 job.Submit();\r \r                 // Check job execution and wait for job to finish.\r                 Task progressJobTask = job.GetExecutionProgressTask(CancellationToken.None);\r \r                 progressJobTask.Wait();\r \r                 // If job state is Error, the event handling\r                 // method for job progress should log errors.  Here we check\r                 // for error state and exit if needed.\r                 if (job.State == JobState.Error)\r                 {\r                     ErrorDetail error = job.Tasks.First().ErrorDetails.First();\r                     Console.WriteLine(string.Format(\"Error: {0}. {1}\",\r                                                     error.Code,\r                                                     error.Message));\r                     return null;\r                 }\r \r                 return job.OutputMediaAssets[0];\r             }\r \r             static IAsset CreateAssetAndUploadSingleFile(string filePath, string assetName, AssetCreationOptions options)\r             {\r                 IAsset asset = _context.Assets.Create(assetName, options);\r \r                 var assetFile = asset.AssetFiles.Create(Path.GetFileName(filePath));\r                 assetFile.Upload(filePath);\r \r                 return asset;\r             }\r \r             static void DownloadAsset(IAsset asset, string outputDirectory)\r             {\r                 foreach (IAssetFile file in asset.AssetFiles)\r                 {\r                     file.Download(Path.Combine(outputDirectory, file.Name));\r                 }\r             }\r \r             static IMediaProcessor GetLatestMediaProcessorByName(string mediaProcessorName)\r             {\r                 var processor = _context.MediaProcessors\r                     .Where(p => p.Name == mediaProcessorName)\r                     .ToList()\r                     .OrderBy(p => new Version(p.Version))\r                     .LastOrDefault();\r \r                 if (processor == null)\r                     throw new ArgumentException(string.Format(\"Unknown media processor\",\r                                                                mediaProcessorName));\r \r                 return processor;\r             }\r \r             static private void StateChanged(object sender, JobStateChangedEventArgs e)\r             {\r                 Console.WriteLine(\"Job state changed event:\");\r                 Console.WriteLine(\"  Previous state: \" + e.PreviousState);\r                 Console.WriteLine(\"  Current state: \" + e.CurrentState);\r \r                 switch (e.CurrentState)\r                 {\r                     case JobState.Finished:\r                         Console.WriteLine();\r                         Console.WriteLine(\"Job is finished.\");\r                         Console.WriteLine();\r                         break;\r                     case JobState.Canceling:\r                     case JobState.Queued:\r                     case JobState.Scheduled:\r                     case JobState.Processing:\r                         Console.WriteLine(\"Please wait...\\n\");\r                         break;\r                     case JobState.Canceled:\r                     case JobState.Error:\r                         // Cast sender as a job.\r                         IJob job = (IJob)sender;\r                         // Display or log error details as needed.\r                         // LogJobStop(job.Id);\r                         break;\r                     default:\r                         break;\r                 }\r             }\r         }\r     }\r \r \r ## <a name=\"related-links\"></a>相关链接\r [Azure 媒体服务分析概述](media-services-analytics-overview.md)\r \r [Azure Media Analytics demos（Azure 媒体分析演示）](http://amslabs.azurewebsites.net/demos/Analytics.html)\r \r <!--Update_Description: update Aggregate defaults table-->"}