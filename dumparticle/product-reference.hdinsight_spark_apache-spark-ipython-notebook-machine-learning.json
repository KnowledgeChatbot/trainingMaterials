{"Title":"在 Azure HDInsight 上生成 Apache Spark 机器学习应用程序","Description":"逐步说明如何使用 Jupyter notebook 在 HDInsight Spark 群集中生成 Apache Spark 机器学习应用程序","Content":"# <a name=\"build-apache-spark-machine-learning-applications-on-azure-hdinsight\"></a>在 Azure HDInsight 上生成 Apache Spark 机器学习应用程序\r \r 了解如何使用 HDInsight 中的 Spark 群集生成 Apache Spark 机器学习应用程序。 本文说明如何使用群集随附的 Jupyter notebook 来生成和测试此应用程序。 应用程序默认使用所有群集提供的 HVAC.csv 数据示例。\r \r **先决条件：**\r \r 必须满足以下条件：\r \r * HDInsight 上的 Apache Spark 群集。 有关说明，请参阅[在 Azure HDInsight 中创建 Apache Spark 群集](apache-spark-jupyter-spark-sql.md)。 \r \r ## <a name=\"data\"></a>了解数据集\r 在开始生成应用程序之前，我们先来了解要为其生成应用程序的数据的结构，以及要对数据执行的分析类型。 \r \r 在本文中，使用与 HDInsight 群集关联的 Azure 存储帐户中提供的 **HVAC.csv** 数据文件示例。 该文件位于存储帐户中的 **\\HdiSamples\\HdiSamples\\SensorSampleData\\hvac** 位置。 下载并打开 CSV 文件，以获取数据的快照。  \r \r ![用于 Spark 机器学习示例的数据的快照](./media/apache-spark-ipython-notebook-machine-learning/spark-machine-learning-understand-data.png \"用于 Spark 机器学习示例的数据的快照\")\r \r 该数据显示安装 HVAC 系统的建筑物的目标温度和实际温度。 我们假设 **System** 列代表系统 ID，**SystemAge** 列代表建筑物安装 HVAC 系统的年数。\r \r 如果指定系统 ID 和系统年数，可以使用此数据预测建筑物的温度比目标温度高还是低。\r \r ## <a name=\"app\"></a>使用 Spark MLlib 编写 Spark 机器学习应用程序\r 在此应用程序中，使用 Spark ML 管道执行文档分类。 在管道中，将文档分割成单字，将单字转换成数字特征向量，并最后使用特征向量和标签创建预测模型。 执行下列步骤创建应用程序。\r \r 1. 在 [Azure 门户](https://portal.azure.cn/)上的启动板中，单击 Spark 群集的磁贴（如果已将它固定到启动板）。 也可以单击“全部浏览” > “HDInsight 群集”导航到群集。   \r 2. 在 Spark 群集边栏选项卡中单击“群集仪表板”，然后单击“Jupyter Notebook”。 出现提示时，请输入群集的管理员凭据。\r \r    > [!NOTE]\r    > 也可以在浏览器中打开以下 URL 访问群集的 Jupyter 笔记本。 将 **CLUSTERNAME** 替换为群集的名称：\r    > \r    > `https://CLUSTERNAME.azurehdinsight.cn/jupyter`\r    > \r    > \r    \r 3. 创建新的笔记本。 单击“新建”，然后单击“PySpark”。\r \r     ![创建用于 Spark 机器学习示例的 Jupyter notebook](./media/apache-spark-ipython-notebook-machine-learning/spark-machine-learning-create-notebook.png \"创建用于 Spark 机器学习示例的 Jupyter notebook\")\r 4. 随即创建新笔记本，并以 Untitled.pynb 名称打开。 在顶部单击笔记本名称，并输入一个友好名称。\r \r     ![提供用于 Spark 机器学习示例的笔记本名称](./media/apache-spark-ipython-notebook-machine-learning/spark-machine-learning-notebook-name.png \"提供用于 Spark 机器学习示例的笔记本名称\")\r 5. 使用笔记本是使用 PySpark 内核创建的，因此不需要显式创建任何上下文。 运行第一个代码单元格时，系统自动创建 Spark 和 Hive 上下文。 首先，可以导入此方案所需的类型。 将以下代码段粘贴到空白单元格中，并按 **SHIFT + ENTER**。 \r \r         from pyspark.ml import Pipeline\r         from pyspark.ml.classification import LogisticRegression\r         from pyspark.ml.feature import HashingTF, Tokenizer\r         from pyspark.sql import Row\r \r         import os\r         import sys\r         from pyspark.sql.types import *\r \r         from pyspark.mllib.classification import LogisticRegressionWithSGD\r         from pyspark.mllib.regression import LabeledPoint\r         from numpy import array\r         \r 6. 现在必须加载数据 (hvac.csv)，分析数据，并使用它来训练模型。 为此，需要定义检查建筑物实际温度是否高于目标温度的函数。 如果实际温度较高，则表示建筑物处于高温状态，用值 **1.0**表示。 如果实际温度较低，则表示建筑物处于低温状态，用值 **0.0**表示。 \r \r     将以下代码段粘贴到空白单元格中，并按 **SHIFT + ENTER**。\r \r         # List the structure of data for better understanding. Because the data will be\r         # loaded as an array, this structure makes it easy to understand what each element\r         # in the array corresponds to\r \r         # 0 Date\r         # 1 Time\r         # 2 TargetTemp\r         # 3 ActualTemp\r         # 4 System\r         # 5 SystemAge\r         # 6 BuildingID\r \r         LabeledDocument = Row(\"BuildingID\", \"SystemInfo\", \"label\")\r \r         # Define a function that parses the raw CSV file and returns an object of type LabeledDocument\r \r         def parseDocument(line):\r             values = [str(x) for x in line.split(',')]\r             if (values[3] > values[2]):\r                 hot = 1.0\r             else:\r                 hot = 0.0        \r \r             textValue = str(values[4]) + \" \" + str(values[5])\r \r             return LabeledDocument((values[6]), textValue, hot)\r \r         # Load the raw HVAC.csv file, parse it using the function\r         data = sc.textFile(\"wasb:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\")\r \r         documents = data.filter(lambda s: \"Date\" not in s).map(parseDocument)\r         training = documents.toDF()\r \r 1. 设置包括三个阶段的 Spark 机器学习管道：tokenizer、hashingTF 和 lr。 有关管道介绍及其工作原理的详细信息，请参阅 <a href=\"http://spark.apache.org/docs/latest/ml-guide.html#how-it-works\" target=\"_blank\">Spark 机器学习管道</a>。\r \r     将以下代码段粘贴到空白单元格中，并按 **SHIFT + ENTER**。\r \r         tokenizer = Tokenizer(inputCol=\"SystemInfo\", outputCol=\"words\")\r         hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\r         lr = LogisticRegression(maxIter=10, regParam=0.01)\r         pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\r         \r 2. 将管道拟合到培训文档中。 将以下代码段粘贴到空白单元格中，并按 **SHIFT + ENTER**。\r \r         model = pipeline.fit(training)\r         \r 3. 验证训练文档以根据应用程序进度创建检查点。 将以下代码段粘贴到空白单元格中，并按 **SHIFT + ENTER**。\r \r         training.show()\r \r     输出应如下所示：\r \r         +----------+----------+-----+\r         |BuildingID|SystemInfo|label|\r         +----------+----------+-----+\r         |         4|     13 20|  0.0|\r         |        17|      3 20|  0.0|\r         |        18|     17 20|  1.0|\r         |        15|      2 23|  0.0|\r         |         3|      16 9|  1.0|\r         |         4|     13 28|  0.0|\r         |         2|     12 24|  0.0|\r         |        16|     20 26|  1.0|\r         |         9|      16 9|  1.0|\r         |        12|       6 5|  0.0|\r         |        15|     10 17|  1.0|\r         |         7|      2 11|  0.0|\r         |        15|      14 2|  1.0|\r         |         6|       3 2|  0.0|\r         |        20|     19 22|  0.0|\r         |         8|     19 11|  0.0|\r         |         6|      15 7|  0.0|\r         |        13|      12 5|  0.0|\r         |         4|      8 22|  0.0|\r         |         7|      17 5|  0.0|\r         +----------+----------+-----+\r \r     返回并根据原始 CSV 文件验证输出。 例如，CSV 文件中第一行包含此数据：\r \r     ![Spark 机器学习示例的输出数据快照](./media/apache-spark-ipython-notebook-machine-learning/spark-machine-learning-output-data.png \"Spark 机器学习示例的输出数据快照\")\r \r     请注意，实际温度比目标温度低表示建筑物处于低温状态。 因此在训练输出中，第一行中的 **label** 值为 **0.0**，表示建筑物并非处于高温状态。\r \r 1. 准备要对其运行训练模型的数据集。 为此，我们传递了系统 ID 和系统年数（以训练输出中的 **SystemInfo** 表示），模型将预测具有该系统 ID 和系统年数的建筑物的温度是较高（以 1.0 表示）还是较低（以 0.0 表示）。\r \r    将以下代码段粘贴到空白单元格中，并按 **SHIFT + ENTER**。\r \r        # SystemInfo here is a combination of system ID followed by system age\r        Document = Row(\"id\", \"SystemInfo\")\r        test = sc.parallelize([(1L, \"20 25\"),\r                      (2L, \"4 15\"),\r                      (3L, \"16 9\"),\r                      (4L, \"9 22\"),\r                      (5L, \"17 10\"),\r                      (6L, \"7 22\")]) \\\r            .map(lambda x: Document(*x)).toDF() \r            \r 2. 最后，对测试数据进行预测。 将以下代码段粘贴到空白单元格中，并按 **SHIFT + ENTER**。\r \r         # Make predictions on test documents and print columns of interest\r         prediction = model.transform(test)\r         selected = prediction.select(\"SystemInfo\", \"prediction\", \"probability\")\r         for row in selected.collect():\r             print row\r             \r 3. 应该会看到与下面类似的输出：\r \r         Row(SystemInfo=u'20 25', prediction=1.0, probability=DenseVector([0.4999, 0.5001]))\r         Row(SystemInfo=u'4 15', prediction=0.0, probability=DenseVector([0.5016, 0.4984]))\r         Row(SystemInfo=u'16 9', prediction=1.0, probability=DenseVector([0.4785, 0.5215]))\r         Row(SystemInfo=u'9 22', prediction=1.0, probability=DenseVector([0.4549, 0.5451]))\r         Row(SystemInfo=u'17 10', prediction=1.0, probability=DenseVector([0.4925, 0.5075]))\r         Row(SystemInfo=u'7 22', prediction=0.0, probability=DenseVector([0.5015, 0.4985]))\r \r    从预测中的第一行可以看出，对于 ID 为 20 且系统年数为 25 的 HVAC 系统，建筑物处于高温状态 (**prediction=1.0**)。 DenseVector (0.49999) 的第一个值对应于预测 0.0，第二个值 (0.5001) 对应于预测 1.0。 在输出中，即使第二个值仅稍微偏高，模型仍显示 **prediction=1.0**。\r 4. 完成运行应用程序之后，应该要关闭笔记本以释放资源。 为此，请在 Notebook 的“文件”菜单中，单击“关闭并停止”。 这将会关闭 notebook。\r \r ## <a name=\"anaconda\"></a>将 Anaconda scikit-learn 库用于 Spark 机器学习\r HDInsight 上的 Apache Spark 群集包含 Anaconda 库， 还包含适用于机器学习的 **scikit-learn** 库。 该库还包含可用于直接从 Jupyter notebook 生成示例应用程序的各种数据集。 有关使用 scikit-learn 库的示例，请参阅 [http://scikit-learn.org/stable/auto_examples/index.html](http://scikit-learn.org/stable/auto_examples/index.html)。\r \r ## <a name=\"seealso\"></a>另请参阅\r * [概述：Azure HDInsight 上的 Apache Spark](apache-spark-overview.md)\r \r ### <a name=\"scenarios\"></a>方案\r * [Spark 和 BI：使用 HDInsight 中的 Spark 和 BI 工具执行交互式数据分析](apache-spark-use-bi-tools.md)\r * [Spark 和机器学习：使用 HDInsight 中的 Spark 预测食品检查结果](apache-spark-machine-learning-mllib-ipython.md)\r * [Spark 流式处理：使用 HDInsight 中的 Spark 生成实时流式处理应用程序](apache-spark-eventhub-streaming.md)\r * [使用 HDInsight 中的 Spark 分析网站日志](apache-spark-custom-library-website-log-analysis.md)\r \r ### <a name=\"create-and-run-applications\"></a>创建和运行应用程序\r * [使用 Scala 创建独立的应用程序](apache-spark-create-standalone-application.md)\r * [使用 Livy 在 Spark 群集中远程运行作业](apache-spark-livy-rest-interface.md)\r \r ### <a name=\"tools-and-extensions\"></a>工具和扩展\r * [使用用于 IntelliJ IDEA 的 HDInsight 工具插件创建和提交 Spark Scala 应用程序](apache-spark-intellij-tool-plugin.md)\r * [使用用于 IntelliJ IDEA 的 HDInsight 工具插件远程调试 Spark 应用程序](apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)\r * [在 HDInsight 上的 Spark 群集中使用 Zeppelin 笔记本](apache-spark-zeppelin-notebook.md)\r * [在 HDInsight 的 Spark 群集中可用于 Jupyter 笔记本的内核](apache-spark-jupyter-notebook-kernels.md)\r * [Use external packages with Jupyter notebooks（将外部包与 Jupyter 笔记本配合使用）](apache-spark-jupyter-notebook-use-external-packages.md)\r * [Install Jupyter on your computer and connect to an HDInsight Spark cluster（在计算机上安装 Jupyter 并连接到 HDInsight Spark 群集）](apache-spark-jupyter-notebook-install-locally.md)\r \r ### <a name=\"manage-resources\"></a>管理资源\r * [管理 Azure HDInsight 中 Apache Spark 群集的资源](apache-spark-resource-manager.md)\r * [Track and debug jobs running on an Apache Spark cluster in HDInsight（跟踪和调试 HDInsight 中的 Apache Spark 群集上运行的作业）](apache-spark-job-debugging.md)\r \r [hdinsight-versions]: ../hdinsight-component-versioning.md\r [hdinsight-upload-data]: ../hdinsight-upload-data.md\r [hdinsight-storage]: ../hdinsight-hadoop-use-blob-storage.md\r \r \r [azure-purchase-options]: https://www.azure.cn/pricing/overview/\r [azure-member-offers]: https://www.azure.cn/pricing/member-offers/\r [azure-trial]: https://www.azure.cn/pricing/1rmb-trial/\r <!--Update_Description: wording meta data only-->"}