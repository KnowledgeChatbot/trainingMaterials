{"Title":"排查 Azure HDInsight 中的 Apache Spark 群集问题","Description":"了解与 Azure HDInsight 中的 Apache Spark 群集相关的问题以及如何解决这些问题。","Content":"# <a name=\"known-issues-for-apache-spark-cluster-on-hdinsight\"></a>HDInsight 上的 Apache Spark 群集的已知问题\r \r 本文档记述了 HDInsight Spark 公共预览版的所有已知问题。  \r \r ## <a name=\"livy-leaks-interactive-session\"></a>Livy 泄漏交互式会话\r 在某个交互式会话仍保持活动状态的情况下，Livy 重新启动（从 Ambari 重新启动，或者由于头节点 0 虚拟机重新启动），同时交互式作业会话泄漏。 因此，新作业可能陷于“已接受”状态中，且无法启动。\r \r **缓解：**\r \r 使用以下过程解决该问题：\r \r 1. 通过 SSH 连接到头节点。 有关信息，请参阅[将 SSH 与 HDInsight 配合使用](../hdinsight-hadoop-linux-use-ssh-unix.md)。\r \r 2. 运行以下命令，以查找通过 Livy 启动的交互式作业的应用程序 ID。 \r \r         yarn application -list\r \r     如果作业是通过未指定明确名称的 Livy 交互式会话启动的，则默认的作业名称是 Livy；对于 Jupyter 笔记本启动的 Livy 会话，作业名称以 remotesparkmagics_* 开头。 \r 3. 运行以下命令以终止这些作业。 \r \r         yarn application -kill <Application ID>\r \r 新作业将开始运行。 \r \r ## <a name=\"spark-history-server-not-started\"></a>Spark History Server未启动\r 创建群集后，Spark History Server 不自动启动。  \r \r **缓解：** \r \r 从 Ambari 手动启动 History Server。\r \r ## <a name=\"permission-issue-in-spark-log-directory\"></a>Spark 日志目录中的权限问题\r 当 hdiuser 通过 spark-submit 提交作业时发生错误 java.io.FileNotFoundException：/var/log/spark/sparkdriver_hdiuser.log（权限被拒绝），且不会写入驱动程序日志。 \r \r **缓解：**\r \r 1. 将 hdiuser 添加到 Hadoop 组。 \r 2. 创建群集后，提供对 /var/log/spark 的 777 权限。 \r 3. 使用 Ambari 将 Spark 日志位置更新为具有 777 权限的目录。  \r 4. 以 sudo 身份运行 spark-submit。  \r \r ## <a name=\"spark-phoenix-connector-is-not-supported\"></a>不支持 Spark-Phoenix 连接器\r \r 目前，HDInsight Spark 群集不支持 Spark-Phoenix 连接器。\r \r **缓解：**\r \r 必须改用 Spark-HBase 连接器。 有关说明，请参阅[如何使用 Spark-HBase 连接器](https://blogs.msdn.microsoft.com/azuredatalake/2016/07/25/hdinsight-how-to-use-spark-hbase-connector/)。\r \r ## <a name=\"issues-related-to-jupyter-notebooks\"></a>Jupyter 笔记本的相关问题\r 下面是与 Jupyter 笔记本相关的一些已知问题。\r \r ### <a name=\"notebooks-with-non-ascii-characters-in-filenames\"></a>笔记本的文件名中包含非 ASCII 字符\r 可在 Spark HDInsight 群集中使用的 Jupyter 笔记本的文件名不应包含非 ASCII 字符。 如果你尝试通过 Jupyter UI 上传文件名中包含非 ASCII 字符的文件，操作会失败且不发出提示（即，Jupyter 不允许你上传该文件，但同时也不引发可视的错误）。 \r \r ### <a name=\"error-while-loading-notebooks-of-larger-sizes\"></a>加载大型笔记本时发生错误\r 加载大型笔记本时，可能会看到错误“ **`Error loading notebook`** 。  \r \r **缓解：**\r \r 收到此错误并不表示数据已损坏或丢失。  笔记本仍在磁盘上的 `/var/lib/jupyter` 中，可以通过 SSH 连接到群集来访问它。 有关信息，请参阅[将 SSH 与 HDInsight 配合使用](../hdinsight-hadoop-linux-use-ssh-unix.md)。\r \r 一旦使用 SSH 连接到群集后，即可将笔记本从群集复制到本地计算机（使用 SCP 或 WinSCP）作为备份，以免丢失笔记本中的重要数据。 然后，可以使用端口 8001 通过 SSH 隧道（不通过网关）连接到头节点来访问 Jupyter。  可以从该处清除笔记本的输出，并将其重新保存，以尽量缩小笔记本的大小。\r \r 若要防止今后发生此错误，必须遵循一些最佳实践：\r \r * 必须保持较小的笔记本大小。 发回到 Jupyter 的所有 Spark 作业输出都会保存在笔记本中。  一般而言，Jupyter 的最佳实践是避免对大型 RDD 或数据帧运行 `.collect()`；如果想要查看 RDD 的内容，请考虑运行 `.take()` 或 `.sample()`，这样，输出便不会太大。\r * 此外，在保存笔记本时，请清除所有输出单元以减小大小。\r \r ### <a name=\"notebook-initial-startup-takes-longer-than-expected\"></a>笔记本初次启动花费的时间比预期要长\r 在使用 Spark magic 的 Jupyter 笔记本中，第一个代码语句可能需要花费一分钟以上。  \r \r **解释：**\r \r 这发生在运行第一个代码单元时。 它在后台启动设置会话配置，以及设置 SQL、Spark 和 Hive 上下文。 设置这些上下文后，第一个语句才运行，因此让人觉得完成语句需要花费很长时间。\r \r ### <a name=\"jupyter-notebook-timeout-in-creating-the-session\"></a>创建会话时 Jupyter 笔记本超时\r 如果 Spark 群集的资源不足，Jupyter 笔记本中的 Spark 和 Pyspark 内核在尝试创建会话时会超时。 \r \r **缓解措施：** \r \r 1. 通过以下方式释放 Spark 群集中的一些资源：\r \r    * 转到“关闭并停止”菜单或单击笔记本资源管理器中的“关闭”，以停止其他 Spark 笔记本。\r    * 通过 YARN 停止其他 Spark 应用程序。\r 2. 重新启动先前尝试启动的笔记本。 现在应有足够的资源用于创建会话。\r \r ## <a name=\"see-also\"></a>另请参阅\r * [概述：Azure HDInsight 上的 Apache Spark](apache-spark-overview.md)\r \r ### <a name=\"scenarios\"></a>方案\r * [Spark 和 BI：使用 HDInsight 中的 Spark 和 BI 工具执行交互式数据分析](apache-spark-use-bi-tools.md)\r * [Spark 和机器学习：使用 HDInsight 中的 Spark 对使用 HVAC 数据生成温度进行分析](apache-spark-ipython-notebook-machine-learning.md)\r * [Spark 和机器学习：使用 HDInsight 中的 Spark 预测食品检查结果](apache-spark-machine-learning-mllib-ipython.md)\r * [Spark 流式处理：使用 HDInsight 中的 Spark 生成实时流式处理应用程序](apache-spark-eventhub-streaming.md)\r * [使用 HDInsight 中的 Spark 分析网站日志](apache-spark-custom-library-website-log-analysis.md)\r \r ### <a name=\"create-and-run-applications\"></a>创建和运行应用程序\r * [使用 Scala 创建独立的应用程序](apache-spark-create-standalone-application.md)\r * [使用 Livy 在 Spark 群集中远程运行作业](apache-spark-livy-rest-interface.md)\r \r ### <a name=\"tools-and-extensions\"></a>工具和扩展\r * [使用用于 IntelliJ IDEA 的 HDInsight 工具插件创建和提交 Spark Scala 应用程序](apache-spark-intellij-tool-plugin.md)\r * [使用用于 IntelliJ IDEA 的 HDInsight 工具插件远程调试 Spark 应用程序](apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)\r * [在 HDInsight 上的 Spark 群集中使用 Zeppelin 笔记本](apache-spark-zeppelin-notebook.md)\r * [在 HDInsight 的 Spark 群集中可用于 Jupyter 笔记本的内核](apache-spark-jupyter-notebook-kernels.md)\r * [Use external packages with Jupyter notebooks（将外部包与 Jupyter 笔记本配合使用）](apache-spark-jupyter-notebook-use-external-packages.md)\r * [Install Jupyter on your computer and connect to an HDInsight Spark cluster（在计算机上安装 Jupyter 并连接到 HDInsight Spark 群集）](apache-spark-jupyter-notebook-install-locally.md)\r \r ### <a name=\"manage-resources\"></a>管理资源\r * [管理 Azure HDInsight 中 Apache Spark 群集的资源](apache-spark-resource-manager.md)\r * [Track and debug jobs running on an Apache Spark cluster in HDInsight（跟踪和调试 HDInsight 中的 Apache Spark 群集上运行的作业）](apache-spark-job-debugging.md)\r \r \r \r <!--Update_Description: update wording and link references-->"}