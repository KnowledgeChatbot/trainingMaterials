{"Title":"使用 Caffe on Azure HDInsight Spark 进行分布式深度学习","Description":"使用 Caffe on Azure HDInsight Spark 进行分布式深度学习","Content":"# <a name=\"use-caffe-on-azure-hdinsight-spark-for-distributed-deep-learning\"></a>使用 Caffe on Azure HDInsight Spark 进行分布式深度学习\r \r ## <a name=\"introduction\"></a>介绍\r \r 深度学习正在影响我们生活中的方方面面，从医疗保健到交通运输到生产制造，不一而足。 很多公司都在考虑通过深度学习来解决各种棘手的问题，例如[图像分类](http://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/)、[语音识别](http://googleresearch.blogspot.jp/2015/08/the-neural-networks-behind-google-voice.html)、物体识别、机器翻译。 \r \r 有[许多常用框架](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software)，其中包括 [Microsoft 认知工具包](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/)、[Tensorflow](https://www.tensorflow.org/)、MXNet、Theano 等。Caffe 是最著名的非符号（命令式）神经网络框架之一，广泛用于包括计算机视觉在内的许多领域。 此外，[CaffeOnSpark](http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep) 将 Caffe 与 Apache Spark 相结合，因此，可在现有 Hadoop 集群上轻松使用深度学习。 可将深度学习与 Spark ETL 管道搭配使用，降低系统复杂性和完整解决方案学习中的延迟。\r \r [HDInsight](https://www.azure.cn/home/features/hdinsight/) 是云 Hadoop 产品，为 Spark、Hive、Hadoop、HBase、Storm、Kafka 提供优化的开源分析群集。 HDInsight 提供 99.9% SLA 支持。 这些大数据技术和 ISV 应用程序均可轻松部署为受企业保护和监视的托管群集。\r \r 本文演示如何为 HDInsight 群集安装 [CaffeonSpark](https://github.com/yahoo/CaffeOnSpark)。 本文还内置了 MNIST 演示，展示如何通过 CPU 上的 HDInsight Spark 使用分布式深度学习。\r \r 需完成四大步骤才能让其在 HDInsight 上运行。\r \r 1. 在所有节点上安装必需的依赖项\r 2. 在头节点上生成 Caffe on Spark for HDInsight\r 3. 将所需库分发到所有工作节点\r 4. 编写 Caffe 模型，以分布式方式运行。\r \r HDInsight 是一种 PaaS 解决方案，因此提供了出色的平台功能，可以轻松地执行某些任务。 我们在本博客文章中多次使用的一项功能称为[脚本操作](/hdinsight/hdinsight-hadoop-customize-cluster-linux)，适合用于执行 Shell 命令自定义群集节点（头节点、工作节点或边缘节点）。\r \r ## <a name=\"step-1--install-the-required-dependencies-on-all-the-nodes\"></a>步骤 1：在所有节点上安装必需的依赖项\r \r 若要开始，需安装所需依赖项。 Caffe 站点和 [CaffeOnSpark 站点](https://github.com/yahoo/CaffeOnSpark/wiki/GetStarted_yarn)提供一些有用 wiki，用于安装 Spark on YARN 模式的依赖项。 HDInsight 也使用 Spark on YARN 模式。 但是，我们还需要为 HDInsight 平台添加一些其他依赖项。 因此，我们使用脚本操作并让其在所有头节点和工作节点上运行。 该脚本操作需时约 20 分钟，因为那些依赖项也依赖于其他包。 应将其置于某个可供 HDInsight 群集访问的位置，例如置于 GitHub 或默认的 BLOB 存储帐户中。\r \r     #!/bin/bash\r     #Please be aware that installing the below will add additional 20 mins to cluster creation because of the dependencies\r     #installing all dependencies, including the ones mentioned in http://caffe.berkeleyvision.org/install_apt.html, as well a few packages that are not included in HDInsight, such as gflags, glog, lmdb, numpy\r     #It seems numpy will only needed during compilation time, but for safety purpose we install them on all the nodes\r \r     sudo apt-get install -y libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler maven libatlas-base-dev libgflags-dev libgoogle-glog-dev liblmdb-dev build-essential  libboost-all-dev python-numpy python-scipy python-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose\r \r     #install protobuf\r     wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz\r     sudo tar xzvf protobuf-2.5.0.tar.gz -C /tmp/\r     cd /tmp/protobuf-2.5.0/\r     sudo ./configure\r     sudo make\r     sudo make check\r     sudo make install\r     sudo ldconfig\r     echo \"protobuf installation done\"\r \r 脚本操作有两个步骤。 第一步是安装所有必需的库。 这些库包括编译 Caffe 所必需的库（例如 gflags、glog）和运行 Caffe 所必需的库（例如 numpy）。 考虑到 CPU 优化，我们使用 libatlas，但始终可以按照 CaffeOnSpark Wiki 上的说明操作，安装其他优化库，例如 MKL 或 CUDA（适合 GPU）。\r \r 第二步是在运行时下载、编译和安装适用于 Caffe 的 protobuf 2.5.0。 Protobuf 2.5.0 [是必需的](https://github.com/yahoo/CaffeOnSpark/issues/87)，但 Ubuntu 16 不提供包形式的该版本，因此需从源代码对其进行编译。 Internet 上也有一些介绍其编译方法的资源。 有关详细信息，请参阅[此文](http://jugnu-life.blogspot.com/2013/09/install-protobuf-25-on-ubuntu.html)。\r \r 若要开始，可直接针对群集的所有工作节点和头节点运行此脚本操作（适用于 HDInsight 3.5）。 可在现有群集上运行脚本操作，或在群集创建过程中使用脚本操作。 有关脚本操作的详细信息，请参阅[此处](/hdinsight/hdinsight-hadoop-customize-cluster-linux#view-history-promote-and-demote-script-actions)的文档\r \r ![用于安装依赖项的脚本操作](./media/apache-spark-deep-learning-caffe/Script-Action-1.png)\r \r ## <a name=\"step-2-build-caffe-on-spark-for-hdinsight-on-the-head-node\"></a>步骤 2：在头节点上生成 Caffe on Spark for HDInsight\r \r 第二步是在头节点上生成 Caffe，然后将编译的库分发到所有工作节点。 在此步骤中，必须[使用 SSH 连接到头节点](/hdinsight/hdinsight-hadoop-linux-use-ssh-unix)。 之后，必须执行 [CaffeOnSpark 生成步骤](https://github.com/yahoo/CaffeOnSpark/wiki/GetStarted_yarn)。 下面是使用其他步骤生成 CaffeOnSpark 的脚本。 \r \r     #!/bin/bash\r     git clone https://github.com/yahoo/CaffeOnSpark.git --recursive\r     export CAFFE_ON_SPARK=$(pwd)/CaffeOnSpark\r \r     pushd ${CAFFE_ON_SPARK}/caffe-public/\r     cp Makefile.config.example Makefile.config\r     echo \"INCLUDE_DIRS += ${JAVA_HOME}/include\" >> Makefile.config\r     #Below configurations might need to be updated based on actual cases. For example, if you are using GPU, or using a different BLAS library, you may want to update those settings accordingly.\r     echo \"CPU_ONLY := 1\" >> Makefile.config\r     echo \"BLAS := atlas\" >> Makefile.config\r     echo \"INCLUDE_DIRS += /usr/include/hdf5/serial/\" >> Makefile.config\r     echo \"LIBRARY_DIRS += /usr/lib/x86_64-linux-gnu/hdf5/serial/\" >> Makefile.config\r     popd\r \r     #compile CaffeOnSpark\r     pushd ${CAFFE_ON_SPARK}\r     #always clean up the environment before building (especially when rebuiding), or there will be errors such as \"failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (proto) on project caffe-distri: An Ant BuildException has occured: exec returned: 2\"\r     make clean \r     #the build step usually takes 20~30 mins, since it has a lot maven dependencies\r     make build \r     popd\r     export LD_LIBRARY_PATH=${CAFFE_ON_SPARK}/caffe-public/distribute/lib:${CAFFE_ON_SPARK}/caffe-distri/distribute/lib\r \r     hadoop fs -mkdir -p wasb:///projects/machine_learning/image_dataset\r \r     ${CAFFE_ON_SPARK}/scripts/setup-mnist.sh\r     hadoop fs -put -f ${CAFFE_ON_SPARK}/data/mnist_*_lmdb wasb:///projects/machine_learning/image_dataset/\r \r     ${CAFFE_ON_SPARK}/scripts/setup-cifar10.sh\r     hadoop fs -put -f ${CAFFE_ON_SPARK}/data/cifar10_*_lmdb wasb:///projects/machine_learning/image_dataset/\r \r     #put the already compiled CaffeOnSpark libraries to wasb storage, then read back to each node using script actions. This is because CaffeOnSpark requires all the nodes have the libarries\r     hadoop fs -mkdir -p /CaffeOnSpark/caffe-public/distribute/lib/\r     hadoop fs -mkdir -p /CaffeOnSpark/caffe-distri/distribute/lib/\r     hadoop fs -put CaffeOnSpark/caffe-distri/distribute/lib/* /CaffeOnSpark/caffe-distri/distribute/lib/\r     hadoop fs -put CaffeOnSpark/caffe-public/distribute/lib/* /CaffeOnSpark/caffe-public/distribute/lib/\r \r 除了 CaffeOnSpark 文档所述步骤，可能还需执行其他步骤。 所做更改如下：\r - 更改仅针对 CPU，特此使用 libatlas。\r - 将数据集置于 BLOB 存储，这是一个共享位置，可供所有工作节点在以后使用时访问。\r - 将编译的 Caffe 库置于 BLOB 存储，以便将来使用脚本操作将这些库复制到所有节点，不再需要编译。\r \r \r ### <a name=\"troubleshooting-an-ant-buildexception-has-occurred-exec-returned-2\"></a>故障排除：出现 Ant BuildException: exec 返回: 2\r \r 首次尝试生成 CaffeOnSpark 时，有时会出现以下错误消息：\r \r     failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (proto) on project caffe-distri: An Ant BuildException has occured: exec returned: 2\r \r 运行“make clean”清除代码存储库，再运行“make build”即可解决该问题，前提是依赖项正确。\r \r ### <a name=\"troubleshooting-maven-repository-connection-time-out\"></a>故障排除：Maven 存储库连接超时\r \r 有时 Maven 会出现连接超时错误，类似如下片段：\r \r     Retry:\r     [INFO] Downloading: https://repo.maven.apache.org/maven2/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar\r     Feb 01, 2017 5:14:49 AM org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec execute\r     INFO: I/O exception (java.net.SocketException) caught when processing request to {s}->https://repo.maven.apache.org:443: Connection timed out (Read failed)\r \r 必须在几分钟后重试。\r \r \r ### <a name=\"troubleshooting-test-failure-for-caffe\"></a>故障排除：Caffe 测试失败\r \r 对 CaffeOnSpark 进行最后检查时，可能会出现一条测试失败的消息。 这可能与 UTF-8 编码有关，但不会影响 Caffe 的使用\r \r     Run completed in 32 seconds, 78 milliseconds.\r     Total number of tests run: 7\r     Suites: completed 5, aborted 0\r     Tests: succeeded 6, failed 1, canceled 0, ignored 0, pending 0\r     *** 1 TEST FAILED ***\r \r ## <a name=\"step-3-distribute-the-required-libraries-to-all-the-worker-nodes\"></a>步骤 3：将所需库分发到所有工作节点\r \r 下一步是将库（基本上是 CaffeOnSpark/caffe-public/distribute/lib/ 和 CaffeOnSpark/caffe-distri/distribute/lib/ 中的库）分发到所有节点。 在步骤 2 中，我们将这些库置于 BLOB 存储，而在此步骤中，我们需使用脚本操作将其复制到所有头节点和工作节点。\r \r 因此，请运行脚本操作，如以下片段所示：\r \r     #!/bin/bash\r     hadoop fs -get wasb:///CaffeOnSpark /home/changetoyourusername/\r \r 请确保指向特定群集的正确位置\r \r 我们已在步骤 2 中将其置于可供所有节点访问的 BLOB 存储，因此在此步骤中，只需直接将其复制到所有节点即可。\r \r ## <a name=\"step-4-compose-a-caffe-model-and-run-it-in-a-distributed-manner\"></a>步骤 4：编写 Caffe 模型，以分布式方式运行\r \r 执行上述步骤后，Caffe 安装完成。 下一步是编写 Caffe 模型。 \r \r Caffe 使用“富有表现力的体系结构”，因此若要编写模型，只需定义配置文件即可，根本不需编码（大多数情况下）。 让我们看看实际情况。 \r \r 我们要训练的模型是用于 MNIST 训练的示例模型。 包含手写数字的 MNIST 数据库有一个 60,000 示例的训练集，还有一个 10,000 示例的测试集。 它是 NIST 提供的更大型集的子集。 这些数字已在大小方面规范化，在固定大小的图像中居中。 CaffeOnSpark 提供的一些脚本可以下载该数据集并将其转换成正确的格式。\r \r CaffeOnSpark 提供了一些用于 MNIST 培训的网络拓扑示例。 它具有良好的设计，将网络体系结构（网络拓扑）和优化进行了拆分。 在本示例中，需要两个文件： \r \r “解算器”文件 (${CAFFE_ON_SPARK}/data/lenet_memory_solver.prototxt) 用于监控优化情况和生成参数更新。 例如，它可以定义是使用 CPU 还是 GPU，以及具体的动量和迭代次数等。它还定义程序应使用哪个神经元网络拓扑（即我们需要的第二个文件）。 有关解算器的详细信息，请参阅 [Caffe 文档](http://caffe.berkeleyvision.org/tutorial/solver.html)。\r \r 就此示例来说，我们使用的是 CPU 而不是 GPU，因此应将最后一行更改为：\r \r     # solver mode: CPU or GPU\r     solver_mode: CPU\r \r ![Caffe 配置](./media/apache-spark-deep-learning-caffe/Caffe-1.png)\r \r 可以根据需要更改其他行。\r \r 第二个文件 (${CAFFE_ON_SPARK}/data/lenet_memory_train_test.prototxt) 定义神经元网络的情况，以及相关的输入和输出文件。 此外还需根据训练数据位置更新文件。 更改 lenet_memory_train_test.prototxt 中的以下部分（需指向特定于群集的正确位置）：\r \r - 将 \"file:/Users/mridul/bigml/demodl/mnist_train_lmdb\" 更改为 \"wasb:///projects/machine_learning/image_dataset/mnist_train_lmdb\"\r - 将 \"file:/Users/mridul/bigml/demodl/mnist_test_lmdb/\" 更改为 \"wasb:///projects/machine_learning/image_dataset/mnist_test_lmdb\"\r \r ![Caffe 配置](./media/apache-spark-deep-learning-caffe/Caffe-2.png)\r \r 如需详细了解如何定义网络，请查看[有关 MNIST 数据集的 Caffe 文档](http://caffe.berkeleyvision.org/gathered/examples/mnist.html)\r \r 对于本文，我们使用此 MNIST 示例。 从头节点运行以下命令：\r \r     spark-submit --master yarn --deploy-mode cluster --num-executors 8 --files ${CAFFE_ON_SPARK}/data/lenet_memory_solver.prototxt,${CAFFE_ON_SPARK}/data/lenet_memory_train_test.prototxt --conf spark.driver.extraLibraryPath=\"${LD_LIBRARY_PATH}\" --conf spark.executorEnv.LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}\" --class com.yahoo.ml.caffe.CaffeOnSpark ${CAFFE_ON_SPARK}/caffe-grid/target/caffe-grid-0.1-SNAPSHOT-jar-with-dependencies.jar -train -features accuracy,loss -label label -conf lenet_memory_solver.prototxt -devices 1 -connection ethernet -model wasb:///mnist.model -output wasb:///mnist_features_result\r \r 上述命令将所需的文件（lenet_memory_solver.prototxt 和 lenet_memory_train_test.prototxt）分发给每个 YARN 容器。 该命令还将每个 Spark 驱动程序/执行程序的相关路径设置为 LD_LIBRARY_PATH。 LD_LIBRARY_PATH 在前面的代码片段中定义并指向具有 CaffeOnSpark 库的位置。 \r \r ## <a name=\"monitoring-and-troubleshooting\"></a>监视和故障排除\r \r 我们使用 YARN 群集模式，可将 Spark 驱动程序调度到任意容器（以及任意工作节点），因此你会在控制台中看到类似如下的输出：\r \r     17/02/01 23:22:16 INFO Client: Application report for application_1485916338528_0015 (state: RUNNING)\r \r 若要了解所发生的情况，通常需获取 Spark 驱动程序的日志，其中包含详细信息。 在本例中，需转到 YARN UI 查找相关的 YARN 日志。 可通过以下 URL 获取 YARN UI： \r \r     https://yourclustername.azurehdinsight.cn/yarnui\r \r ![YARN UI](./media/apache-spark-deep-learning-caffe/YARN-UI-1.png)\r \r 可以看看为这个特定的应用程序分配了多少资源。 单击“计划程序”链接即可查看此应用程序的资源分配情况，有 9 个容器正在运行。 我们要求 YARN 提供 8 个执行程序，另一个容器用于驱动程序进程。 \r \r ![YARN 计划程序](./media/apache-spark-deep-learning-caffe/YARN-Scheduler.png)\r \r 如果发生故障，可能需要查看驱动程序日志或容器日志。 要查看驱动程序日志，可在 YARN UI 中单击应用程序 ID，并单击“日志”按钮。 此时驱动程序日志会写入 stderr 中。\r \r ![YARN UI 2](./media/apache-spark-deep-learning-caffe/YARN-UI-2.png)\r \r 例如，可能会显示下面列出的来自驱动程序日志的部分错误，指示用户分配的执行程序过多。\r \r     17/02/01 07:26:06 ERROR ApplicationMaster: User class threw exception: java.lang.IllegalStateException: Insufficient training data. Please adjust hyperparameters or increase dataset.\r     java.lang.IllegalStateException: Insufficient training data. Please adjust hyperparameters or increase dataset.\r         at com.yahoo.ml.caffe.CaffeOnSpark.trainWithValidation(CaffeOnSpark.scala:261)\r         at com.yahoo.ml.caffe.CaffeOnSpark$.main(CaffeOnSpark.scala:42)\r         at com.yahoo.ml.caffe.CaffeOnSpark.main(CaffeOnSpark.scala)\r         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r         at java.lang.reflect.Method.invoke(Method.java:498)\r         at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)\r \r 有时，问题可能会发生在执行程序而非驱动程序中。 在这种情况下，需检查容器日志。 始终可以获取容器日志，然后获取发生故障的容器。 例如，可能会在运行 Caffe 时遇到这种故障。\r \r     17/02/01 07:12:05 WARN YarnAllocator: Container marked as failed: container_1485916338528_0008_05_000005 on host: 10.0.0.14. Exit status: 134. Diagnostics: Exception from container-launch.\r     Container id: container_1485916338528_0008_05_000005\r     Exit code: 134\r     Exception message: /bin/bash: line 1: 12230 Aborted                 (core dumped) LD_LIBRARY_PATH=/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/home/xiaoyzhu/CaffeOnSpark/caffe-public/distribute/lib:/home/xiaoyzhu/CaffeOnSpark/caffe-distri/distribute/lib /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx4608m '-Dhdp.version=' '-Detwlogger.component=sparkexecutor' '-DlogFilter.filename=SparkLogFilters.xml' '-DpatternGroup.filename=SparkPatternGroups.xml' '-Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer' '-Dlog4jspark.log.dir=/var/log/sparkapp/${user.name}' '-Dlog4jspark.log.file=sparkexecutor.log' '-Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties' '-Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl' -Djava.io.tmpdir=/mnt/resource/hadoop/yarn/local/usercache/xiaoyzhu/appcache/application_1485916338528_0008/container_1485916338528_0008_05_000005/tmp '-Dspark.driver.port=43942' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1485916338528_0008/container_1485916338528_0008_05_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.0.0.13:43942 --executor-id 4 --hostname 10.0.0.14 --cores 3 --app-id application_1485916338528_0008 --user-class-path file:/mnt/resource/hadoop/yarn/local/usercache/xiaoyzhu/appcache/application_1485916338528_0008/container_1485916338528_0008_05_000005/__app__.jar > /mnt/resource/hadoop/yarn/log/application_1485916338528_0008/container_1485916338528_0008_05_000005/stdout 2> /mnt/resource/hadoop/yarn/log/application_1485916338528_0008/container_1485916338528_0008_05_000005/stderr\r \r     Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 12230 Aborted                 (core dumped) LD_LIBRARY_PATH=/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/home/xiaoyzhu/CaffeOnSpark/caffe-public/distribute/lib:/home/xiaoyzhu/CaffeOnSpark/caffe-distri/distribute/lib /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx4608m '-Dhdp.version=' '-Detwlogger.component=sparkexecutor' '-DlogFilter.filename=SparkLogFilters.xml' '-DpatternGroup.filename=SparkPatternGroups.xml' '-Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer' '-Dlog4jspark.log.dir=/var/log/sparkapp/${user.name}' '-Dlog4jspark.log.file=sparkexecutor.log' '-Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties' '-Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl' -Djava.io.tmpdir=/mnt/resource/hadoop/yarn/local/usercache/xiaoyzhu/appcache/application_1485916338528_0008/container_1485916338528_0008_05_000005/tmp '-Dspark.driver.port=43942' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1485916338528_0008/container_1485916338528_0008_05_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.0.0.13:43942 --executor-id 4 --hostname 10.0.0.14 --cores 3 --app-id application_1485916338528_0008 --user-class-path file:/mnt/resource/hadoop/yarn/local/usercache/xiaoyzhu/appcache/application_1485916338528_0008/container_1485916338528_0008_05_000005/__app__.jar > /mnt/resource/hadoop/yarn/log/application_1485916338528_0008/container_1485916338528_0008_05_000005/stdout 2> /mnt/resource/hadoop/yarn/log/application_1485916338528_0008/container_1485916338528_0008_05_000005/stderr\r \r         at org.apache.hadoop.util.Shell.runCommand(Shell.java:933)\r         at org.apache.hadoop.util.Shell.run(Shell.java:844)\r         at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1123)\r         at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:225)\r         at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:317)\r         at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:83)\r         at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r         at java.lang.Thread.run(Thread.java:745)\r \r     Container exited with a non-zero exit code 134\r \r 在这种情况下，需获取发生故障的容器的 ID（在上面的示例中，该 ID 为 container_1485916338528_0008_05_000005）。 然后需要运行 \r \r     yarn logs -containerId container_1485916338528_0008_03_000005\r \r （从头节点）。 查看容器故障后，会发现该故障的原因是在 lenet_memory_solver.prototxt 中使用了 GPU 模式（本应使用 CPU 模式）。\r \r     17/02/01 07:10:48 INFO LMDB: Batch size:100\r     WARNING: Logging before InitGoogleLogging() is written to STDERR\r     F0201 07:10:48.309725 11624 common.cpp:79] Cannot use GPU in CPU-only Caffe: check mode.\r \r ## <a name=\"getting-results\"></a>获取结果\r \r 由于我们分配的执行程序为 8 个，且网络拓扑简单，获取结果应该只需要大约 30 分钟。 从命令行中，可以看到我们将模型置于 wasb:///mnist.model 中，将结果置于名为 wasb:///mnist_features_result 的文件夹中。\r \r 可以通过运行以下命令获取结果：\r \r     hadoop fs -cat hdfs:///mnist_features_result/*\r \r 结果如下所示：\r \r     {\"SampleID\":\"00009597\",\"accuracy\":[1.0],\"loss\":[0.028171852],\"label\":[2.0]}\r     {\"SampleID\":\"00009598\",\"accuracy\":[1.0],\"loss\":[0.028171852],\"label\":[6.0]}\r     {\"SampleID\":\"00009599\",\"accuracy\":[1.0],\"loss\":[0.028171852],\"label\":[1.0]}\r     {\"SampleID\":\"00009600\",\"accuracy\":[0.97],\"loss\":[0.0677709],\"label\":[5.0]}\r     {\"SampleID\":\"00009601\",\"accuracy\":[0.97],\"loss\":[0.0677709],\"label\":[0.0]}\r     {\"SampleID\":\"00009602\",\"accuracy\":[0.97],\"loss\":[0.0677709],\"label\":[1.0]}\r     {\"SampleID\":\"00009603\",\"accuracy\":[0.97],\"loss\":[0.0677709],\"label\":[2.0]}\r     {\"SampleID\":\"00009604\",\"accuracy\":[0.97],\"loss\":[0.0677709],\"label\":[3.0]}\r     {\"SampleID\":\"00009605\",\"accuracy\":[0.97],\"loss\":[0.0677709],\"label\":[4.0]}\r \r SampleID 表示 MNIST 数据集中的 ID，标签是模型的标识数字。\r \r ## <a name=\"conclusion\"></a>结束语\r \r 在本文档中，用户尝试安装 CaffeOnSpark 并运行一个简单的示例。 HDInsight 是完全托管的云分布式计算平台，尤其适合在大型数据集上运行机器学习和高级分析工作负荷，而对于分布式深度学习，可以使用 Caffe on HDInsight Spark 执行深度学习任务。\r \r ## <a name=\"seealso\"></a>另请参阅\r * [概述：Azure HDInsight 上的 Apache Spark](apache-spark-overview.md)\r \r ### <a name=\"scenarios\"></a>方案\r * [Spark 和机器学习：使用 HDInsight 中的 Spark 对使用 HVAC 数据生成温度进行分析](apache-spark-ipython-notebook-machine-learning.md)\r * [Spark 和机器学习：使用 HDInsight 中的 Spark 预测食品检查结果](apache-spark-machine-learning-mllib-ipython.md)\r \r ### <a name=\"manage-resources\"></a>管理资源\r * [管理 Azure HDInsight 中 Apache Spark 群集的资源](apache-spark-resource-manager.md)\r \r <!--Update_Description: update wording and link references-->"}