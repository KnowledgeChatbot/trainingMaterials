{"Title":"在 HDInsight 中上传 Hadoop 作业的数据","Description":"了解如何在 HDInsight 中使用 Azure CLI、Azure 存储资源管理器、Azure PowerShell、Hadoop 命令行或 Sqoop 上传和访问 Hadoop 作业的数据。","Content":"# <a name=\"upload-data-for-hadoop-jobs-in-hdinsight\"></a>在 HDInsight 中上传 Hadoop 作业的数据\r Azure HDInsight 在 Azure 存储之上提供了一个功能完备的 Hadoop 分布式文件系统 (HDFS)。 该系统为一个 HDFS 扩展，可为客户提供无缝体验。 在该系统的帮助下，Hadoop 生态系统中的整套组件能够直接操作其管理的数据。 Azure Blob 存储和 HDFS 是独立的文件系统，并且已针对数据的存储和计算进行了优化。 有关使用 Azure 存储的益处的信息，请参阅[将 Azure 存储与 HDInsight 配合使用][hdinsight-storage]。\r \r ## <a name=\"prerequisites\"></a>先决条件\r \r 在开始下一步之前，请注意以下要求：\r \r * 一个 Azure HDInsight 群集。 有关说明，请参阅 [Azure HDInsight 入门][hdinsight-get-started]或[创建 HDInsight 群集](hdinsight-hadoop-provision-linux-clusters.md)。\r * 学习以下文章：\r \r     - [将 Azure 存储与 HDInsight 配合使用][hdinsight-storage]\r \r ## <a name=\"upload-data-to-azure-storage\"></a>将数据上传到 Azure 存储\r \r ### <a name=\"command-line-utilities\"></a>命令行实用程序\r Microsoft 提供以下实用工具用于操作 Azure 存储：\r \r | 工具 | Linux | OS X | Windows |\r | --- |:---:|:---:|:---:|\r | [Azure 命令行界面][azurecli] |✔ |✔ |✔ |\r | [Azure PowerShell][azure-powershell] | | |✔ |\r | [AzCopy][azure-azcopy] |✔ | |✔ |\r | [Hadoop 命令](#commandline) |✔ |✔ |✔ |\r \r > [!NOTE]\r > 尽管 Azure CLI、Azure PowerShell 和 AzCopy 都可从 Azure 外部使用，但是 Hadoop 命令只能在 HDInsight 群集上使用。 使用该命令只能将数据从本地文件系统载入 Azure 存储。\r >\r >\r \r #### <a id=\"xplatcli\"></a>Azure CLI\r Azure CLI 是一个跨平台工具，可用于管理 Azure 服务。 使用以下步骤将数据上传到 Azure 存储：\r \r [!INCLUDE [use-latest-version](../../includes/hdinsight-use-latest-cli.md)]\r \r 1. [安装和配置适用于 Mac、Linux 和 Windows 的 Azure CLI](../cli-install-nodejs.md)。\r 2. 打开命令提示符、bash 或其他 shell，并使用以下方法对 Azure 订阅进行身份验证。\r \r     ```cli\r     azure login -e AzureChinaCloud\r     ```\r \r     出现提示时，输入订阅的用户名和密码。\r 3. 输入以下命令，列出订阅的存储帐户：\r \r     ```cli\r     azure storage account list\r     ```\r \r 4. 选择包含用户要使用的 Blob 的存储帐户，并使用以下命令检索此帐户的密钥：\r \r     ```cli\r     azure storage account keys list <storage-account-name>\r     ```\r \r     此命令返回**主**密钥和**辅助**密钥。 复制 **主** 密钥值，因为后续步骤要用到它。\r 5. 使用以下命令可检索存储帐户中的 Blob 容器列表：\r \r     ```cli\r     azure storage container list -a <storage-account-name> -k <primary-key>\r     ```\r \r 6. 使用以下命令可将文件上传和下载到 Blob：\r \r    * 上传文件：\r \r         ```cli\r         azure storage blob upload -a <storage-account-name> -k <primary-key> <source-file> <container-name> <blob-name>\r         ```\r \r    * 下载文件：\r \r         ```cli\r         azure storage blob download -a <storage-account-name> -k <primary-key> <container-name> <blob-name> <destination-file>\r         ```\r     \r > [!NOTE]\r > 如果始终使用同一个存储帐户，可以设置以下环境变量，而无需为每条命令指定帐户和密钥：\r >\r > * AZURE\\_STORAGE\\_ACCOUNT：存储帐户名称\r > * AZURE\\_STORAGE\\_ACCESS\\_KEY：存储帐户密钥\r >\r >\r \r #### <a id=\"powershell\"></a>Azure PowerShell\r Azure PowerShell 是一个脚本编写环境，可用于在 Azure 中控制和自动执行工作负荷的部署和管理。 有关配置工作站运行 Azure PowerShell 的信息，请参阅[安装和配置 Azure PowerShell](https://docs.microsoft.com/powershell/azure/overview)。\r \r [!INCLUDE [use-latest-version](../../includes/hdinsight-use-latest-powershell.md)]\r \r **将本地文件上传到 Azure 存储**\r \r 1. 根据[安装和配置 Azure PowerShell](https://docs.microsoft.com/powershell/azure/overview) 中的说明打开 Azure PowerShell 控制台。\r 2. 设置以下脚本中前五个变量的值：\r \r     ```powershell\r     $resourceGroupName = \"<AzureResourceGroupName>\"\r     $storageAccountName = \"<StorageAccountName>\"\r     $containerName = \"<ContainerName>\"\r \r     $fileName =\"<LocalFileName>\"\r     $blobName = \"<BlobName>\"\r \r     # Get the storage account key\r     $storageAccountKey = (Get-AzureRmStorageAccountKey -ResourceGroupName $resourceGroupName -Name $storageAccountName)[0].Value\r     # Create the storage context object\r     $destContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageaccountkey\r \r     # Copy the file from local workstation to the Blob container\r     Set-AzureStorageBlobContent -File $fileName -Container $containerName -Blob $blobName -context $destContext\r     ```\r \r 3. 将脚本粘贴到 Azure PowerShell 控制台中，以运行它来复制文件。\r \r 有关创建用来使用 HDInsight 的 PowerShell 脚本示例，请参阅 [HDInsight 工具](https://github.com/blackmist/hdinsight-tools)。\r \r #### <a id=\"azcopy\"></a>AzCopy\r AzCopy 是一个命令行工具，用于简化将数据传入和传出 Azure 存储帐户的任务。 可以将它作为独立的工具使用，也可以将此工具融入到现有应用程序中。 [下载 AzCopy][azure-azcopy-download]。\r \r AzCopy 语法为：\r \r ```command\r AzCopy <Source> <Destination> [filePattern [filePattern...]] [Options]\r ```\r \r 有关详细信息，请参阅 [AzCopy - 上传/下载 Azure Blob 的文件][azure-azcopy]。\r \r AzCopy on Linux 预览版已推出。  请参阅[宣布推出 AzCopy on Linux 预览版](https://blogs.msdn.microsoft.com/windowsazurestorage/2017/05/16/announcing-azcopy-on-linux-preview/)。\r \r #### <a id=\"commandline\"></a>Hadoop 命令行\r 仅当数据已存在于群集头节点中时，才可以使用 Hadoop 命令行将数据存储到 Azure 存储 Blob。\r \r 若要使用 Hadoop 命令，必须先使用以下方法之一连接到头节点：\r \r * **基于 Windows 的 HDInsight**：[使用远程桌面连接](hdinsight-administer-use-management-portal.md#connect-to-clusters-using-rdp)\r * **基于 Linux 的 HDInsight**：使用 [SSH 或 PuTTY](hdinsight-hadoop-linux-use-ssh-unix.md) 进行连接。\r \r 连接之后，可以使用以下语法将文件上传到存储。\r \r ```bash\r hadoop -copyFromLocal <localFilePath> <storageFilePath>\r ```\r \r 例如 `hadoop fs -copyFromLocal data.txt /example/data/data.txt`\r \r 由于 HDInsight 的默认文件系统在 Azure 存储中，/example/data.txt 实际是在 Azure 存储中。 也可以将该文件表示为：\r \r     wasb:///example/data/data.txt\r \r 或\r \r     wasb://<ContainerName>@<StorageAccountName>.blob.core.chinacloudapi.cn/example/data/davinci.txt\r \r 有关用于处理文件的其他 Hadoop 命令列表，请参阅 [http://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/FileSystemShell.html](http://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/FileSystemShell.html)\r \r > [!WARNING]\r > 在 HBase 群集上，写入数据为 256 KB 时会使用默认块大小。 虽然在使用 HBase Api 或 REST API 时可良好运行，但使用 `hadoop` 或 `hdfs dfs` 命令编写大于 ~12 GB 的数据会导致错误。 有关详细信息，请参阅本文的[在 Blob 上编写时的存储异常](#storageexception)部分。\r >\r >\r \r ### <a name=\"graphical-clients\"></a>图形客户端\r 还有一些应用程序可提供用于 Azure 存储的图形界面。 下表是其中一些应用程序的列表：\r \r | 客户端 | Linux | OS X | Windows |\r | --- |:---:|:---:|:---:|\r | [Azure 存储资源管理器](http://storageexplorer.com/) |✔ |✔ |✔ |\r | [Cloud Storage Studio 2](http://www.cerebrata.com/Products/CloudStorageStudio/) | | |✔ |\r | [CloudXplorer](http://clumsyleaf.com/products/cloudxplorer) | | |✔ |\r | [Azure Resource Manager](http://www.cloudberrylab.com/free-microsoft-azure-explorer.aspx) | | |✔ |\r | [Cyberduck](https://cyberduck.io/) | |✔ |✔ |\r \r \r #### <a id=\"storageexplorer\"></a>Azure 存储资源管理器\r *Azure 存储资源管理器* 是一种用于在 Blob 中检查和更改数据的实用工具。 它是一个免费的开源工具，可从 [http://storageexplorer.com/](http://storageexplorer.com/)下载。 也可以从此链接获取源代码。\r \r 使用该工具之前，必须知道 Azure 存储帐户名和帐户密钥。 有关如何获取此信息的说明，请参阅 [创建、管理或删除存储帐户][azure-create-storage-account]中的“如何：查看、复制和重新生成存储访问密钥”部分。\r \r 1. 运行 Azure 存储资源管理器。 首次运行存储资源管理器时，系统会提示输入“存储帐户名”和“存储帐户密钥”。 如果以前运行过存储资源管理器，请使用  “添加”按钮添加一个新的存储帐户名和密钥。\r \r     输入 HDInsight 群集使用的存储帐户的名称和密钥，然后选择“保存并打开”。\r \r     ![HDI.AzureStorageExplorer][image-azure-storage-explorer]\r 2. 在界面左侧的容器列表中，单击与 HDInsight 群集关联的容器名称。 默认情况下，这是 HDInsight 群集的名称，但如果在创建群集时输入了特定的名称，则该名称可能不同。\r 3. 在工具栏中选择上传图标。\r \r     ![突出显示了上传图标的工具栏](./media/hdinsight-upload-data/toolbar.png)\r 4. 指定要上传的文件，然后单击“打开”。 出现提示时，请选择“上传”将文件上传到存储容器的根目录。 若要将文件上传到特定的路径，请在“目标”字段中输入该路径，然后选择“上传”。\r \r     ![文件上传对话框](./media/hdinsight-upload-data/fileupload.png)\r \r     上传完文件后，可以通过 HDInsight 群集中的作业来使用该文件。\r \r ### <a name=\"mount-azure-storage-as-local-drive\"></a>将 Azure 存储装载为本地驱动器\r 请参阅[将 Azure 存储装载为本地驱动器](http://blogs.msdn.com/b/bigdatasupport/archive/2014/01/09/mount-azure-blob-storage-as-local-drive.aspx)。\r \r ### <a name=\"upload-using-services\"></a>使用服务上传\r \r #### <a id=\"sqoop\"></a>Apache Sqoop\r Sqoop 是一种专用于在 Hadoop 和关系数据库之间传输数据的工具。 可以使用此工具将数据从关系数据库管理系统 (RDBMS)（如 SQL Server、MySQL 或 Oracle）中导入到 Hadoop 分布式文件系统 (HDFS)，在 Hadoop 中使用 MapReduce 或 Hive 转换数据，然后回过来将数据导出到 RDBMS。\r \r 有关详细信息，请参阅[将 Sqoop 与 HDInsight 配合使用][hdinsight-use-sqoop]。\r \r ### <a name=\"development-sdks\"></a>开发 SDK\r 还可以使用 Azure SDK 通过以下编程语言来访问 Azure 存储：\r \r * .NET\r * Java\r * Node.js\r * PHP\r * Python\r * Ruby\r \r 有关安装 Azure SDK 的详细信息，请参阅 [Azure 下载](/downloads/)\r \r ### <a name=\"troubleshooting\"></a>故障排除\r #### <a id=\"storageexception\"></a>写入 blob 时的存储异常\r **症状**：使用 `hadoop` 或 `hdfs dfs` 命令在 HBase 群集上编写大于或等于 ~12 GB 的文件时，可能会遇到以下错误：\r \r     ERROR azure.NativeAzureFileSystem: Encountered Storage Exception for write on Blob : example/test_large_file.bin._COPYING_ Exception details: null Error Code : RequestBodyTooLarge\r     copyFromLocal: java.io.IOException\r             at com.microsoft.azure.storage.core.Utility.initIOException(Utility.java:661)\r             at com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:366)\r             at com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:350)\r             at java.util.concurrent.FutureTask.run(FutureTask.java:262)\r             at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r             at java.util.concurrent.FutureTask.run(FutureTask.java:262)\r             at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r             at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r             at java.lang.Thread.run(Thread.java:745)\r     Caused by: com.microsoft.azure.storage.StorageException: The request body is too large and exceeds the maximum permissible limit.\r             at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)\r             at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\r             at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:182)\r             at com.microsoft.azure.storage.blob.CloudBlockBlob.uploadBlockInternal(CloudBlockBlob.java:816)\r             at com.microsoft.azure.storage.blob.CloudBlockBlob.uploadBlock(CloudBlockBlob.java:788)\r             at com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:354)\r             ... 7 more\r \r **原因**：HBase on HDInsight 群集在写入 Azure 存储时默认阻止 256KB 大小的块。 尽管这对 HBase API 或 REST API 来说可良好运行，但使用 `hadoop` 或 `hdfs dfs` 命令行实用工具时则会导致错误。\r \r 解决方法：使用 `fs.azure.write.request.size` 指定更大的块大小。 可以使用 `-D` 参数基于使用情况执行此操作。 以下命令是将此参数用于 `hadoop` 命令的示例：\r \r ```bash\r hadoop -fs -D fs.azure.write.request.size=4194304 -copyFromLocal test_large_file.bin /example/data\r ```\r \r 还可以使用 Ambari 全局增加 `fs.azure.write.request.size` 的值。 可以使用以下步骤在 Ambari Web UI 中更改该值：\r \r 1. 在浏览器中，转到群集的 Ambari Web UI。 网址为 https://CLUSTERNAME.azurehdinsight.cn，其中 CLUSTERNAME 是群集的名称。\r \r     出现提示时，输入群集的管理员名称和密码。\r 2. 在屏幕左侧选择“HDFS”，然后选择“配置”选项卡。\r 3. 在“筛选...”字段中输入 `fs.azure.write.request.size`。 这会在页面中间显示字段和当前值。\r 4. 将值从 262144 (256KB) 更改为新的值。 例如，4194304 (4MB)。\r \r ![通过 Ambari Web UI 更改值的图像](./media/hdinsight-upload-data/hbase-change-block-write-size.png)\r \r 有关如何使用 Ambari 的详细信息，请参阅[使用 Ambari Web UI 管理 HDInsight 群集](hdinsight-hadoop-manage-ambari.md)。\r \r ## <a name=\"next-steps\"></a>后续步骤\r 现在，已了解如何将数据导入 HDInsight，请阅读以下文章了解如何执行分析：\r \r * [Azure HDInsight 入门][hdinsight-get-started]\r * [以编程方式提交 Hadoop 作业][hdinsight-submit-jobs]\r * [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]\r * [将 Pig 与 HDInsight 配合使用][hdinsight-use-pig]\r \r [azure-management-portal]: https://portal.azure.cn\r [azure-powershell]: http://msdn.microsoft.com/library/windowsazure/jj152841.aspx\r \r [azure-storage-client-library]: ../storage/blobs/storage-dotnet-how-to-use-blobs.md\r [azure-create-storage-account]:../storage/common/storage-create-storage-account.md\r [azure-azcopy-download]:../storage/common/storage-use-azcopy.md\r [azure-azcopy]:../storage/common/storage-use-azcopy.md\r \r [hdinsight-use-sqoop]: hdinsight-use-sqoop.md\r \r [hdinsight-storage]: hdinsight-hadoop-use-blob-storage.md\r [hdinsight-submit-jobs]: hadoop/submit-apache-hadoop-jobs-programmatically.md.md\r [hdinsight-get-started]: hdinsight-hadoop-linux-tutorial-get-started.md\r \r [hdinsight-use-hive]: hdinsight-use-hive.md\r [hdinsight-use-pig]: hdinsight-use-pig.md\r \r [apache-sqoop-guide]: http://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html\r \r [Powershell-install-configure]: https://docs.microsoft.com/powershell/azureps-cmdlets-docs\r \r [azurecli]: ../cli-install-nodejs.md\r \r [image-azure-storage-explorer]: ./media/hdinsight-upload-data/HDI.AzureStorageExplorer.png\r [image-ase-addaccount]: ./media/hdinsight-upload-data/HDI.ASEAddAccount.png\r [image-ase-blob]: ./media/hdinsight-upload-data/HDI.ASEBlob.png\r \r <!--Update_Description: update wording and link references-->"}