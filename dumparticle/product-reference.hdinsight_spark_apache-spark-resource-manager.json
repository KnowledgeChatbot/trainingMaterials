{"Title":"管理 Azure HDInsight 上 Apache Spark 群集的资源","Description":"了解如何管理 Azure HDInsight 上 Spark 群集的资源以提高性能。","Content":"# <a name=\"manage-resources-for-apache-spark-cluster-on-azure-hdinsight\"></a>管理 Azure HDInsight 上 Apache Spark 群集的资源 \r \r 本文介绍如何访问与 Spark 群集关联的界面，例如 Ambari UI、YARN UI 和 Spark History Server。 此外，介绍如何优化群集配置以获得最佳性能。\r \r **先决条件：**\r \r * Azure 订阅。 请参阅[获取 Azure 试用版](https://www.azure.cn/pricing/1rmb-trial/)。\r * HDInsight 上的 Apache Spark 群集。 有关说明，请参阅[在 Azure HDInsight 中创建 Apache Spark 群集](apache-spark-jupyter-spark-sql.md)。\r \r ## <a name=\"how-do-i-launch-the-ambari-web-ui\"></a>如何启动 Ambari Web UI？\r 1. 在 [Azure 门户](https://portal.azure.cn/)上的启动板中，单击 Spark 群集的磁贴（如果已将它固定到启动板）。 也可以单击“全部浏览” > “HDInsight 群集”导航到群集。\r 2. 单击 Spark 群集的“仪表板”。 出现提示时，输入 Spark 群集的管理员凭据。\r \r     ![启动 Ambari](./media/apache-spark-resource-manager/hdinsight-launch-cluster-dashboard.png \"启动 Resource Manager\")\r     \r 3. 这应会启动 Ambari Web UI，如以下屏幕截图所示。\r \r     ![Ambari Web UI](./media/apache-spark-resource-manager/ambari-web-ui.png \"Ambari Web UI\")   \r \r ## <a name=\"how-do-i-launch-the-spark-history-server\"></a>如何启动 Spark History Server？\r 1. 在 [Azure 门户](https://portal.azure.cn/)上的启动板中，单击 Spark 群集的磁贴（如果已将它固定到启动板）。\r 2. 在群集边栏选项卡中的“快速链接”下，单击“群集仪表板”。 在“群集仪表板”边栏选项卡中，单击“Spark History Server”。\r \r     ![](./media/apache-spark-resource-manager/launch-history-server.png \"\")\r \r     出现提示时，输入 Spark 群集的管理员凭据。\r \r ## <a name=\"how-do-i-launch-the-yarn-ui\"></a>如何启动 Yarn UI？\r 可以使用 YARN UI 监视当前正在 Spark 群集上运行的应用程序。\r \r 1. 在群集边栏选项卡中，单击“群集仪表板”，然后单击“YARN”。\r \r     ![启动 YARN UI](./media/apache-spark-resource-manager/launch-yarn-ui.png)\r \r    > [!TIP]\r    > 或者，也可以从 Ambari UI 启动 YARN UI。 若要启动 Ambari UI，请在群集边栏选项卡中单击“群集仪表板”，然后单击“HDInsight 群集仪表板”。 在 Ambari UI 中依次单击“YARN”、“快速链接”、活动的 Resource Manager 和“ResourceManager UI”。\r    >\r    >\r \r ## <a name=\"what-is-the-optimum-cluster-configuration-to-run-spark-applications\"></a>用于运行 Spark 应用程序的最佳群集配置是什么？\r 根据应用程序的要求，可用于 Spark 配置的三个关键参数为 `spark.executor.instances`、`spark.executor.cores` 和 `spark.executor.memory`。 执行器是针对 Spark 应用程序启动的进程。 它在工作节点上运行，负责执行应用程序的任务。 执行器的默认数目和每个群集的执行器大小均根据工作节点数目和工作节点大小计算。 这些信息存储在群集头节点上的 `spark-defaults.conf` 中。\r \r 这三个配置参数可在群集级别配置（适用于群集上运行的所有应用程序），也可以针对每个应用程序指定。\r \r ### <a name=\"change-the-parameters-using-ambari-ui\"></a>使用 Ambari UI 更改参数\r 1. 在 Ambari UI 中单击“Spark”，单击“配置”，并展开“自定义 spark-defaults”。\r \r     ![使用 Ambari 设置参数](./media/apache-spark-resource-manager/set-parameters-using-ambari.png)\r     \r 2. 默认值（在群集上并发运行 4 个 Spark 应用程序）是合理的。 可以从用户界面更改这些值，如下所示。\r \r     ![使用 Ambari 设置参数](./media/apache-spark-resource-manager/set-executor-parameters.png)\r     \r 3. 单击“保存”  保存配置更改。 在页面顶部，系统会提示是否重启所有受影响的服务。 单击“重新启动” 。\r \r     ![重新启动服务](./media/apache-spark-resource-manager/restart-services.png)\r \r ### <a name=\"change-the-parameters-for-an-application-running-in-jupyter-notebook\"></a>更改 Jupyter 笔记本中运行的应用程序的参数\r 对于在 Jupyter 笔记本中运行的应用程序，可以使用 `%%configure` magic 进行配置更改。 理想情况下，必须先在应用程序开头进行此类更改，然后再运行第一个代码单元。 这可以确保在创建 Livy 会话时会配置应用到该会话。 如果想要更改处于应用程序中后面某个阶段的配置，必须使用 `-f` 参数。 但是，这样做会使应用程序中的所有进度丢失。\r \r 以下片段演示如何更改在 Jupyter 中运行的应用程序的配置。\r \r     %%configure\r     {\"executorMemory\": \"3072M\", \"executorCores\": 4, \"numExecutors\":10}\r \r 配置参数必须以 JSON 字符串形式传入，并且必须位于 magic 后面的下一行，如示例列中所示。\r \r ### <a name=\"change-the-parameters-for-an-application-submitted-using-spark-submit\"></a>更改使用 spark-submit 提交的应用程序的参数\r 以下命令示范了如何更改使用 `spark-submit`提交的批处理应用程序的配置参数。\r \r     spark-submit --class <the application class to execute> --executor-memory 3072M --executor-cores 4 --num-executors 10 <location of application jar file> <application parameters>\r \r ### <a name=\"change-the-parameters-for-an-application-submitted-using-curl\"></a>更改使用 cURL 提交的应用程序的参数\r 以下命令示范了如何更改使用 cURL 提交的批处理应用程序的配置参数。\r \r     curl -k -v -H 'Content-Type: application/json' -X POST -d '{\"file\":\"<location of application jar file>\", \"className\":\"<the application class to execute>\", \"args\":[<application parameters>], \"numExecutors\":10, \"executorMemory\":\"2G\", \"executorCores\":5' localhost:8998/batches\r \r ### <a name=\"how-do-i-change-these-parameters-on-a-spark-thrift-server\"></a>如何更改 Spark Thrift 服务器上的这些参数？\r Spark Thrift 服务器提供对 Spark 群集的 JDBC/ODBC 访问，用来为 Spark SQL 查询提供服务。 Power BI、Tableau 等工具 使用 ODBC 协议与 Spark Thrift 服务器通信，以便将 Spark SQL 查询作为 Spark 应用程序执行。 创建 Spark 群集时，将启动 Spark Thrift 服务器的两个实例（每个头节点上各有一个实例）。 在 YARN UI 中，每个 Spark Thrift 服务器显示为一个 Spark 应用程序。\r \r Spark Thrift 服务器使用 Spark 动态执行器分配，因此未使用 `spark.executor.instances` 。 相反，Spark Thrift 服务器使用 `spark.dynamicAllocation.minExecutors` 和 `spark.dynamicAllocation.maxExecutors` 来指定执行器计数。 使用配置参数 `spark.executor.cores` 和 `spark.executor.memory` 可以修改执行器大小。 可遵循以下步骤更改这些参数。\r \r * 展开“高级 spark-thrift-sparkconf”类别可更新参数 `spark.dynamicAllocation.minExecutors`、`spark.dynamicAllocation.maxExecutors` 和 `spark.executor.memory`。\r \r     ![配置 Spark Thrift 服务器](./media/apache-spark-resource-manager/spark-thrift-server-1.png)    \r     \r * 展开“自定义 spark-thrift-sparkconf”类别可更新参数 `spark.executor.cores`。\r \r     ![配置 Spark Thrift 服务器](./media/apache-spark-resource-manager/spark-thrift-server-2.png)\r \r ### <a name=\"how-do-i-change-the-driver-memory-of-the-spark-thrift-server\"></a>如何更改 Spark Thrift 服务器的驱动程序内存？\r Spark Thrift 服务器驱动程序内存配置为头节点 RAM 大小的 25%，前提是头节点的 RAM 总大小大于 14GB。 可按如下所示使用 Ambari UI 更改驱动程序内存配置。\r \r * 在 Ambari UI 中单击“Spark”，单击“配置”，展开“高级 spark-env”，然后提供“spark_thrift_cmd_opts”的值。\r \r     ![配置 Spark Thrift 服务器 RAM](./media/apache-spark-resource-manager/spark-thrift-server-ram.png)\r \r ## <a name=\"i-do-not-use-bi-with-spark-cluster-how-do-i-take-the-resources-back\"></a>我没有在 Spark 群集上使用 BI。 该如何回收资源？\r 因为我们使用 Spark 动态分配，Thrift 服务器使用的唯一资源是两个应用程序主机的资源。 若要回收这些资源，必须停止群集上运行的 Thrift 服务器服务。\r \r 1. 在 Ambari UI 的左侧窗格中，单击“Spark” 。\r 2. 在下一页中，单击“Spark Thrift 服务器” 。\r \r     ![重新启动 Thrift 服务器](./media/apache-spark-resource-manager/restart-thrift-server-1.png)\r     \r 3. 应会看到正在运行 Spark Thrift 服务器的两个头节点。 单击其中一个头节点。\r \r     ![重新启动 Thrift 服务器](./media/apache-spark-resource-manager/restart-thrift-server-2.png)\r     \r 4. 下一页将列出该头节点上运行的所有服务。 在该列表中，单击 Spark Thrift 服务器旁边的下拉按钮，并单击“停止” 。\r \r     ![重新启动 Thrift 服务器](./media/apache-spark-resource-manager/restart-thrift-server-3.png)\r     \r 5. 同时对其他头节点重复上述步骤。\r \r ## <a name=\"my-jupyter-notebooks-are-not-running-as-expected-how-can-i-restart-the-service\"></a>我的 Jupyter 笔记本未按预期运行。 如何重新启动该服务？\r 如上所示启动 Ambari Web UI。 在左侧导航窗格中，依次单击“Jupyter”、“服务操作”和“全部重启”。 随后会在所有头节点上启动 Jupyter 服务。\r \r     ![Restart Jupyter](./media/apache-spark-resource-manager/restart-jupyter.png \"Restart Jupyter\")\r \r ## <a name=\"how-do-i-know-if-i-am-running-out-of-resources\"></a>如何知道资源是否用完了？\r 如上所示启动 Yarn UI。 在屏幕顶部的“群集指标”表中，选中“已用内存”和“内存总计”列的值。 如果这 2 个值非常接近，则不可能资源不足，无法启动下一个应用程序。 这同样适用于“已用 VCore”和“VCore 总计”列。 此外，在主视图中，如果有应用程序保持“已接受”状态，而不转换为“正在运行”或“失败”状态，这也可能指示该应用程序未获得足够的资源来启动。\r \r     ![Resource Limit](./media/apache-spark-resource-manager/resource-limit.png \"Resource Limit\")\r \r ## <a name=\"how-do-i-kill-a-running-application-to-free-up-resource\"></a>如何终止正在运行的应用程序以释放资源？\r 1. 在 Yarn UI 中，从左侧面板中，单击“正在运行”。 在正在运行的应用程序的列表中，确定要终止的应用程序，并单击 **ID**。\r \r     ![终止 App1](./media/apache-spark-resource-manager/kill-app1.png \"终止 App1\")\r \r 2. 单击右上角的“终止应用程序”，然后单击“确定”。\r \r     ![终止 App2](./media/apache-spark-resource-manager/kill-app2.png \"终止 App2\")\r \r ## <a name=\"see-also\"></a>另请参阅\r * [Track and debug jobs running on an Apache Spark cluster in HDInsight（跟踪和调试 HDInsight 中的 Apache Spark 群集上运行的作业）](apache-spark-job-debugging.md)\r \r ### <a name=\"for-data-analysts\"></a>适用于数据分析师\r \r * [Spark 和机器学习：使用 HDInsight 中的 Spark 对使用 HVAC 数据生成温度进行分析](apache-spark-ipython-notebook-machine-learning.md)\r * [Spark 和机器学习：使用 HDInsight 中的 Spark 预测食品检查结果](apache-spark-machine-learning-mllib-ipython.md)\r * [使用 HDInsight 中的 Spark 分析网站日志](apache-spark-custom-library-website-log-analysis.md)\r * [使用 Caffe on Azure HDInsight Spark 进行分布式深度学习](apache-spark-deep-learning-caffe.md)\r \r ### <a name=\"for-spark-developers\"></a>适用于 Spark 开发人员\r \r * [使用 Scala 创建独立的应用程序](apache-spark-create-standalone-application.md)\r * [使用 Livy 在 Spark 群集中远程运行作业](apache-spark-livy-rest-interface.md)\r * [使用适用于 IntelliJ IDEA 的 HDInsight 工具插件创建和提交 Spark Scala 应用程序](apache-spark-intellij-tool-plugin.md)\r * [Spark 流式处理：使用 HDInsight 中的 Spark 生成实时流式处理应用程序](apache-spark-eventhub-streaming.md)\r * [Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely（使用 IntelliJ IDEA 的 HDInsight 工具插件远程调试 Spark 应用程序）](apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)\r * [在 HDInsight 上的 Spark 群集中使用 Zeppelin 笔记本](apache-spark-zeppelin-notebook.md)\r * [在 HDInsight 的 Spark 群集中可用于 Jupyter 笔记本的内核](apache-spark-jupyter-notebook-kernels.md)\r * [Use external packages with Jupyter notebooks（将外部包与 Jupyter 笔记本配合使用）](apache-spark-jupyter-notebook-use-external-packages.md)\r * [Install Jupyter on your computer and connect to an HDInsight Spark cluster（在计算机上安装 Jupyter 并连接到 HDInsight Spark 群集）](apache-spark-jupyter-notebook-install-locally.md)\r \r \r \r <!--Update_Description: update wording and link references-->"}