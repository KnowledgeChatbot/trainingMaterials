{"Title":"在 HDInsight 群集上将 Hadoop Pig 与 SSH 配合使用 - Azure","Description":"了解如何使用 SSH 连接到基于 Linux 的 Hadoop 群集，并使用 Pig 命令以交互方式或以批处理作业形式运行 Pig Latin 语句。","Content":"# <a name=\"run-pig-jobs-on-a-linux-based-cluster-with-the-pig-command-ssh\"></a>使用 Pig 命令 (SSH) 在基于 Linux 的群集上运行 Pig 作业\r \r [!INCLUDE [pig-selector](../../../includes/hdinsight-selector-use-pig.md)]\r \r 了解如何以交互方式从 HDInsight 群集的 SSH 连接中运行 Pig 作业。 可以使用 Pig Latin 编程语言来描述应用到输入数据以生成所需输出的转换。\r \r > [!IMPORTANT]\r > 本文档中的步骤要求使用基于 Linux 的 HDInsight 群集。 Linux 是在 HDInsight 3.4 版或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 在 Windows 上停用](../hdinsight-component-versioning.md#hdinsight-windows-retirement)。\r \r ## <a id=\"ssh\"></a>使用 SSH 进行连接\r \r 使用 SSH 连接到 HDInsight 群集。 以下示例连接到名为 **myhdinsight** 的群集，以作为名为 **sshuser** 的帐户：\r \r     ssh sshuser@myhdinsight-ssh.azurehdinsight.cn\r \r 如果创建 HDInsight 群集时**提供了 SSH 身份验证的证书密钥**，则可能需要指定客户端系统上的私钥位置。\r \r     ssh sshuser@myhdinsight-ssh.azurehdinsight.cn -i ~/mykey.key\r \r 如果在创建 HDInsight 群集时**提供了 SSH 身份验证密码**，请在系统提示时提供该密码。\r \r 有关将 SSH 与 HDInsight 配合使用的详细信息，请参阅[将 SSH 与 HDInsight 配合使用](../hdinsight-hadoop-linux-use-ssh-unix.md)。\r \r ## <a id=\"pig\"></a>使用 Pig 命令\r \r 1. 连接后，请使用以下命令启动 Pig 命令行接口 (CLI)：\r \r         pig\r \r     稍后，应该会看到 `grunt>` 提示符。\r \r 2. 输入以下语句：\r \r         LOGS = LOAD '/example/data/sample.log';\r \r     此命令会将 sample.log 文件的内容载入到 LOGS。 可以使用以下语句查看该文件的内容：\r \r         DUMP LOGS;\r \r 3. 接下来，应用正则表达式以使用以下语句仅提取每条记录的日志记录级别，从而转换数据：\r \r         LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\r \r     转换后，可以使用 **DUMP** 来查看数据。 在这种情况下，使用 `DUMP LEVELS;`。\r \r 4. 使用下表中的语句继续应用转换：\r \r     | Pig Latin 语句 | 语句的作用 |\r     | ---- | ---- |\r     | `FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;` | 删除包含日志级别 null 值的行，并将结果存储到 `FILTEREDLEVELS` 中。 |\r     | `GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;` | 按日志级别对行进行分组，并将结果存储到 `GROUPEDLEVELS` 中。 |\r     | `FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;` | 创建包含每个唯一日志级别值及其发生次数的数据集。 该数据集存储到 `FREQUENCIES` 中。 |\r     | `RESULT = order FREQUENCIES by COUNT desc;` | 按计数为日志级别排序（降序），并存储到 `RESULT` 中。 |\r \r     > [!TIP]\r     > 使用 `DUMP` 查看每个步骤后的转换结果。\r \r 5. 也可以使用 `STORE` 语句保存转换结果。 例如，以下语句将 `RESULT` 保存到群集的默认存储的 `/example/data/pigout` 目录：\r \r         STORE RESULT into '/example/data/pigout';\r \r    > [!NOTE]\r    > 该数据存储在名为 `part-nnnnn` 的文件的指定目录中。 如果该目录已存在，则会收到错误。\r \r 6. 若要退出 grunt 提示符，请输入以下语句：\r \r         QUIT;\r \r ### <a name=\"pig-latin-batch-files\"></a>Pig Latin 批处理文件\r \r 也可以使用 Pig 命令运行文件中包含的 Pig Latin。\r \r 1. 退出 grunt 提示符之后，请使用以下命令将 STDIN 发送到名为 `pigbatch.pig` 的文件中。 此文件创建于 SSH 用户帐户的主目录中。\r \r         cat > ~/pigbatch.pig\r \r 2. 输入或粘贴以下行，并在完成后按 Ctrl+D。\r \r         LOGS = LOAD '/example/data/sample.log';\r         LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\r         FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;\r         GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;\r         FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;\r         RESULT = order FREQUENCIES by COUNT desc;\r         DUMP RESULT;\r \r 3. 使用以下命令，以使用 Pig 命令运行 `pigbatch.pig` 文件。\r \r         pig ~/pigbatch.pig\r \r     批处理作业完成后，会看到以下输出：\r \r         (TRACE,816)\r         (DEBUG,434)\r         (INFO,96)\r         (WARN,11)\r         (ERROR,6)\r         (FATAL,2)\r \r ## <a id=\"nextsteps\"></a>后续步骤\r \r 有关 HDInsight 中 Pig 的常规信息，请参阅以下文档：\r \r * [将 Pig 与 Hadoop on HDInsight 配合使用](hdinsight-use-pig.md)\r \r 有关使用 HDInsight 上 Hadoop 的其他方式的详细信息，请参阅以下文档：\r \r * [将 Hive 与 Hadoop on HDInsight 配合使用](hdinsight-use-hive.md)\r * [将 MapReduce 与 HDInsight 上的 Hadoop 配合使用](hdinsight-use-mapreduce.md)\r \r \r <!--Update_Description: update wording and link references-->"}