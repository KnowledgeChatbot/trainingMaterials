{"Title":"在 HDInsight 中使用基于时间的 Hadoop Oozie 协调器","Description":"在 HDInsight 中使用基于时间的 Hadoop Oozie 协调器（大数据服务）。 了解如何定义 Oozie 工作流和协调器，以及如何提交作业。","Content":"# <a name=\"use-time-based-oozie-coordinator-with-hadoop-in-hdinsight-to-define-workflows-and-coordinate-jobs\"></a>将基于时间的 Oozie 协调器与 HDInsight 中的 Hadoop 配合使用以定义工作流和协调作业\r \r [!INCLUDE [azure-sdk-developer-differences](../../includes/azure-sdk-developer-differences.md)]\r \r 在本文中，学习如何定义工作流和协调器，以及如何基于时间触发协调器作业。 阅读本文前，浏览[将 Oozie 与 HDInsight 配合使用][hdinsight-use-oozie]很有帮助。 除了 Oozie，还可以使用 Azure 数据工厂来计划作业。\r \r > [!NOTE]\r > 本文需要基于 Windows 的 HDInsight 群集。 有关在基于 Linux 的群集上使用 Oozie 的信息，包括基于时间的作业，请参阅[在基于 Linux 的 HDInsight 上将 Oozie 与 Hadoop 配合使用以定义和运行工作流](hdinsight-use-oozie-linux-mac.md)\r \r ## <a name=\"what-is-oozie\"></a>什么是 Oozie\r Apache Oozie 是一个管理 Hadoop 作业的工作流/协调系统。 该系统与 Hadoop 堆栈集成，支持 Apache MapReduce、Apache Pig、Apache Hive 和 Apache Sqoop 的 Hadoop 作业。 此外，还可用于调度系统特定作业，如 Java 程序或 shell 脚本。\r \r 下图显示将要实施的工作流：\r \r ![工作流关系图][img-workflow-diagram]\r \r 工作流包含两个操作：\r \r 1. Hive 操作运行 HiveQL 脚本，以统计 log4j 日志文件中每个日志级类型的次数。 每个 log4j 日志都包含一行字段，其中包含 [LOG LEVEL] 字段，可显示类型和严重性，例如：\r \r         2012-02-03 18:35:34 SampleClass6 [INFO] everything normal for id 577725851\r         2012-02-03 18:35:34 SampleClass4 [FATAL] system problem at id 1991281254\r         2012-02-03 18:35:34 SampleClass3 [DEBUG] detail for id 1304807656\r         ...\r \r     Hive 脚本的输出结果类似如下：\r \r         [DEBUG] 434\r         [ERROR] 3\r         [FATAL] 1\r         [INFO]  96\r         [TRACE] 816\r         [WARN]  4\r \r     有关 Hive 的详细信息，请参阅 [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]。\r 2. Sqoop 操作将 HiveQL 操作输出结果导出到 Azure SQL 数据库中的表。 有关 Sqoop 的详细信息，请参阅 [将 Sqoop 与 HDInsight 配合使用][hdinsight-use-sqoop]。\r \r > [!NOTE]\r > 有关 HDInsight 群集上支持的 Oozie 版本，请参阅 [HDInsight 提供的群集版本有哪些新功能？][hdinsight-versions]。\r >\r >\r \r ## <a name=\"prerequisites\"></a>先决条件\r 在开始阅读本教程前，必须具有：\r \r * **配备 Azure PowerShell 的工作站**。\r \r     > [!IMPORTANT]\r     > Azure PowerShell 对于使用 Azure Service Manager 管理 HDInsight 资源的支持已**弃用**，会于 2017 年 1 月 1 日删除。 本文档中的步骤使用的是与 Azure Resource Manager 兼容的新 HDInsight cmdlet。\r     >\r     > 请按照 [Install and configure Azure PowerShell](https://docs.microsoft.com/powershell/azureps-cmdlets-docs) （安装和配置 Azure PowerShell）中的步骤安装最新版本的 Azure PowerShell。 如果脚本需要修改才能使用与 Azure Resource Manager 兼容的新 cmdlet，请参阅[迁移到适用于 HDInsight 群集的基于 Azure Resource Manager 的开发工具](hdinsight-hadoop-development-using-azure-resource-manager.md)，了解详细信息。\r \r * **HDInsight 群集**。 有关创建 HDInsight 群集的信息，请参阅[创建 HDInsight 群集][hdinsight-provision]或 [HDInsight 入门][hdinsight-get-started]。 完成本教程需要以下数据：\r \r     <table border = \"1\">\r     <tr><th>群集属性</th><th>Windows PowerShell 变量名</th><th>值</th><th>说明</th></tr>\r     <tr><td>HDInsight 群集名称</td><td>$clusterName</td><td></td><td>要在其中运行本教程的 HDInsight 群集。</td></tr>\r     <tr><td>HDInsight 群集用户名</td><td>$clusterUsername</td><td></td><td>HDInsight 群集用户名。 </td></tr>\r     <tr><td>HDInsight 群集用户密码 </td><td>$clusterPassword</td><td></td><td>HDInsight 群集用户的密码。</td></tr>\r     <tr><td>Azure 存储帐户名称</td><td>$storageAccountName</td><td></td><td>可用于 HDInsight 群集的 Azure 存储帐户。 在本教程中，使用在群集设置过程中指定的默认存储帐户。</td></tr>\r     <tr><td>Azure Blob 容器名称</td><td>$containerName</td><td></td><td>在此示例中，使用用于默认 HDInsight 群集文件系统的 Azure Blob 存储容器。 默认情况下，该容器与 HDInsight 群集同名。</td></tr>\r     </table>Azure SQL 数据库\r * ****。 必须为 SQL 数据库服务器配置防火墙规则，以允许从工作站访问。 有关创建 Azure SQL 数据库和配置防火墙的说明，请参阅使用 [Azure SQL 数据库入门][sqldatabase-get-started]。 本文提供用于创建本教程所需 Azure SQL 数据库表的 Windows PowerShell 脚本。\r \r     <table border = \"1\">\r     <tr><th>SQL 数据库属性</th><th>Windows PowerShell 变量名</th><th>值</th><th>说明</th></tr>\r     <tr><td>SQL 数据库服务器名称</td><td>$sqlDatabaseServer</td><td></td><td>作为 Sqoop 数据导出目标的 SQL 数据库服务器。 </td></tr>\r     <tr><td>SQL 数据库登录名</td><td>$sqlDatabaseLogin</td><td></td><td>SQL 数据库的登录名。</td></tr>\r     <tr><td>SQL 数据库登录密码</td><td>$sqlDatabaseLoginPassword</td><td></td><td>SQL 数据库的登录密码。</td></tr>\r     <tr><td>SQL 数据库名</td><td>$sqlDatabaseName</td><td></td><td>作为 Sqoop 数据导出目标的 Azure SQL 数据库。 </td></tr>\r     </table>\r \r   > [!NOTE]\r   > 默认情况下，可从 Azure 服务（如 Azure HDInsight）连接 Azure SQL 数据库。 如果禁用了此防火墙设置，则必须从 Azure 门户启用它。 有关创建 SQL 数据库和配置防火墙规则的说明，请参阅 [创建和配置 SQL 数据库][sqldatabase-get-started]。\r \r > [!NOTE]\r > 填写表中的值。 这会有助于学习本教程。\r \r ## <a name=\"define-oozie-workflow-and-the-related-hiveql-script\"></a>定义 Oozie 工作流及相关 HiveQL 脚本\r Oozie 工作流定义是用 hPDL（一种 XML 过程定义语言）编写的。 默认的工作流文件名为 *workflow.xml*。  将在本地保存该工作流文件，并会在本教程后面使用 Azure PowerShell 将它部署到 HDInsight 群集。\r \r 工作流中的 Hive 操作调用 HiveQL 脚本文件。 此脚本文件包含三个 HiveQL 语句：\r \r 1. **DROP TABLE 语句** 删除 log4j Hive 表（如果存在）。\r 2. **CREATE TABLE 语句** 创建 log4j Hive 外部表，指向 log4j 日志文件位置；\r 3. **log4j 日志文件位置**。 字段分隔符为“,”。 默认分行符为“\\n”。 如果要多次运行 Oozie 工作流，可使用 Hive 外部表避免从原始位置删除数据文件。\r 4. **INSERT OVERWRITE 语句**从 log4j Hive 表统计每个日志级类型的次数，并将输出结果保存到 Azure Blob 存储位置。\r \r > [!NOTE]\r > 有一个已知的 Hive 路径问题。 在提交 Oozie 作业时会遇到这个问题。 可在 TechNet Wiki 上找到解决此问题的说明： [HDInsight Hive 错误:无法重命名][technetwiki-hive-error]。\r \r **定义由工作流调用的 HiveQL 脚本文件**\r \r 1. 创建包含以下内容的文本文件：\r \r         DROP TABLE ${hiveTableName};\r         CREATE EXTERNAL TABLE ${hiveTableName}(t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE LOCATION '${hiveDataFolder}';\r         INSERT OVERWRITE DIRECTORY '${hiveOutputFolder}' SELECT t4 AS sev, COUNT(*) AS cnt FROM ${hiveTableName} WHERE t4 LIKE '[%' GROUP BY t4;\r \r     在脚本中使用三个变量：\r \r    * ${hiveTableName}\r    * ${hiveDataFolder}\r    * ${hiveOutputFolder}\r \r      工作流定义文件（本教程中的 workflow.xml）在运行时会将三个值传递到这个 HiveQL 脚本。\r 2. 使用 ANSI(ASCII) 编码将文件另存为 C:\\Tutorials\\UseOozie\\useooziewf.hql。 （如果文本编辑器不提供此选项，则使用记事本。）在本教程的后面，此脚本文件会被部署到 HDInsight 群集。\r \r **定义工作流**\r \r 1. 创建包含以下内容的文本文件：\r \r     ```xml\r     <workflow-app name=\"useooziewf\" xmlns=\"uri:oozie:workflow:0.2\">\r         <start to = \"RunHiveScript\"/>\r \r         <action name=\"RunHiveScript\">\r             <hive xmlns=\"uri:oozie:hive-action:0.2\">\r                 <job-tracker>${jobTracker}</job-tracker>\r                 <name-node>${nameNode}</name-node>\r                 <configuration>\r                     <property>\r                         <name>mapred.job.queue.name</name>\r                         <value>${queueName}</value>\r                     </property>\r                 </configuration>\r                 <script>${hiveScript}</script>\r                 <param>hiveTableName=${hiveTableName}</param>\r                 <param>hiveDataFolder=${hiveDataFolder}</param>\r                 <param>hiveOutputFolder=${hiveOutputFolder}</param>\r             </hive>\r             <ok to=\"RunSqoopExport\"/>\r             <error to=\"fail\"/>\r         </action>\r \r         <action name=\"RunSqoopExport\">\r             <sqoop xmlns=\"uri:oozie:sqoop-action:0.2\">\r                 <job-tracker>${jobTracker}</job-tracker>\r                 <name-node>${nameNode}</name-node>\r                 <configuration>\r                     <property>\r                         <name>mapred.compress.map.output</name>\r                         <value>true</value>\r                     </property>\r                 </configuration>\r             <arg>export</arg>\r             <arg>--connect</arg>\r             <arg>${sqlDatabaseConnectionString}</arg>\r             <arg>--table</arg>\r             <arg>${sqlDatabaseTableName}</arg>\r             <arg>--export-dir</arg>\r             <arg>${hiveOutputFolder}</arg>\r             <arg>-m</arg>\r             <arg>1</arg>\r             <arg>--input-fields-terminated-by</arg>\r             <arg>\"\\001\"</arg>\r             </sqoop>\r             <ok to=\"end\"/>\r             <error to=\"fail\"/>\r         </action>\r \r         <kill name=\"fail\">\r             <message>Job failed, error message[${wf:errorMessage(wf:lastErrorNode())}] </message>\r         </kill>\r \r         <end name=\"end\"/>\r     </workflow-app>\r     ```\r \r     在工作流中定义两个操作。 start-to 操作是 *RunHiveScript*。 如果该操作运行正常，则下一个操作是 RunSqoopExport。\r \r     RunHiveScript 包含几个变量。 在从工作站使用 Azure PowerShell 提交 Oozie 作业时，会传递值。\r \r     工作流变量\r \r     <table border = \"1\">\r     <tr><th>工作流变量</th><th>说明</th></tr>\r     <tr><td>${jobTracker}</td><td>指定 Hadoop 作业跟踪器的 URL。 在 HDInsight 群集版本 3.0 和 2.0 上使用 jobtrackerhost:9010<strong></strong>。</td></tr>\r     <tr><td>${nameNode}</td><td>指定 Hadoop 名称节点的 URL。 使用默认的文件系统 wasb:// 地址，例如 <i>wasb://&lt;containerName&gt;@&lt;storageAccountName&gt;.blob.core.chinacloudapi.cn</i>。</td></tr>\r     <tr><td>${queueName}</td><td>指定要将作业提交到的队列名称。 使用“默认”<strong></strong>。</td></tr>\r     </table>\r \r     Hive 操作变量\r \r     <table border = \"1\">\r     <tr><th>Hive 操作变量</th><th>说明</th></tr>\r     <tr><td>${hiveDataFolder}</td><td>Hive Create Table 命令的源目录。</td></tr>\r     <tr><td>${hiveOutputFolder}</td><td>INSERT OVERWRITE 语句的输出文件夹。</td></tr>\r     <tr><td>${hiveTableName}</td><td>引用 log4j 数据文件的 Hive 表的名称。</td></tr>\r     </table>\r \r     Sqoop 操作变量\r \r     <table border = \"1\">\r     <tr><th>Sqoop 操作变量</th><th>说明</th></tr>\r     <tr><td>${sqlDatabaseConnectionString}</td><td>SQL 数据库连接字符串。</td></tr>\r     <tr><td>${sqlDatabaseTableName}</td><td>数据导出的目标 Azure SQL 数据库表。</td></tr>\r     <tr><td>${hiveOutputFolder}</td><td>Hive INSERT OVERWRITE 语句的输出文件夹。 这是用于 Sqoop 导出 (export-dir) 的同一个文件夹。</td></tr>\r     </table>\r \r     有关 Oozie 工作流和使用工作流操作的详细信息，请参阅 [Apache Oozie 4.0 文档][apache-oozie-400]（对于 HDInsight 群集 3.0 版）或 [Apache Oozie 3.3.2 文档][apache-oozie-332]（对于 HDInsight 群集 2.1 版）。\r \r 1. 使用 ANSI (ASCII) 编码将文件另存为 C:\\Tutorials\\UseOozie\\workflow.xml。 （如果文本编辑器不提供此选项，则使用记事本。）\r \r **定义协调器**\r \r 1. 创建包含以下内容的文本文件：\r \r     ```xml\r     <coordinator-app name=\"my_coord_app\" frequency=\"${coordFrequency}\" start=\"${coordStart}\" end=\"${coordEnd}\" timezone=\"${coordTimezone}\" xmlns=\"uri:oozie:coordinator:0.4\">\r         <action>\r             <workflow>\r                 <app-path>${wfPath}</app-path>\r             </workflow>\r         </action>\r     </coordinator-app>\r     ```\r \r     定义文件中使用五个变量：\r \r    | 变量 | 说明 |\r    | --- | --- |\r    | ${coordFrequency} |作业暂停时间。 频率始终用分钟表示。 |\r    | ${coordStart} |作业开始时间。 |\r    | ${coordEnd} |作业结束时间。 |\r    | ${coordTimezone} |Oozie 在没有夏时制的固定时区（通常用 UTC 表示）处理协调器作业。 此时区被称为“Oozie 处理时区”。 |\r    | ${wfPath} |workflow.xml 的路径。  如果该工作流文件名不是默认文件名 (workflow.xml)，则必须指定该名称。 |\r 2. 使用 ANSI (ASCII) 编码将文件另存为 C:\\Tutorials\\UseOozie\\coordinator.xml。 （如果文本编辑器不提供此选项，则使用记事本。）\r \r ## <a name=\"deploy-the-oozie-project-and-prepare-the-tutorial\"></a>部署 Oozie 项目并准备教程\r 将运行 Azure PowerShell 脚本来执行以下操作：\r \r * 将 HiveQL 脚本 (useoozie.hql) 复制到 Azure Blob 存储 (wasb:///tutorials/useoozie/useoozie.hql)。\r * 将 workflow.xml 复制到 wasb:///tutorials/useoozie/workflow.xml。\r * 将 coordinator.xml 复制到 wasb:///tutorials/useoozie/coordinator.xml。\r * 将数据文件 (/example/data/sample.log) 复制到 wasb:///tutorials/useoozie/data/sample.log。\r * 创建用于存储 Sqoop 导出数据的 Azure SQL 数据库表。 表名为 *log4jLogCount*。\r \r **了解 HDInsight 存储**\r \r HDInsight 将 Azure Blob 存储用于数据存储。 在 Azure Blob 存储中，wasb:// 是 Microsoft 的 Hadoop 分布式文件系统 (HDFS) 的实现。 有关详细信息，请参阅将 [Azure Blob 存储与 HDInsight 配合使用][hdinsight-storage]。\r \r 设置 HDInsight 群集时，请将 Azure Blob 存储帐户和该帐户上的特定容器指定为默认文件系统，就像在 HDFS 中一样。 除此存储帐户以外，在预配过程中还可以从相同或不同 Azure 订阅添加其他存储帐户。 有关添加其他存储帐户的说明，请参阅 [设置 HDInsight 群集][hdinsight-provision]。 为了简化本教程中使用的 Azure PowerShell 脚本，所有文件都存储在默认文件系统容器（位于 */tutorials/useoozie*）中。 默认情况下，此容器与 HDInsight 群集同名。\r 语法为：\r \r     wasb[s]://<ContainerName>@<StorageAccountName>.blob.core.chinacloudapi.cn/<path>/<filename>\r \r > [!NOTE]\r > HDInsight 群集 3.0 版只支持 wasb:// 语法。 较早的 asv:// 语法在 HDInsight 2.1 和 1.6 群集中受支持，但在 HDInsight 3.0 群集中不受支持。\r >\r > wasb:// 路径是虚拟路径。 有关详细信息，请参阅将 [Azure Blob 存储与 HDInsight 配合使用][hdinsight-storage]。\r \r 存储在默认文件系统容器中的文件可以使用以下任一 URI 从 HDInsight 进行访问（以 workflow.xml 为例）：\r \r     wasb://mycontainer@mystorageaccount.blob.core.chinacloudapi.cn/tutorials/useoozie/workflow.xml\r     wasb:///tutorials/useoozie/workflow.xml\r     /tutorials/useoozie/workflow.xml\r \r 如果要从存储帐户直接访问文件，则文件的 Blob 名称为：\r \r     tutorials/useoozie/workflow.xml\r \r **了解 Hive 内部表和外部表**\r \r 以下是需要了解的有关 Hive 内部表和外部表的一些信息：\r \r * CREATE TABLE 命令创建内部表，也称为托管表。 数据文件必须位于默认容器中。\r * CREATE TABLE 命令将数据文件移动到默认容器中的 /hive/warehouse/<TableName> 文件夹。\r * CREATE EXTERNAL TABLE 命令创建外部表。 数据文件可以位于默认容器以外的位置。\r * CREATE EXTERNAL TABLE 命令不移动数据文件。\r * CREATE EXTERNAL TABLE 命令不允许 LOCATION 子句中指定的文件夹下有任何子文件夹。 这是本教程生成 sample.log 文件副本的原因。\r \r 有关详细信息，请参阅 [HDInsight：Hive 内部和外部表简介][cindygross-hive-tables]。\r \r **准备教程**\r \r 1. 打开 Windows PowerShell ISE（在 Windows 8“开始”屏幕上，键入 PowerShell_ISE，然后单击“Windows PowerShell ISE”。 有关详细信息，请参阅[在 Windows 8 和 Windows 上启动 Windows PowerShell][powershell-start]）。\r 2. 在底部窗格中，运行以下命令连接到 Azure 订阅：\r \r     ```powershell\r     Add-AzureAccount -Environment AzureChinaCloud\r     ```\r \r     系统会提示输入 Azure 帐户凭据。 添加订阅连接的此方法会超时，12 小时后必须重新运行 cmdlet。\r \r    > [!NOTE]\r    > 如果有多个 Azure 订阅，而默认订阅不是想使用的，则请使用 Select-AzureSubscription cmdlet 来选择订阅<strong></strong>。\r \r 3. 将以下脚本复制到脚本窗格，并设置前六个变量：\r \r     ```powershell\r     # WASB variables\r     $storageAccountName = \"<StorageAccountName>\"\r     $containerName = \"<BlobStorageContainerName>\"\r \r     # SQL database variables\r     $sqlDatabaseServer = \"<SQLDatabaseServerName>\"\r     $sqlDatabaseLogin = \"<SQLDatabaseLoginName>\"\r     $sqlDatabaseLoginPassword = \"SQLDatabaseLoginPassword>\"\r     $sqlDatabaseName = \"<SQLDatabaseName>\"\r     $sqlDatabaseTableName = \"log4jLogsCount\"\r \r     # Oozie files for the tutorial\r     $hiveQLScript = \"C:\\Tutorials\\UseOozie\\useooziewf.hql\"\r     $workflowDefinition = \"C:\\Tutorials\\UseOozie\\workflow.xml\"\r     $coordDefinition =  \"C:\\Tutorials\\UseOozie\\coordinator.xml\"\r \r     # WASB folder for storing the Oozie tutorial files.\r     $destFolder = \"tutorials/useoozie\"  # Do NOT use the long path here\r     ```\r \r     有关这些变量的详细说明，请参阅本教程中的 [先决条件](#prerequisites) 一节。\r \r 4. 在脚本窗格中将以下内容追加到脚本：\r \r     ```powershell\r     # Create a storage context object\r     $storageaccountkey = get-azurestoragekey $storageAccountName | %{$_.Primary}\r     $destContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageaccountkey\r \r     function uploadOozieFiles()\r     {\r         Write-Host \"Copy HiveQL script, workflow definition and coordinator definition ...\" -ForegroundColor Green\r         Set-AzureStorageBlobContent -File $hiveQLScript -Container $containerName -Blob \"$destFolder/useooziewf.hql\" -Context $destContext\r         Set-AzureStorageBlobContent -File $workflowDefinition -Container $containerName -Blob \"$destFolder/workflow.xml\" -Context $destContext\r         Set-AzureStorageBlobContent -File $coordDefinition -Container $containerName -Blob \"$destFolder/coordinator.xml\" -Context $destContext\r     }\r \r     function prepareHiveDataFile()\r     {\r         Write-Host \"Make a copy of the sample.log file ... \" -ForegroundColor Green\r         Start-CopyAzureStorageBlob -SrcContainer $containerName -SrcBlob \"example/data/sample.log\" -Context $destContext -DestContainer $containerName -destBlob \"$destFolder/data/sample.log\" -DestContext $destContext\r     }\r \r     function prepareSQLDatabase()\r     {\r         # SQL query string for creating log4jLogsCount table\r         $cmdCreateLog4jCountTable = \" CREATE TABLE [dbo].[$sqlDatabaseTableName](\r                 [Level] [nvarchar](10) NOT NULL,\r                 [Total] float,\r             CONSTRAINT [PK_$sqlDatabaseTableName] PRIMARY KEY CLUSTERED\r             (\r             [Level] ASC\r             )\r             )\"\r \r         #Create the log4jLogsCount table\r         Write-Host \"Create Log4jLogsCount table ...\" -ForegroundColor Green\r         $conn = New-Object System.Data.SqlClient.SqlConnection\r         $conn.ConnectionString = \"Data Source=$sqlDatabaseServer.database.chinacloudapi.cn;Initial Catalog=$sqlDatabaseName;User ID=$sqlDatabaseLogin;Password=$sqlDatabaseLoginPassword;Encrypt=true;Trusted_Connection=false;\"\r         $conn.open()\r         $cmd = New-Object System.Data.SqlClient.SqlCommand\r         $cmd.connection = $conn\r         $cmd.commandtext = $cmdCreateLog4jCountTable\r         $cmd.executenonquery()\r \r         $conn.close()\r     }\r \r     # upload workflow.xml, coordinator.xml, and ooziewf.hql\r     uploadOozieFiles;\r \r     # make a copy of example/data/sample.log to example/data/log4j/sample.log\r     prepareHiveDataFile;\r \r     # create log4jlogsCount table on SQL database\r     prepareSQLDatabase;\r     ```\r \r 5. 单击“运行脚本”或按 F5 运行该脚本。 输出结果会类似于：\r \r     ![教程准备的输出结果][img-preparation-output]\r \r ## <a name=\"run-the-oozie-project\"></a>运行 Oozie 项目\r 目前，Azure PowerShell 不提供用于定义 Oozie 作业的任何 cmdlet。 可以使用 **Invoke-RestMethod** cmdlet 调用 Oozie Web 服务。 Oozie Web 服务 API 是 HTTP REST JSON API。 有关 Oozie Web 服务 API 的详细信息，请参阅 [Apache Oozie 4.0 文档][apache-oozie-400]（对于 HDInsight 群集 3.0 版）或 [Apache Oozie 3.3.2 文档][apache-oozie-332]（对于 HDInsight 群集 2.1 版）。\r \r **提交 Oozie 作业**\r \r 1. 打开 Windows PowerShell ISE（在 Windows 8“开始”屏幕上，键入 PowerShell_ISE，然后单击“Windows PowerShell ISE”。 有关详细信息，请参阅[在 Windows 8 和 Windows 上启动 Windows PowerShell][powershell-start]）。\r 2. 将以下脚本复制到脚本窗格，然后设置前 14 个变量（不过，请跳过 $storageUri）。\r \r     ```powershell\r     #HDInsight cluster variables\r     $clusterName = \"<HDInsightClusterName>\"\r     $clusterUsername = \"<HDInsightClusterUsername>\"\r     $clusterPassword = \"<HDInsightClusterUserPassword>\"\r \r     #Azure Blob storage (WASB) variables\r     $storageAccountName = \"<StorageAccountName>\"\r     $storageContainerName = \"<BlobContainerName>\"\r     $storageUri=\"wasb://$storageContainerName@$storageAccountName.blob.core.chinacloudapi.cn\"\r \r     #Azure SQL database variables\r     $sqlDatabaseServer = \"<SQLDatabaseServerName>\"\r     $sqlDatabaseLogin = \"<SQLDatabaseLoginName>\"\r     $sqlDatabaseLoginPassword = \"<SQLDatabaseloginPassword>\"\r     $sqlDatabaseName = \"<SQLDatabaseName>\"\r \r     #Oozie WF/coordinator variables\r     $coordStart = \"2014-03-21T13:45Z\"\r     $coordEnd = \"2014-03-21T13:45Z\"\r     $coordFrequency = \"1440\"    # in minutes, 24h x 60m = 1440m\r     $coordTimezone = \"UTC\"    #UTC/GMT\r \r     $oozieWFPath=\"$storageUri/tutorials/useoozie\"  # The default name is workflow.xml. And you don't need to specify the file name.\r     $waitTimeBetweenOozieJobStatusCheck=10\r \r     #Hive action variables\r     $hiveScript = \"$storageUri/tutorials/useoozie/useooziewf.hql\"\r     $hiveTableName = \"log4jlogs\"\r     $hiveDataFolder = \"$storageUri/tutorials/useoozie/data\"\r     $hiveOutputFolder = \"$storageUri/tutorials/useoozie/output\"\r \r     #Sqoop action variables\r     $sqlDatabaseConnectionString = \"Data Source=$sqlDatabaseServer.database.chinacloudapi.cn;user=$sqlDatabaseLogin@$sqlDatabaseServer;password=$sqlDatabaseLoginPassword;database=$sqlDatabaseName\"\r     $sqlDatabaseTableName = \"log4jLogsCount\"\r \r     $passwd = ConvertTo-SecureString $clusterPassword -AsPlainText -Force\r     $creds = New-Object System.Management.Automation.PSCredential ($clusterUsername, $passwd)\r     ```\r \r     有关这些变量的详细说明，请参阅本教程中的 [先决条件](#prerequisites) 一节。\r \r     $coordstart 和 $coordend 是工作流的开始和结束时间。 要了解 UTC/GMT 时间，请在 bing.com 上搜索“utc 时间”。 $coordFrequency 是要运行工作流的频率（以分钟为单位）。\r 3. 将以下内容追加到脚本。 此部分定义 Oozie 有效负载：\r \r     ```powershell\r     #OoziePayload used for Oozie web service submission\r     $OoziePayload =  @\"\r     <?xml version=\"1.0\" encoding=\"UTF-8\"?>\r     <configuration>\r \r         <property>\r             <name>nameNode</name>\r             <value>$storageUrI</value>\r         </property>\r \r         <property>\r             <name>jobTracker</name>\r             <value>jobtrackerhost:9010</value>\r         </property>\r \r         <property>\r             <name>queueName</name>\r             <value>default</value>\r         </property>\r \r         <property>\r             <name>oozie.use.system.libpath</name>\r             <value>true</value>\r         </property>\r \r         <property>\r             <name>oozie.coord.application.path</name>\r             <value>$oozieWFPath</value>\r         </property>\r \r         <property>\r             <name>wfPath</name>\r             <value>$oozieWFPath</value>\r         </property>\r \r         <property>\r             <name>coordStart</name>\r             <value>$coordStart</value>\r         </property>\r \r         <property>\r             <name>coordEnd</name>\r             <value>$coordEnd</value>\r         </property>\r \r         <property>\r             <name>coordFrequency</name>\r             <value>$coordFrequency</value>\r         </property>\r \r         <property>\r             <name>coordTimezone</name>\r             <value>$coordTimezone</value>\r         </property>\r \r         <property>\r             <name>hiveScript</name>\r             <value>$hiveScript</value>\r         </property>\r \r         <property>\r             <name>hiveTableName</name>\r             <value>$hiveTableName</value>\r         </property>\r \r         <property>\r             <name>hiveDataFolder</name>\r             <value>$hiveDataFolder</value>\r         </property>\r \r         <property>\r             <name>hiveOutputFolder</name>\r             <value>$hiveOutputFolder</value>\r         </property>\r \r         <property>\r             <name>sqlDatabaseConnectionString</name>\r             <value>&quot;$sqlDatabaseConnectionString&quot;</value>\r         </property>\r \r         <property>\r             <name>sqlDatabaseTableName</name>\r             <value>$SQLDatabaseTableName</value>\r         </property>\r \r         <property>\r             <name>user.name</name>\r             <value>admin</value>\r         </property>\r \r     </configuration>\r     \"@\r     ```\r \r    > [!NOTE]\r    > 与工作流提交有效负载文件相比，主要区别是变量 **oozie.coord.application.path**。 提交工作流作业时，使用 **oozie.wf.application.path** 。\r \r 4. 将以下内容追加到脚本。 此部分检查 Oozie Web 服务状态：\r \r     ```powershell\r     function checkOozieServerStatus()\r     {\r         Write-Host \"Checking Oozie server status...\" -ForegroundColor Green\r         $clusterUriStatus = \"https://$clusterName.azurehdinsight.cn:443/oozie/v2/admin/status\"\r         $response = Invoke-RestMethod -Method Get -Uri $clusterUriStatus -Credential $creds -OutVariable $OozieServerStatus\r \r         $jsonResponse = ConvertFrom-Json (ConvertTo-Json -InputObject $response)\r         $oozieServerSatus = $jsonResponse[0].(\"systemMode\")\r         Write-Host \"Oozie server status is $oozieServerSatus...\"\r \r         if($oozieServerSatus -notmatch \"NORMAL\")\r         {\r             Write-Host \"Oozie server status is $oozieServerSatus...cannot submit Oozie jobs. Check the server status and re-run the job.\"\r         }\r     }\r     ```\r \r 5. 将以下内容追加到脚本。 此部分创建 Oozie 作业：\r \r     ```powershell\r     function createOozieJob()\r     {\r         # create Oozie job\r         Write-Host \"Sending the following Payload to the cluster:\" -ForegroundColor Green\r         Write-Host \"`n--------`n$OoziePayload`n--------\"\r         $clusterUriCreateJob = \"https://$clusterName.azurehdinsight.cn:443/oozie/v2/jobs\"\r         $response = Invoke-RestMethod -Method Post -Uri $clusterUriCreateJob -Credential $creds -Body $OoziePayload -ContentType \"application/xml\" -OutVariable $OozieJobName -debug -Verbose\r \r         $jsonResponse = ConvertFrom-Json (ConvertTo-Json -InputObject $response)\r         $oozieJobId = $jsonResponse[0].(\"id\")\r         Write-Host \"Oozie job id is $oozieJobId...\"\r \r         return $oozieJobId\r     }\r     ```\r \r    > [!NOTE]\r    > 提交工作流作业时，必须在创建作业后进行另一次 Web 服务调用以启动作业。 在这种情况下，协调器作业会按时间触发。 该作业会自动启动。\r \r 6. 将以下内容追加到脚本。 此部分检查 Oozie 作业状态：\r \r     ```powershell\r     function checkOozieJobStatus($oozieJobId)\r     {\r         # get job status\r         Write-Host \"Sleeping for $waitTimeBetweenOozieJobStatusCheck seconds until the job metadata is populated in the Oozie metastore...\" -ForegroundColor Green\r         Start-Sleep -Seconds $waitTimeBetweenOozieJobStatusCheck\r \r         Write-Host \"Getting job status and waiting for the job to complete...\" -ForegroundColor Green\r         $clusterUriGetJobStatus = \"https://$clusterName.azurehdinsight.cn:443/oozie/v2/job/\" + $oozieJobId + \"?show=info\"\r         $response = Invoke-RestMethod -Method Get -Uri $clusterUriGetJobStatus -Credential $creds\r         $jsonResponse = ConvertFrom-Json (ConvertTo-Json -InputObject $response)\r         $JobStatus = $jsonResponse[0].(\"status\")\r \r         while($JobStatus -notmatch \"SUCCEEDED|KILLED\")\r         {\r             Write-Host \"$(Get-Date -format 'G'): $oozieJobId is in $JobStatus state...waiting $waitTimeBetweenOozieJobStatusCheck seconds for the job to complete...\"\r             Start-Sleep -Seconds $waitTimeBetweenOozieJobStatusCheck\r             $response = Invoke-RestMethod -Method Get -Uri $clusterUriGetJobStatus -Credential $creds\r             $jsonResponse = ConvertFrom-Json (ConvertTo-Json -InputObject $response)\r             $JobStatus = $jsonResponse[0].(\"status\")\r         }\r \r         Write-Host \"$(Get-Date -format 'G'): $oozieJobId is in $JobStatus state!\"\r         if($JobStatus -notmatch \"SUCCEEDED\")\r         {\r             Write-Host \"Check logs at http://headnode0:9014/cluster for detais.\"\r         }\r     }\r     ```\r \r 7. （可选）将以下内容追加到脚本。\r \r     ```powershell\r     function listOozieJobs()\r     {\r         Write-Host \"Listing Oozie jobs...\" -ForegroundColor Green\r         $clusterUriStatus = \"https://$clusterName.azurehdinsight.cn:443/oozie/v2/jobs\"\r         $response = Invoke-RestMethod -Method Get -Uri $clusterUriStatus -Credential $creds\r \r         write-host \"Job ID                                   App Name        Status      Started                         Ended\"\r         write-host \"----------------------------------------------------------------------------------------------------------------------------------\"\r         foreach($job in $response.workflows)\r         {\r             Write-Host $job.id \"`t\" $job.appName \"`t\" $job.status \"`t\" $job.startTime \"`t\" $job.endTime\r         }\r     }\r \r     function ShowOozieJobLog($oozieJobId)\r     {\r         Write-Host \"Showing Oozie job info...\" -ForegroundColor Green\r         $clusterUriStatus = \"https://$clusterName.azurehdinsight.cn:443/oozie/v2/job/$oozieJobId\" + \"?show=log\"\r         $response = Invoke-RestMethod -Method Get -Uri $clusterUriStatus -Credential $creds\r         write-host $response\r     }\r \r     function killOozieJob($oozieJobId)\r     {\r         Write-Host \"Killing the Oozie job $oozieJobId...\" -ForegroundColor Green\r         $clusterUriStartJob = \"https://$clusterName.azurehdinsight.cn:443/oozie/v2/job/\" + $oozieJobId + \"?action=kill\" #Valid values for the 'action' parameter are 'start', 'suspend', 'resume', 'kill', 'dryrun', 'rerun', and 'change'.\r         $response = Invoke-RestMethod -Method Put -Uri $clusterUriStartJob -Credential $creds | Format-Table -HideTableHeaders -debug\r     }\r     ```\r \r 8. 将以下内容追加到脚本：\r \r     ```powershell\r     checkOozieServerStatus\r     # listOozieJobs\r     $oozieJobId = createOozieJob($oozieJobId)\r     checkOozieJobStatus($oozieJobId)\r     # ShowOozieJobLog($oozieJobId)\r     # killOozieJob($oozieJobId)\r     ```\r \r     如果要运行这些附加的功能，请删除这些 # 号。\r 9. 如果 HDinsight 群集版本为 2.1，请将“https://$clusterName.azurehdinsight.cn:443/oozie/v2/”替换为“https://$clusterName.azurehdinsight.cn:443/oozie/v1/”。 HDInsight 群集 2.1 版不支持 2 版的 Web 服务。\r 10. 单击“运行脚本”或按 F5 运行该脚本。 输出结果会类似于：\r \r      ![教程运行工作流输出][img-runworkflow-output]\r 11. 连接到 SQL 数据库以查看导出的数据。\r \r **检查作业错误日志**\r \r 若要解决工作流的疑难问题，可从群集头节点中的 C:\\apps\\dist\\oozie-3.3.2.1.3.2.0-05\\oozie-win-distro\\logs\\Oozie.log 位置找到 Oozie 日志文件。 有关 RDP 的信息，请参阅[使用 Azure 门户管理 HDInsight 群集][hdinsight-admin-portal]。\r \r **重新运行教程**\r \r 要重新运行工作流，必须执行以下任务：\r \r * 删除 Hive 脚本输出文件。\r * 删除 log4jLogsCount 表中的数据。\r \r 以下是可以使用的示例 Windows PowerShell 脚本：\r \r ```powershell\r $storageAccountName = \"<AzureStorageAccountName>\"\r $containerName = \"<ContainerName>\"\r \r #SQL database variables\r $sqlDatabaseServer = \"<SQLDatabaseServerName>\"\r $sqlDatabaseLogin = \"<SQLDatabaseLoginName>\"\r $sqlDatabaseLoginPassword = \"<SQLDatabaseLoginPassword>\"\r $sqlDatabaseName = \"<SQLDatabaseName>\"\r $sqlDatabaseTableName = \"log4jLogsCount\"\r \r Write-host \"Delete the Hive script output file ...\" -ForegroundColor Green\r $storageaccountkey = get-azurestoragekey $storageAccountName | %{$_.Primary}\r $destContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageaccountkey\r Remove-AzureStorageBlob -Context $destContext -Blob \"tutorials/useoozie/output/000000_0\" -Container $containerName\r \r Write-host \"Delete all the records from the log4jLogsCount table ...\" -ForegroundColor Green\r $conn = New-Object System.Data.SqlClient.SqlConnection\r $conn.ConnectionString = \"Data Source=$sqlDatabaseServer.database.chinacloudapi.cn;Initial Catalog=$sqlDatabaseName;User ID=$sqlDatabaseLogin;Password=$sqlDatabaseLoginPassword;Encrypt=true;Trusted_Connection=false;\"\r $conn.open()\r $cmd = New-Object System.Data.SqlClient.SqlCommand\r $cmd.connection = $conn\r $cmd.commandtext = \"delete from $sqlDatabaseTableName\"\r $cmd.executenonquery()\r \r $conn.close()\r ```\r \r ## <a name=\"next-steps\"></a>后续步骤\r 在本教程中，已经学习如何定义 Oozie 工作流，Oozie 协调器，以及如何使用 Azure PowerShell 运行 Oozie 协调器作业。 要了解更多信息，请参阅下列文章：\r \r * [开始使用 HDInsight][hdinsight-get-started]\r * [将 Azure Blob 存储与 HDInsight 配合使用][hdinsight-storage]\r * [使用 Azure PowerShell 管理 HDInsight][hdinsight-admin-powershell]\r * [将数据上传到 HDInsight][hdinsight-upload-data]\r * [将 Sqoop 与 HDInsight 配合使用][hdinsight-use-sqoop]\r * [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]\r * [将 Pig 与 HDInsight 配合使用][hdinsight-use-pig]\r * [为 HDInsight 开发 Java MapReduce 程序][hdinsight-develop-java-mapreduce]\r \r [hdinsight-cmdlets-download]: http://go.microsoft.com/fwlink/?LinkID=325563\r \r [hdinsight-versions]:  hdinsight-component-versioning.md\r [hdinsight-storage]: hdinsight-hadoop-use-blob-storage.md\r [hdinsight-get-started]:hadoop/apache-hadoop-linux-tutorial-get-started.md\r [hdinsight-admin-portal]: hdinsight-administer-use-management-portal.md\r \r [hdinsight-use-sqoop]:hadoop/hdinsight-use-sqoop.md\r [hdinsight-provision]: hdinsight-hadoop-provision-linux-clusters.md\r [hdinsight-admin-powershell]: hdinsight-administer-use-powershell.md\r [hdinsight-upload-data]: hdinsight-upload-data.md\r [hdinsight-use-hive]:hadoop/hdinsight-use-hive.md\r [hdinsight-use-pig]:hadoop/hdinsight-use-pig.md\r [hdinsight-storage]: hdinsight-hadoop-use-blob-storage.md\r [hdinsight-develop-java-mapreduce]:hadoop/apache-hadoop-develop-deploy-java-mapreduce-linux.md\r [hdinsight-use-oozie]: hdinsight-use-oozie.md\r \r [sqldatabase-get-started]: ../sql-database/sql-database-get-started.md\r \r [azure-management-portal]: https://portal.azure.cn/\r [azure-create-storageaccount]:../storage/common/storage-create-storage-account.md\r \r [apache-hadoop]: http://hadoop.apache.org/\r [apache-oozie-400]: http://oozie.apache.org/docs/4.0.0/\r [apache-oozie-332]: http://oozie.apache.org/docs/3.3.2/\r \r [powershell-download]: /downloads/\r [powershell-about-profiles]: http://go.microsoft.com/fwlink/?LinkID=113729\r [powershell-install-configure]: https://docs.microsoft.com/powershell/azureps-cmdlets-docs\r [powershell-start]: http://technet.microsoft.com/library/hh847889.aspx\r [powershell-script]: http://technet.microsoft.com/library/ee176949.aspx\r \r [cindygross-hive-tables]: http://blogs.msdn.com/b/cindygross/archive/2013/02/06/hdinsight-hive-internal-and-external-tables-intro.aspx\r \r [img-workflow-diagram]: ./media/hdinsight-use-oozie-coordinator-time/HDI.UseOozie.Workflow.Diagram.png\r [img-preparation-output]: ./media/hdinsight-use-oozie-coordinator-time/HDI.UseOozie.Preparation.Output1.png\r [img-runworkflow-output]: ./media/hdinsight-use-oozie-coordinator-time/HDI.UseOozie.RunCoord.Output.png\r \r [technetwiki-hive-error]: http://social.technet.microsoft.com/wiki/contents/articles/23047.hdinsight-hive-error-unable-to-rename.aspx\r \r \r <!--Update_Description: update wording and link references-->"}