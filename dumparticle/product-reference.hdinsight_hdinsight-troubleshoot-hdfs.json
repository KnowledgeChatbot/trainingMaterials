{"Title":"使用 Azure HDinsight 排除 HDFS 故障","Description":"获取有关使用 HDFS 和 Azure HDInsight 的常见问题答案。","Content":"# <a name=\"troubleshoot-hdfs-by-using-azure-hdinsight\"></a>使用 Azure HDInsight 排除 HDFS 故障\r \r 了解在 Apache Ambari 中使用 Hadoop 分布式文件系统 (HDFS) 有效负载时遇到的常见问题及其解决方法。\r \r ## <a name=\"how-do-i-access-local-hdfs-from-inside-a-cluster\"></a>如何从群集内访问本地 HDFS？\r \r ### <a name=\"issue\"></a>问题\r \r 从 HDInsight 群集内通过命令行和应用程序代码（而不是 Azure Blob 存储）访问本地 HDFS。   \r \r ### <a name=\"resolution-steps\"></a>解决步骤\r \r 1. 在命令提示符下，按原义使用 `hdfs dfs -D \"fs.default.name=hdfs://mycluster/\" ...`，如以下命令中所示：\r \r     ```apache\r     hdiuser@hn0-spark2:~$ hdfs dfs -D \"fs.default.name=hdfs://mycluster/\" -ls /\r     Found 3 items\r     drwxr-xr-x   - hdiuser hdfs          0 2017-03-24 14:12 /EventCheckpoint-30-8-24-11102016-01\r     drwx-wx-wx   - hive    hdfs          0 2016-11-10 18:42 /tmp\r     drwx------   - hdiuser hdfs          0 2016-11-10 22:22 /user\r     ```\r \r 2. 从源代码按原义使用 URI `hdfs://mycluster/`，如以下示例应用程序中所示：\r \r     ```csharp\r     import java.io.IOException;\r     import java.net.URI;\r     import org.apache.commons.io.IOUtils;\r     import org.apache.hadoop.conf.Configuration;\r     import org.apache.hadoop.fs.*;\r     \r     public class JavaUnitTests {\r     \r         public static void main(String[] args) throws Exception {\r     \r             Configuration conf = new Configuration();\r             String hdfsUri = \"hdfs://mycluster/\";\r             conf.set(\"fs.defaultFS\", hdfsUri);\r             FileSystem fileSystem = FileSystem.get(URI.create(hdfsUri), conf);\r             RemoteIterator<LocatedFileStatus> fileStatusIterator = fileSystem.listFiles(new Path(\"/tmp\"), true);\r             while(fileStatusIterator.hasNext()) {\r                 System.out.println(fileStatusIterator.next().getPath().toString());\r             }\r         }\r     }\r     ```\r     \r 3. 使用以下命令在 HDInsight 群集上运行已编译的 .jar 文件（例如，名为 `java-unit-tests-1.0.jar` 的文件）：\r \r     ```apache\r     hdiuser@hn0-spark2:~$ hadoop jar java-unit-tests-1.0.jar JavaUnitTests\r     hdfs://mycluster/tmp/hive/hive/5d9cf301-2503-48c7-9963-923fb5ef79a7/inuse.info\r     hdfs://mycluster/tmp/hive/hive/5d9cf301-2503-48c7-9963-923fb5ef79a7/inuse.lck\r     hdfs://mycluster/tmp/hive/hive/a0be04ea-ae01-4cc4-b56d-f263baf2e314/inuse.info\r     hdfs://mycluster/tmp/hive/hive/a0be04ea-ae01-4cc4-b56d-f263baf2e314/inuse.lck\r     ```\r \r \r ## <a name=\"how-do-i-force-disable-hdfs-safe-mode-in-a-cluster\"></a>如何在群集中强制禁用 HDFS 安全模式？\r \r ### <a name=\"issue\"></a>问题\r \r 本地 HDFS 在 HDInsight 群集上的安全模式下停止响应。   \r \r ### <a name=\"detailed-description\"></a>详细说明\r \r 运行以下 HDFS 命令时，将发生错误：\r \r ```apache\r hdfs dfs -D \"fs.default.name=hdfs://mycluster/\" -mkdir /temp\r ```\r \r 运行命令时，会看到以下错误：\r \r ```apache\r hdiuser@hn0-spark2:~$ hdfs dfs -D \"fs.default.name=hdfs://mycluster/\" -mkdir /temp\r 17/04/05 16:20:52 WARN retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.mkdirs over hn0-spark2.2oyzcdm4sfjuzjmj5dnmvscjpg.dx.internal.chinacloudapp.cn/10.0.0.22:8020. Not retrying because try once and fail.\r org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /temp. Name node is in safe mode.\r It was turned on manually. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off.\r         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1359)\r         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4010)\r         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1102)\r         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:630)\r         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\r         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)\r         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)\r         at java.security.AccessController.doPrivileged(Native Method)\r         at javax.security.auth.Subject.doAs(Subject.java:422)\r         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\r         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)\r         at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)\r         at org.apache.hadoop.ipc.Client.call(Client.java:1496)\r         at org.apache.hadoop.ipc.Client.call(Client.java:1396)\r         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r         at com.sun.proxy.$Proxy10.mkdirs(Unknown Source)\r         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:603)\r         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r         at java.lang.reflect.Method.invoke(Method.java:498)\r         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)\r         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)\r         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)\r         at com.sun.proxy.$Proxy11.mkdirs(Unknown Source)\r         at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3061)\r         at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3031)\r         at org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1162)\r         at org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1158)\r         at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r         at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1158)\r         at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1150)\r         at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1898)\r         at org.apache.hadoop.fs.shell.Mkdir.processNonexistentPath(Mkdir.java:76)\r         at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:273)\r         at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:255)\r         at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:119)\r         at org.apache.hadoop.fs.shell.Command.run(Command.java:165)\r         at org.apache.hadoop.fs.FsShell.run(FsShell.java:297)\r         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\r         at org.apache.hadoop.fs.FsShell.main(FsShell.java:350)\r mkdir: Cannot create directory /temp. Name node is in safe mode.\r ```\r \r ### <a name=\"probable-cause\"></a>可能的原因\r \r HDInsight 群集已减少到很少的节点。 节点数低于或接近于 HDFS 复制因子。\r \r ### <a name=\"resolution-steps\"></a>解决步骤 \r \r 1. 使用以下命令获取 HDInsight 群集上的 HDFS 状态：\r \r     ```apache\r     hdfs dfsadmin -D \"fs.default.name=hdfs://mycluster/\" -report\r     ```\r \r     ```apache\r     hdiuser@hn0-spark2:~$ hdfs dfsadmin -D \"fs.default.name=hdfs://mycluster/\" -report\r     Safe mode is ON\r     Configured Capacity: 3372381241344 (3.07 TB)\r     Present Capacity: 3138625077248 (2.85 TB)\r     DFS Remaining: 3102710317056 (2.82 TB)\r     DFS Used: 35914760192 (33.45 GB)\r     DFS Used%: 1.14%\r     Under replicated blocks: 0\r     Blocks with corrupt replicas: 0\r     Missing blocks: 0\r     Missing blocks (with replication factor 1): 0\r     \r     -------------------------------------------------\r     Live datanodes (8):\r     \r     Name: 10.0.0.17:30010 (10.0.0.17)\r     Hostname: 10.0.0.17\r     Decommission Status : Normal\r     Configured Capacity: 421547655168 (392.60 GB)\r     DFS Used: 5288128512 (4.92 GB)\r     Non DFS Used: 29087272960 (27.09 GB)\r     DFS Remaining: 387172253696 (360.58 GB)\r     DFS Used%: 1.25%\r     DFS Remaining%: 91.85%\r     Configured Cache Capacity: 0 (0 B)\r     Cache Used: 0 (0 B)\r     Cache Remaining: 0 (0 B)\r     Cache Used%: 100.00%\r     Cache Remaining%: 0.00%\r     Xceivers: 2\r     Last contact: Wed Apr 05 16:22:00 UTC 2017\r     ...\r     ```\r \r 2. 使用以下命令检查 HDInsight 群集上的 HDFS 完整性：\r \r     ```apache\r     hdiuser@hn0-spark2:~$ hdfs fsck -D \"fs.default.name=hdfs://mycluster/\" /\r     ```\r     \r     ```apache\r     Connecting to namenode via http://hn0-spark2.2oyzcdm4sfjuzjmj5dnmvscjpg.dx.internal.chinacloudapp.cn:30070/fsck?ugi=hdiuser&path=%2F\r     FSCK started by hdiuser (auth:SIMPLE) from /10.0.0.22 for path / at Wed Apr 05 16:40:28 UTC 2017\r     ....................................................................................................\r     \r     ....................................................................................................\r     ..................Status: HEALTHY\r      Total size:    9330539472 B\r      Total dirs:    37\r      Total files:   2618\r      Total symlinks:                0 (Files currently being written: 2)\r      Total blocks (validated):      2535 (avg. block size 3680686 B)\r      Minimally replicated blocks:   2535 (100.0 %)\r      Over-replicated blocks:        0 (0.0 %)\r      Under-replicated blocks:       0 (0.0 %)\r      Mis-replicated blocks:         0 (0.0 %)\r      Default replication factor:    3\r      Average block replication:     3.0\r      Corrupt blocks:                0\r      Missing replicas:              0 (0.0 %)\r      Number of data-nodes:          8\r      Number of racks:               1\r     FSCK ended at Wed Apr 05 16:40:28 UTC 2017 in 187 milliseconds\r     \r     The filesystem under path '/' is HEALTHY\r     ```\r \r 3. 如果确定没有缺失、损坏或复制不足的块，或者这些块可以忽略，请运行以下命令使名称节点退出安全模式：\r \r     ```apache\r     hdfs dfsadmin -D \"fs.default.name=hdfs://mycluster/\" -safemode leave\r     ```\r ### <a name=\"see-also\"></a>另请参阅\r [使用 Azure HDInsight 进行故障排除](hdinsight-troubleshoot-guide.md)\r \r \r <!--Update_Description: update wording and link references-->"}