{"Title":"将 Java 用户定义函数 (UDF) 与 HDInsight 中的 Hive 配合使用 - Azure","Description":"了解如何创建可用于 Hive 的基于 Java 的用户定义的函数 (UDF)。 此示例 UDF 将文本字符串表转换为小写。","Content":"# <a name=\"use-a-java-udf-with-hive-in-hdinsight\"></a>在 HDInsight 中通过 Hive 使用 Java UDF\r \r 了解如何创建可用于 Hive 的基于 Java 的用户定义的函数 (UDF)。 此示例中的 Java UDF 将文本字符串表转换为全小写字符。\r \r ## <a name=\"requirements\"></a>要求\r \r [!INCLUDE [hdinsight-linux-acn-version.md](../../../includes/hdinsight-linux-acn-version.md)]\r \r * HDInsight 群集 \r \r     > [!IMPORTANT]\r     > Linux 是在 HDInsight 3.4 版或更高版本上使用的唯一操作系统。 有关详细信息，请参阅 [HDInsight 在 Windows 上停用](../hdinsight-component-versioning.md#hdinsight-windows-retirement)。\r \r     本文档中的大多数步骤同时适用于基于 Windows 和基于 Linux 的群集。 但是，用于将已编译的 UDF 上传到群集并运行的步骤特定于基于 Linux 的群集。 提供可用于基于 Windows 的群集的信息的链接。\r \r * [Java JDK](http://www.oracle.com/technetwork/java/javase/downloads/) 8 或更高版本（或类似程序，如 OpenJDK）\r \r * [Apache Maven](http://maven.apache.org/)\r \r * 文本编辑器或 Java IDE\r \r     > [!IMPORTANT]\r     > 如果在 Windows 客户端上创建 Python 文件，则必须使用将 LF 用作行尾的编辑器。 如果无法确定编辑器使用的是 LF 还是 CRLF，请参阅[故障排除](#troubleshooting)部分，了解删除 CR 字符的步骤。\r     >\r \r ## <a name=\"create-an-example-java-udf\"></a>创建 Java UDF 示例 \r \r 1. 从命令行中，使用以下命令新建 Maven 项目：\r \r     ```bash\r     mvn archetype:generate -DgroupId=com.microsoft.examples -DartifactId=ExampleUDF -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\r     ```\r \r    > [!NOTE]\r    > 如果使用 PowerShell，必须将参数用引号引起来。 例如，`mvn archetype:generate \"-DgroupId=com.microsoft.examples\" \"-DartifactId=ExampleUDF\" \"-DarchetypeArtifactId=maven-archetype-quickstart\" \"-DinteractiveMode=false\"`。\r \r     此命令创建一个名为 **exampleudf** 的目录，其中包含 Maven 项目。\r \r 2. 创建该项目后，删除作为项目的一部分创建的 **exampleudf/src/test** 目录。\r \r 3. 打开 **exampleudf/pom.xml**，将现有 `<dependencies>` 条目替换为以下 XML：\r \r     ```xml\r     <dependencies>\r         <dependency>\r             <groupId>org.apache.hadoop</groupId>\r             <artifactId>hadoop-client</artifactId>\r             <version>2.7.3</version>\r             <scope>provided</scope>\r         </dependency>\r         <dependency>\r             <groupId>org.apache.hive</groupId>\r             <artifactId>hive-exec</artifactId>\r             <version>1.2.1</version>\r             <scope>provided</scope>\r         </dependency>\r     </dependencies>\r     ```\r \r     这些条目指定了 HDInsight 3.5 中包含的 Hadoop 和 Hive 版本。 可以在 [HDInsight 组件版本控制](../hdinsight-component-versioning.md)文档中找到 HDInsight 提供的 Hadoop 和 Hive 的版本信息。\r \r     在文件末尾的 `</project>` 行之前添加 `<build>` 部分。 该部分应包含以下 XML：\r \r     ```xml\r     <build>\r         <plugins>\r             <!-- build for Java 1.8. This is required by HDInsight 3.5  -->\r             <plugin>\r                 <groupId>org.apache.maven.plugins</groupId>\r                 <artifactId>maven-compiler-plugin</artifactId>\r                 <version>3.3</version>\r                 <configuration>\r                     <source>1.8</source>\r                     <target>1.8</target>\r                 </configuration>\r             </plugin>\r             <!-- build an uber jar -->\r             <plugin>\r                 <groupId>org.apache.maven.plugins</groupId>\r                 <artifactId>maven-shade-plugin</artifactId>\r                 <version>2.3</version>\r                 <configuration>\r                     <!-- Keep us from getting a can't overwrite file error -->\r                     <transformers>\r                         <transformer\r                                 implementation=\"org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer\">\r                         </transformer>\r                         <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\">\r                         </transformer>\r                     </transformers>\r                     <!-- Keep us from getting a bad signature error -->\r                     <filters>\r                         <filter>\r                             <artifact>*:*</artifact>\r                             <excludes>\r                                 <exclude>META-INF/*.SF</exclude>\r                                 <exclude>META-INF/*.DSA</exclude>\r                                 <exclude>META-INF/*.RSA</exclude>\r                             </excludes>\r                         </filter>\r                     </filters>\r                 </configuration>\r                 <executions>\r                     <execution>\r                         <phase>package</phase>\r                         <goals>\r                             <goal>shade</goal>\r                         </goals>\r                     </execution>\r                 </executions>\r             </plugin>\r         </plugins>\r     </build>\r     ```\r \r     这些条目用于定义如何生成项目。 具体而言，项目使用的 Java 版本以及如何生成部署到群集的 uberjar。\r \r     一旦进行了更改，请保存该文件。\r \r 4. 将 **exampleudf/src/main/java/com/microsoft/examples/App.java** 重命名为 **ExampleUDF.java**，并在编辑器中打开该文件。\r \r 5. 将 **ExampleUDF.java** 文件的内容替换为以下内容，并保存该文件。\r \r     ```java\r     package com.microsoft.examples;\r \r     import org.apache.hadoop.hive.ql.exec.Description;\r     import org.apache.hadoop.hive.ql.exec.UDF;\r     import org.apache.hadoop.io.*;\r \r     // Description of the UDF\r     @Description(\r         name=\"ExampleUDF\",\r         value=\"returns a lower case version of the input string.\",\r         extended=\"select ExampleUDF(deviceplatform) from hivesampletable limit 10;\"\r     )\r     public class ExampleUDF extends UDF {\r         // Accept a string input\r         public String evaluate(String input) {\r             // If the value is null, return a null\r             if(input == null)\r                 return null;\r             // Lowercase the input string and return it\r             return input.toLowerCase();\r         }\r     }\r     ```\r \r     该代码实现接受一个字符串值，并返回该字符串的小写形式的 UDF。\r \r ## <a name=\"build-and-install-the-udf\"></a>生成并安装 UDF\r \r 1. 使用以下命令编译和打包 UDF：\r \r     ```bash\r     mvn compile package\r     ```\r \r     此命令生成 UDF 并将其打包到 `exampleudf/target/ExampleUDF-1.0-SNAPSHOT.jar` 文件中。\r \r 2. 使用 `scp` 命令将文件复制到 HDInsight 群集。\r \r     ```bash\r     scp ./target/ExampleUDF-1.0-SNAPSHOT.jar myuser@mycluster-ssh.azurehdinsight\r     ```\r \r     将 `myuser` 替换为群集的 SSH 用户帐户。 将 `mycluster` 替换为群集名称。 如果使用了密码保护 SSH 帐户，系统会提示输入该密码。 如果使用了证书，则可能需要使用 `-i` 参数指定私钥文件。\r \r 3. 使用 SSH 连接到群集。\r \r     ```bash\r     ssh myuser@mycluster-ssh.azurehdinsight.cn\r     ```\r \r     有关详细信息，请参阅 [Use SSH with HDInsight](../hdinsight-hadoop-linux-use-ssh-unix.md)（对 HDInsight 使用 SSH）。\r \r 4. 从 SSH 会话中，将 jar 文件复制到 HDInsight 存储。\r \r     ```bash\r     hdfs dfs -put ExampleUDF-1.0-SNAPSHOT.jar /example/jars\r     ```\r \r ## <a name=\"use-the-udf-from-hive\"></a>在 Hive 中使用 UDF\r \r 1. 使用以下命令在 SSH 会话中启动 Beeline 客户端。\r \r     ```bash\r     beeline -u 'jdbc:hive2://localhost:10001/;transportMode=http'\r     ```\r \r     该命令假定你使用默认的群集登录帐户 **admin** 。\r \r 2. 当到达 `jdbc:hive2://localhost:10001/>` 提示符时，输入以下代码将 UDF 添加到 Hive，并将其作为函数公开。\r \r     ```hiveql\r     ADD JAR wasb:///example/jars/ExampleUDF-1.0-SNAPSHOT.jar;\r     CREATE TEMPORARY FUNCTION tolower as 'com.microsoft.examples.ExampleUDF';\r     ```\r \r 3. 使用该 UDF 将从表中检索的值转换为小写字符串。\r \r     ```hiveql\r     SELECT tolower(deviceplatform) FROM hivesampletable LIMIT 10;\r     ```\r \r     此查询将从表中选择设备平台（Android、Windows、iOS 等），并将字符串转换为小写字符串，并显示它们。 显示的输出类似于以下文本：\r \r         +----------+--+\r         |   _c0    |\r         +----------+--+\r         | android  |\r         | android  |\r         | android  |\r         | android  |\r         | android  |\r         | android  |\r         | android  |\r         | android  |\r         | android  |\r         | android  |\r         +----------+--+\r \r ## <a name=\"next-steps\"></a>后续步骤\r \r 有关使用 Hive 的其他方式，请参阅[将 HDInsight 与 Hive 配合使用](hdinsight-use-hive.md)。\r \r 有关 Hive 用户定义函数的详细信息，请参阅 apache.org 网站上的 Hive wiki 的 [Hive 运算符和用户定义函数](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF) 部分。\r \r \r <!--Update_Description: update wording and link references-->"}