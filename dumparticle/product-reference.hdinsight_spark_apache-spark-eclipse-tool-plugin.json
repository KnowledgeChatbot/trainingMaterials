{"Title":"Azure Toolkit for Eclipse：为 HDInsight Spark 创建 Scala 应用程序","Description":"使用 Azure Toolkit for Eclipse 中的 HDInsight 工具开发以 Scala 编写的 Spark 应用程序，并将应用程序直接从 Eclipse IDE 提交到 HDInsight Spark 群集。","Content":"# <a name=\"use-azure-toolkit-for-eclipse-to-create-spark-applications-for-an-hdinsight-cluster\"></a>使用用于 Eclipse 的 Azure 工具包为 HDInsight 群集创建 Spark 应用程序\r \r 在用于 Eclipse 的 Azure 工具包中使用 HDInsight 工具开发以 Scala 编写的 Spark 应用程序，并直接从 Eclipse IDE 将其提交到 Azure HDInsight Spark 群集。 可以通过多种不同方式使用 HDInsight 工具插件：\r \r * 在 HDInsight Spark 群集中开发和提交 Scala Spark 应用程序\r * 访问 Azure HDInsight Spark 群集资源\r * 本地开发和运行 Scala Spark 应用程序\r \r > [!IMPORTANT]\r > 此工具可用于在 Linux 上的 HDInsight Spark 群集中创建和提交应用程序。\r > \r > \r \r ## <a name=\"prerequisites\"></a>先决条件\r \r * HDInsight 上的 Apache Spark 群集。 有关说明，请参阅[在 Azure HDInsight 中创建 Apache Spark 群集](apache-spark-jupyter-spark-sql.md)。\r * Oracle Java 开发工具包版本 8，用于 Eclipse IDE 运行时。 可以从 [Oracle 网站](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)下载它。\r * Eclipse IDE。 本文使用 Eclipse Neon。 可以从 [Eclipse 网站](https://www.eclipse.org/downloads/)安装它。\r \r \r \r ## <a name=\"install-hdinsight-tools-in-azure-toolkit-for-eclipse-and-the-scala-plug-in\"></a>安装 Azure Toolkit for Eclipse 中的 HDInsight 工具和 Scala 插件\r ### <a name=\"install-hdinsight-toolsazure-toolkit-for\"></a>安装 HDInsight 工具\r 用于 Eclipse 的 HDInsight 工具作为用于 Eclipse 的 Azure 工具包的一部分提供。 有关安装说明，请参阅[安装用于 Eclipse 的 Azure 工具包](../../azure-toolkit-for-eclipse-installation.md)。\r ### <a name=\"install-the-scala-plug-in\"></a>安装 Scala 插件\r 打开 Eclipse 时，HDInsight 工具会自动检测是否安装了 Scala 插件。 选择“确定”继续，然后按照说明从 Eclipse Marketplace 安装插件。\r \r ![自动安装 Scala 插件](./media/apache-spark-eclipse-tool-plugin/auto-install-scala.png)\r \r ## <a name=\"sign-in-to-your-azure-subscription\"></a>登录到 Azure 订阅\r 1. 启动 Eclipse IDE 并打开 Azure 资源管理器。 在“窗口”菜单中选择“显示视图”，并选择“其他”。 在打开的对话框中展开“Azure”，选择“Azure 资源管理器”，再选择“确定”。\r \r    ![“显示视图”对话框](./media/apache-spark-eclipse-tool-plugin/view-explorer-1.png)\r    \r 2. 右键单击“Azure”节点，再选择“登录”。\r 3. 在“Azure 登录”对话框框中，选择“身份验证方法”，选择“登录”并输入 Azure 凭据。\r    \r    ![Azure 登录对话框](./media/apache-spark-eclipse-tool-plugin/view-explorer-2.png)\r    \r 4. 登录之后，“选择订阅”对话框会列出与凭据关联的所有 Azure 订阅。 单击“选择”关闭对话框。\r \r    ![“选择订阅”对话框](./media/apache-spark-eclipse-tool-plugin/Select-Subscriptions.png)\r    \r 5. 在“Azure 资源管理器”选项卡中展开“HDInsight”，查看订阅下的 HDInsight Spark 群集。\r    \r    ![Azure 资源管理器中的 HDInsight Spark 群集](./media/apache-spark-eclipse-tool-plugin/view-explorer-3.png)\r    \r 6. 可以进一步展开群集名称节点，查看与群集关联的资源（例如存储帐户）。\r    \r    ![展开群集名称可查看资源](./media/apache-spark-eclipse-tool-plugin/view-explorer-4.png)\r \r \r \r ## <a name=\"set-up-a-spark-scala-project-for-an-hdinsight-spark-cluster\"></a>为 HDInsight Spark 群集设置 Spark Scala 项目\r \r 1. 在 Eclipse IDE 工作区中，依次选择“文件”、“新建”，然后选择“项目”。 \r 2. 在“新建项目”向导中，展开“HDInsight”，选择“Spark on HDInsight (Scala)”，并选择“下一步”。\r \r    ![选择 Spark on HDInsight (Scala) 项目](./media/apache-spark-eclipse-tool-plugin/create-hdi-scala-app-2.png)\r    \r 3. Scala 项目创建向导会自动检测是否安装了 Scala 插件。 选择“确定”继续下载 Scala 插件，然后按照说明重启 Eclipse。\r \r    ![Scala 检查](./media/apache-spark-eclipse-tool-plugin/auto-install-scala-2.png)\r    \r 4. 在“新建 HDInsight Scala 项目”对话框中提供以下值，然后选择“下一步”：\r    * 输入项目的名称。\r    * 在“JRE”区域中，确保“使用执行环境 JRE”设置为“JavaSE-1.7”或更高版本。\r    * 在“Spark 库”区域中，可以选择“使用 Maven 配置 Spark SDK”选项。  我们的工具集成了适当版本的 Spark SDK 和 Scala SDK。 也可以选择“手动添加 Spark SDK”选项，手动下载并添加 Spark SDK。\r \r    ![“新建 HDInsight Scala 项目”对话框](./media/apache-spark-eclipse-tool-plugin/create-hdi-scala-app-3.png)\r    \r 5. 在下一个对话框中，选择“完成”。 \r    \r   \r ## <a name=\"create-a-scala-application-for-an-hdinsight-spark-cluster\"></a>为 HDInsight Spark 群集创建 Scala 应用程序\r \r 1. 在 Eclipse IDE 中，从“包资源管理器”展开之前创建的项目，右键单击“src”，指向“新建”，再选择“其他”。\r 2. 在“选择向导”对话框中，展开“Scala 向导”，选择“Scala 对象”，再选择“下一步”。\r    \r    ![选择向导对话框](./media/apache-spark-eclipse-tool-plugin/create-scala-proj-1.png)\r    \r 3. 在“创建新文件”对话框中，输入对象的名称，然后选择“完成”。\r    \r    ![“创建新文件”对话框](./media/apache-spark-eclipse-tool-plugin/create-scala-proj-2.png)\r    \r 4. 在文本编辑器中粘贴以下代码：\r \r         import org.apache.spark.SparkConf\r         import org.apache.spark.SparkContext\r \r         object MyClusterApp{\r           def main (arg: Array[String]): Unit = {\r             val conf = new SparkConf().setAppName(\"MyClusterApp\")\r             val sc = new SparkContext(conf)\r \r             val rdd = sc.textFile(\"wasb:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\")\r \r             //find the rows that have only one digit in the seventh column in the CSV\r             val rdd1 =  rdd.filter(s => s.split(\",\")(6).length() == 1)\r \r             rdd1.saveAsTextFile(\"wasb:///HVACOut\")\r           }        \r         }\r 5. 在 HDInsight Spark 群集中运行该应用程序：\r    \r    a. 在“包资源管理器”中，右键单击项目名称，然后选择“将 Spark 应用程序提交到 HDInsight”。        \r    b. 在“Spark 提交”对话框中提供以下值，然后选择“提交”：\r       \r       * 对于“群集名称”，选择要在其上运行应用程序的 HDInsight Spark 群集。\r       * 从 Eclipse 项目或硬盘中选择一个项目。 默认值取决于从包资源管理器右键单击的项。\r       * 在“主类名”下拉列表中，提交向导将显示项目中的所有对象名。 选择或输入要运行的对象的名称。 如果从硬盘中选择项目，必须手动输入主类名。 \r       * 由于本示例中的应用程序代码不需要任何命令行参数，也不需要引用 JAR 或文件，因此可以将其余的文本框留空。\r         \r       ![“Spark 提交”对话框](./media/apache-spark-eclipse-tool-plugin/create-scala-proj-3.png)\r       \r 6. “Spark 提交”选项卡应开始显示进度。 可以通过选择“Spark 提交”窗口中的红色按钮停止应用程序。 也可以选择地球图标（以图中的蓝色框表示），查看此特定应用程序运行的日志。\r       \r    ![“Spark 提交”窗口](./media/apache-spark-eclipse-tool-plugin/create-scala-proj-4.png)\r \r ## <a name=\"access-and-manage-hdinsight-spark-clusters-by-using-hdinsight-tools-in-azure-toolkit-for-eclipse\"></a>使用 Azure Toolkit for Eclipse 中的 HDInsight 工具访问和管理 HDInsight Spark 群集\r 可以使用 HDInsight 工具执行各种操作，包括访问作业输出。\r \r ### <a name=\"access-the-job-view\"></a>访问作业视图\r 1. 在“Azure 资源管理器”中依次展开“HDInsight”和 Spark 群集名称，并选择“作业”。 \r \r    ![“作业视图”节点](./media/apache-spark-eclipse-tool-plugin/job-view-node.png)\r \r 2. 选择“作业”节点。 如果 Java 版本低于 **1.8**，HDInsight 工具会自动提醒你安装 **E(fx)clipse** 插件。 选择“确定”继续，然后按照向导指示从 Eclipse Marketplace 安装该插件并重启 Eclipse。 \r \r    ![安装 E(fx)clipse](./media/apache-spark-eclipse-tool-plugin/auto-install-efxclipse.png)\r \r 3. 从“作业”节点打开作业视图。 在右窗格中，“Spark 作业视图”选项卡显示了群集上运行的所有应用程序。 选择想要查看其详细信息的应用程序的名称。\r \r    ![应用程序详细信息](./media/apache-spark-eclipse-tool-plugin/view-job-logs.png)\r \r    然后，可以执行以下任一操作：\r \r    * 将鼠标悬停在作业图上。 将显示有关运行作业的基本信息。 选择作业图，可以看到每个作业生成的阶段和信息。\r \r      ![作业阶段详细信息](./media/apache-spark-eclipse-tool-plugin/Job-graph-stage-info.png)\r \r    * 选择“日志”选项卡查看常用的日志，包括“驱动程序 Stderr”、“驱动程序 Stdout”和“目录信息”。\r \r      ![日志详细信息](./media/apache-spark-eclipse-tool-plugin/Job-log-info.png)\r \r    * 选择窗口顶部的超链接打开 Spark 历史记录 UI 和 YARN UI（应用程序级别）。\r \r ### <a name=\"access-the-storage-container-for-the-cluster\"></a>访问群集的存储容器\r 1. 在 Azure 资源管理器中展开“HDInsight”根节点，查看可用 HDInsight Spark 群集的列表。\r 2. 展开群集名称以查看群集的存储帐户和默认存储容器。\r    \r    ![存储帐户和默认存储容器](./media/apache-spark-eclipse-tool-plugin/view-explorer-5.png)\r    \r 3. 选择与群集关联的存储容器名称。 在右窗格中，双击“HVACOut”文件夹。 打开其中一个 **part-** 文件可查看应用程序的输出。\r \r ### <a name=\"access-the-spark-history-server\"></a>访问 Spark 历史记录服务器\r 1. 在“Azure 资源管理器”中，右键单击 Spark 群集名称，然后选择“打开 Spark 历史记录 UI”。 出现提示时，请输入群集的管理员凭据。 预配群集时已指定这些凭据。\r 2. 在“Spark 历史记录服务器”仪表板中，可以使用应用程序名称查找刚运行完的应用程序。 在上述代码中，已使用 `val conf = new SparkConf().setAppName(\"MyClusterApp\")` 设置了应用程序名称。 因此，Spark 应用程序名称为“MyClusterApp”。\r \r ### <a name=\"start-the-ambari-portal\"></a>启动 Ambari 门户\r 1. 在“Azure 资源管理器”中，右键单击 Spark 群集名称，然后选择“打开群集管理门户(Ambari)”。 \r 2. 出现提示时，请输入群集的管理员凭据。 预配群集时已指定这些凭据。\r \r ### <a name=\"manage-azure-subscriptions\"></a>管理 Azure 订阅\r 默认情况下，用于 Eclipse 的 Azure 工具包中的 HDInsight 工具将列出所有 Azure 订阅中的 Spark 群集。 如果需要，可以指定想要访问其群集的订阅。 \r \r 1. 在“Azure 资源管理器”中，右键单击“Azure”根节点，并选择“管理订阅”。 \r 2. 在对话框中，清除不想访问的订阅对应的复选框，然后选择“关闭”。 如果想要从 Azure 订阅注销，可以选择“注销”。\r \r ## <a name=\"run-a-spark-scala-application-locally\"></a>本地运行 Spark Scala 应用程序\r 可以使用用于 Eclipse 的 Azure 工具包中的 HDInsight 工具在工作站上本地运行 Spark Scala 应用程序。 通常，这些应用程序不需要访问群集资源（如存储容器），并可以在本地运行和测试。\r \r ### <a name=\"prerequisite\"></a>先决条件\r 在 Windows 计算机上运行本地 Spark Scala 应用程序时，可能会发生 [SPARK-2356](https://issues.apache.org/jira/browse/SPARK-2356) 中所述的异常。 发生这些异常的原因是 Windows 中缺少 **WinUtils.exe**。 \r \r 若要解决此错误，需要[下载可执行文件](http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe)到所需位置（例如 **C:\\WinUtils\\bin**），然后添加环境变量 **HADOOP_HOME**，并将该变量的值设为 **C\\WinUtils**。\r \r ### <a name=\"run-a-local-spark-scala-application\"></a>运行本地的 Spark Scala 应用程序\r 1. 启动 Eclipse 并创建项目。 在“新建项目”对话框中做出以下选择，然后选择“下一步”。\r    \r    * 在左窗格中，选择“HDInsight”。\r    * 在右窗格中，选择“Spark on HDInsight 本地运行示例(Scala)”。\r \r    ![“新建项目”对话框](./media/apache-spark-eclipse-tool-plugin/hdi-spark-app-local-run.png)\r    \r 2. 若要提供项目详细信息，请执行前面部分[为 HDInsight Spark 群集设置 Spark Scala 项目](#set-up-a-spark-scala-project-for-an-hdinsight-spark-cluster)中的步骤 3 到步骤 6。\r \r 3. 模板将在 **src** 文件夹下面添加可在计算机上本地运行的示例代码 (**LogQuery**)。\r    \r    ![LogQuery 的位置](./media/apache-spark-eclipse-tool-plugin/local-app.png)\r    \r 4. 右键单击“LogQuery”应用程序，指向“运行方式”，然后选择“1 Scala 应用程序”。 “控制台”选项卡上将出现如下所示的输出：\r    \r    ![Spark 应用程序本地运行结果](./media/apache-spark-eclipse-tool-plugin/hdi-spark-app-local-run-result.png)\r \r ## <a name=\"known-problems\"></a>已知问题\r 目前不支持直接查看 Spark 输出。\r \r ## <a name=\"feedback\"></a>反馈\r 如果有任何反馈，或使用此工具时遇到任何问题，请向 hdivstool@microsoft.com 发送电子邮件。\r \r ## <a name=\"seealso\"></a>另请参阅\r * [概述：Azure HDInsight 上的 Apache Spark](apache-spark-overview.md)\r \r ### <a name=\"scenarios\"></a>方案\r * [Spark 和 BI：使用 HDInsight 中的 Spark 和 BI 工具执行交互式数据分析](apache-spark-use-bi-tools.md)\r * [Spark 和机器学习：使用 HDInsight 中的 Spark 对使用 HVAC 数据生成温度进行分析](apache-spark-ipython-notebook-machine-learning.md)\r * [Spark 和机器学习：使用 HDInsight 中的 Spark 预测食品检查结果](apache-spark-machine-learning-mllib-ipython.md)\r * [Spark 流式处理：使用 HDInsight 中的 Spark 生成实时流式处理应用程序](../hdinsight-apache-spark-eventhub-streaming.md)\r * [使用 HDInsight 中的 Spark 分析网站日志](apache-spark-custom-library-website-log-analysis.md)\r \r ### <a name=\"creating-and-running-applications\"></a>创建和运行应用程序\r * [使用 Scala 创建独立的应用程序](apache-spark-create-standalone-application.md)\r * [使用 Livy 在 Spark 群集中远程运行作业](apache-spark-livy-rest-interface.md)\r \r ### <a name=\"tools-and-extensions\"></a>工具和扩展\r * [使用用于 IntelliJ 的 Azure 工具包创建和提交 Spark Scala 应用程序](apache-spark-intellij-tool-plugin.md)\r * [使用 Azure Toolkit for IntelliJ 通过 VPN 远程调试 Spark 应用程序](../hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)\r * [通过 SSH 使用用于 IntelliJ 的 Azure 工具包远程调试 Spark 应用程序](../hdinsight-apache-spark-intellij-tool-debug-remotely-through-ssh.md)\r * [将用于 IntelliJ 的 HDInsight 工具与 Hortonworks 沙盒配合使用](../hadoop/hdinsight-tools-for-intellij-with-hortonworks-sandbox.md)\r * [在 HDInsight 上的 Spark 群集中使用 Zeppelin 笔记本](apache-spark-zeppelin-notebook.md)\r * [在 HDInsight 的 Spark 群集中可用于 Jupyter 笔记本的内核](apache-spark-jupyter-notebook-kernels.md)\r * [Use external packages with Jupyter notebooks（将外部包与 Jupyter 笔记本配合使用）](apache-spark-jupyter-notebook-use-external-packages.md)\r * [Install Jupyter on your computer and connect to an HDInsight Spark cluster（在计算机上安装 Jupyter 并连接到 HDInsight Spark 群集）](apache-spark-jupyter-notebook-install-locally.md)\r \r ### <a name=\"managing-resources\"></a>管理资源\r * [管理 Azure HDInsight 中 Apache Spark 群集的资源](apache-spark-resource-manager.md)\r * [Track and debug jobs running on an Apache Spark cluster in HDInsight（跟踪和调试 HDInsight 中的 Apache Spark 群集上运行的作业）](apache-spark-job-debugging.md)\r \r \r \r <!--Update_Description: update wording and link references-->"}