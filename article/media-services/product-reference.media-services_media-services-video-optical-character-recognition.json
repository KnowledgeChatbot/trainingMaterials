{"Title":"使用 Azure 媒体分析 OCR 将文本数字化","Description":"Azure 媒体分析 OCR（光学字符识别）可让你将视频文件中的文本内容转换成可编辑、可搜索的数字文本。  这可让你从媒体的视频信号中自动提取有意义的元数据。","Content":"# <a name=\"use-azure-media-analytics-to-convert-text-content-in-video-files-into-digital-text\"></a>使用 Azure 媒体分析将视频文件中的文本内容转换为数字文本\n## <a name=\"overview\"></a>概述\n如果需要提取视频文件的文本内容，并生成可编辑、可搜索的数字文本，则应该使用 Azure 媒体分析 OCR（光学字符识别）。 此 Azure 媒体处理器可检测视频文件的文本内容并生成文本文件供你使用。 OCR 可让你从媒体的视频信号中自动提取有意义的元数据。\n\n与搜索引擎配合使用时，可以根据文本轻松编制媒体的索引，并增强发现内容的能力。 这在包含大量文本的视频（例如视频录制或者幻灯片演示屏幕截图）中非常有用。Azure OCR 媒体处理器已针对数字文本进行优化。\n\n**Azure 媒体 OCR** 媒体处理器目前以预览版提供。\n\n本主题提供有关 **Azure 媒体 OCR** 的详细信息，并演示如何通过适用于 .NET 的媒体服务 SDK 使用它。 有关其他信息和示例，请参阅[此博客](https://azure.microsoft.com/blog/announcing-video-ocr-public-preview-new-config/)。\n\n## <a name=\"ocr-input-files\"></a>OCR 输入文件\n视频文件。 目前支持以下格式：MP4、MOV 和 WMV。\n\n## <a name=\"task-configuration\"></a>任务配置\n任务配置（预设） 在使用 **Azure 媒体 OCR** 创建任务时，必须使用 JSON 或 XML 指定配置预设。 \n\n>[!NOTE]\n>在高度/宽度上，OCR 引擎仅将最小 40 像素到最大 32000 像素的图像区域视为有效输入。\n>\n\n### <a name=\"attribute-descriptions\"></a>属性说明\n| 属性名称 | 说明 |\n| --- | --- |\n|AdvancedOutput| 如果将 AdvancedOutput 设置为 true，则 JSON 输出将包含每个单词的位置数据（除了短语和区域以外）。 如果不想查看这些详细信息，请将标志设置为 false。 默认值为 false。 有关详细信息，请参阅[此博客](https://azure.microsoft.com/blog/azure-media-ocr-simplified-output/)。|\n| 语言 |（可选）描述要查找的文本的语言。 下列其中一项：AutoDetect（默认值）、Arabic、ChineseSimplified、ChineseTraditional、Czech Danish、Dutch、English、Finnish、French、German、Greek、Hungarian、Italian、Japanese、Korean、Norwegian、Polish、Portuguese、Romanian、Russian、SerbianCyrillic、SerbianLatin、Slovak、Spanish、Swedish、Turkish。 |\n| TextOrientation |（可选）描述要查找的文本的方向。  “Left”表示所有字母顶部朝向左侧。  默认文本（例如书籍中出现的文本）的方向为“Up”。  下列其中一项：AutoDetect（默认值）、Up、Right、Down、Left。 |\n| TimeInterval |（可选）描述采样率。  默认值为每 1/2 秒。<br/>JSON 格式 - HH:mm:ss.SSS（默认值 00:00:00.500）<br/>XML 格式 - W3C XSD 持续时间基元（默认值 PT0.5） |\n| DetectRegions |（可选）指定要在其中检测文本的视频帧中的区域的 DetectRegion 对象数组。<br/>DetectRegion 对象由以下四个整数值组成：<br/>Left – 左边距中的像素<br/>Top – 上边距中的像素<br/>Width – 以像素为单位的区域宽度<br/>Height – 以像素为单位的区域高度 |\n\n#### <a name=\"json-preset-example\"></a>JSON 预设示例\n\n    {\n        \"Version\":1.0, \n        \"Options\": \n        {\n            \"AdvancedOutput\":\"true\",\n            \"Language\":\"English\", \n            \"TimeInterval\":\"00:00:01.5\",\n            \"TextOrientation\":\"Up\",\n            \"DetectRegions\": [\n                    {\n                       \"Left\": 10,\n                       \"Top\": 10,\n                       \"Width\": 100,\n                       \"Height\": 50\n                    }\n             ]\n        }\n    }\n\n#### <a name=\"xml-preset-example\"></a>XML 预设示例\n    <?xml version=\"\"1.0\"\" encoding=\"\"utf-16\"\"?>\n    <VideoOcrPreset xmlns:xsi=\"\"http://www.w3.org/2001/XMLSchema-instance\"\" xmlns:xsd=\"\"http://www.w3.org/2001/XMLSchema\"\" Version=\"\"1.0\"\" xmlns=\"\"http://www.windowsazure.com/media/encoding/Preset/2014/03\"\">\n      <Options>\n         <AdvancedOutput>true</AdvancedOutput>\n         <Language>English</Language>\n         <TimeInterval>PT1.5S</TimeInterval>\n         <DetectRegions>\n             <DetectRegion>\n                   <Left>10</Left>\n                   <Top>10</Top>\n                   <Width>100</Width>\n                   <Height>50</Height>\n            </DetectRegion>\n       </DetectRegions>\n       <TextOrientation>Up</TextOrientation>\n      </Options>\n    </VideoOcrPreset>\n\n## <a name=\"ocr-output-files\"></a>OCR 输出文件\nOCR 媒体处理器的输出是一个 JSON 文件。\n\n### <a name=\"elements-of-the-output-json-file\"></a>输出 JSON 文件中的元素\n视频 OCR 输出针对视频中的字符提供时间分段数据。  可以使用属性（例如语言或方向）来锁定你想要分析的文本。 \n\n输出包含以下属性：\n\n| 元素 | 说明 |\n| --- | --- |\n| 时间刻度 |视频每秒的“刻度”数 |\n| Offset |时间戳的时间偏移量。 在版本 1.0 的视频 API 中，此属性始终为 0。 |\n| Framerate |视频的每秒帧数 |\n| width |以像素为单位的视频宽度 |\n| height |以像素为单位的视频高度 |\n| Fragments |将元数据堆积成的基于时间的视频块数组 |\n| start |片段的开始时间（以“刻度”为单位） |\n| duration |片段的长度（以“刻度”为单位） |\n| interval |给定片段中每个事件的间隔 |\n| events |包含区域的数组 |\n| region |表示检测到的单词或短语的对象 |\n| 语言 |区域中检测到的文本的语言 |\n| orientation |区域中检测到的文本的方向 |\n| lines |区域中检测到的文本的行数组 |\n| text |实际文本 |\n\n### <a name=\"json-output-example\"></a>JSON 输出示例\n以下输出示例包含常规视频信息和多个视频片段。 每个视频片段包含 OCR MP 检测到的每个区域及其语言和文本方向。 区域还包含此区域中的每个单词行，以及该行的文本、位置及其中每个单词的信息（单词内容、位置和置信度）。 下面是一个示例，我在其中嵌入了一些注释。\n\n    {\n        \"version\": 1, \n        \"timescale\": 90000, \n        \"offset\": 0, \n        \"framerate\": 30, \n        \"width\": 640, \n        \"height\": 480,  // general video information\n        \"fragments\": [\n            {\n                \"start\": 0, \n                \"duration\": 180000, \n                \"interval\": 90000,  // the time information about this fragment\n                \"events\": [\n                    [\n                       { \n                            \"region\": { // the detected region array in this fragment \n                                \"language\": \"English\",  // region language\n                                \"orientation\": \"Up\",  // text orientation\n                                \"lines\": [  // line information array in this region, including the text and the position\n                                    {\n                                        \"text\": \"One Two\", \n                                        \"left\": 10, \n                                        \"top\": 10, \n                                        \"right\": 210, \n                                        \"bottom\": 110, \n                                        \"word\": [  // word information array in this line\n                                            {\n                                                \"text\": \"One\", \n                                                \"left\": 10, \n                                                \"top\": 10, \n                                                \"right\": 110, \n                                                \"bottom\": 110, \n                                                \"confidence\": 900\n                                            }, \n                                            {\n                                                \"text\": \"Two\", \n                                                \"left\": 110, \n                                                \"top\": 10, \n                                                \"right\": 210, \n                                                \"bottom\": 110, \n                                                \"confidence\": 910\n                                            }\n                                        ]\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                ]\n            }\n        ]\n    }\n\n## <a name=\"net-sample-code\"></a>.NET 示例代码\n\n以下程序演示如何：\n\n1. 创建资产并将媒体文件上传到资产。\n2. 使用 OCR 配置/预设文件创建作业。\n3. 下载输出 JSON 文件。 \n   \n#### <a name=\"create-and-configure-a-visual-studio-project\"></a>创建和配置 Visual Studio 项目\n\n设置开发环境，并在 app.config 文件中填充连接信息，如[使用 .NET 进行媒体服务开发](media-services-dotnet-how-to-use.md)中所述。 \n\n#### <a name=\"example\"></a>示例\n\n    using System;\n    using System.Configuration;\n    using System.IO;\n    using System.Linq;\n    using Microsoft.WindowsAzure.MediaServices.Client;\n    using System.Threading;\n    using System.Threading.Tasks;\n\n    namespace OCR\n    {\n        class Program\n        {\n            // Read values from the App.config file.\n            private static readonly string _AADTenantDomain =\n                ConfigurationManager.AppSettings[\"AADTenantDomain\"];\n            private static readonly string _RESTAPIEndpoint =\n                ConfigurationManager.AppSettings[\"MediaServiceRESTAPIEndpoint\"];\n\n            // Field for service context.\n            private static CloudMediaContext _context = null;\n\n            static void Main(string[] args)\n            {\n                var tokenCredentials = new AzureAdTokenCredentials(_AADTenantDomain, AzureEnvironments.AzureChinaCloudEnvironment);\n                var tokenProvider = new AzureAdTokenProvider(tokenCredentials);\n\n                _context = new CloudMediaContext(new Uri(_RESTAPIEndpoint), tokenProvider);\n\n                // Run the OCR job.\n                var asset = RunOCRJob(@\"C:\\supportFiles\\OCR\\presentation.mp4\",\n                                            @\"C:\\supportFiles\\OCR\\config.json\");\n\n                // Download the job output asset.\n                DownloadAsset(asset, @\"C:\\supportFiles\\OCR\\Output\");\n            }\n\n            static IAsset RunOCRJob(string inputMediaFilePath, string configurationFile)\n            {\n                // Create an asset and upload the input media file to storage.\n                IAsset asset = CreateAssetAndUploadSingleFile(inputMediaFilePath,\n                    \"My OCR Input Asset\",\n                    AssetCreationOptions.None);\n\n                // Declare a new job.\n                IJob job = _context.Jobs.Create(\"My OCR Job\");\n\n                // Get a reference to Azure Media OCR.\n                string MediaProcessorName = \"Azure Media OCR\";\n\n                var processor = GetLatestMediaProcessorByName(MediaProcessorName);\n\n                // Read configuration from the specified file.\n                string configuration = File.ReadAllText(configurationFile);\n\n                // Create a task with the encoding details, using a string preset.\n                ITask task = job.Tasks.AddNew(\"My OCR Task\",\n                    processor,\n                    configuration,\n                    TaskOptions.None);\n\n                // Specify the input asset.\n                task.InputAssets.Add(asset);\n\n                // Add an output asset to contain the results of the job.\n                task.OutputAssets.AddNew(\"My OCR Output Asset\", AssetCreationOptions.None);\n\n                // Use the following event handler to check job progress.  \n                job.StateChanged += new EventHandler<JobStateChangedEventArgs>(StateChanged);\n\n                // Launch the job.\n                job.Submit();\n\n                // Check job execution and wait for job to finish.\n                Task progressJobTask = job.GetExecutionProgressTask(CancellationToken.None);\n\n                progressJobTask.Wait();\n\n                // If job state is Error, the event handling\n                // method for job progress should log errors.  Here we check\n                // for error state and exit if needed.\n                if (job.State == JobState.Error)\n                {\n                    ErrorDetail error = job.Tasks.First().ErrorDetails.First();\n                    Console.WriteLine(string.Format(\"Error: {0}. {1}\",\n                                                    error.Code,\n                                                    error.Message));\n                    return null;\n                }\n\n                return job.OutputMediaAssets[0];\n            }\n\n            static IAsset CreateAssetAndUploadSingleFile(string filePath, string assetName, AssetCreationOptions options)\n            {\n                IAsset asset = _context.Assets.Create(assetName, options);\n\n                var assetFile = asset.AssetFiles.Create(Path.GetFileName(filePath));\n                assetFile.Upload(filePath);\n\n                return asset;\n            }\n\n            static void DownloadAsset(IAsset asset, string outputDirectory)\n            {\n                foreach (IAssetFile file in asset.AssetFiles)\n                {\n                    file.Download(Path.Combine(outputDirectory, file.Name));\n                }\n            }\n\n            static IMediaProcessor GetLatestMediaProcessorByName(string mediaProcessorName)\n            {\n                var processor = _context.MediaProcessors\n                    .Where(p => p.Name == mediaProcessorName)\n                    .ToList()\n                    .OrderBy(p => new Version(p.Version))\n                    .LastOrDefault();\n\n                if (processor == null)\n                    throw new ArgumentException(string.Format(\"Unknown media processor\",\n                                                               mediaProcessorName));\n\n                return processor;\n            }\n\n            static private void StateChanged(object sender, JobStateChangedEventArgs e)\n            {\n                Console.WriteLine(\"Job state changed event:\");\n                Console.WriteLine(\"  Previous state: \" + e.PreviousState);\n                Console.WriteLine(\"  Current state: \" + e.CurrentState);\n\n                switch (e.CurrentState)\n                {\n                    case JobState.Finished:\n                        Console.WriteLine();\n                        Console.WriteLine(\"Job is finished.\");\n                        Console.WriteLine();\n                        break;\n                    case JobState.Canceling:\n                    case JobState.Queued:\n                    case JobState.Scheduled:\n                    case JobState.Processing:\n                        Console.WriteLine(\"Please wait...\\n\");\n                        break;\n                    case JobState.Canceled:\n                    case JobState.Error:\n                        // Cast sender as a job.\n                        IJob job = (IJob)sender;\n                        // Display or log error details as needed.\n                        // LogJobStop(job.Id);\n                        break;\n                    default:\n                        break;\n                }\n            }\n\n        }\n    }\n\n## <a name=\"related-links\"></a>相关链接\n[Azure 媒体服务分析概述](media-services-analytics-overview.md)\n<!--Update_Description: update code to use AAD token instead of ACS-->\n"}