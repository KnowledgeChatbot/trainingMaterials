{"Title":"使用 Azure 媒体分析检测面部和情绪","Description":"本主题演示如何使用 Azure 媒体分析检测人脸和情感。","Content":"# <a name=\"detect-face-and-emotion-with-azure-media-analytics\"></a>使用 Azure 媒体分析检测面部和情绪\r\n\r\n## <a name=\"overview\"></a>概述\r\n\r\n借助 **Azure Media Face Detector** 媒体处理器 (MP)，可通过面部表情来统计、跟踪动作，甚至计量受众的参与和反应。 此服务包含两项功能：\r\n\r\n* **面部检测**\r\n\r\n    面部检测能够找出并跟踪视频中的人脸。 可以检测多个面部，随后随着对象移动进行跟踪，并将时间和位置的元数据以 JSON 文件的形式返回。 跟踪期间，该服务会在人员于屏幕上四处移动时，尝试为他们的面部赋予相同的 ID，即使他们被挡住或暂时离帧。\r\n\r\n  > [!NOTE]\r\n  > 此服务并不执行面部识别。 面部离帧或被挡住太久的人员，会在回来时赋予新的 ID。\r\n\r\n* **情绪检测**\r\n\r\n    情绪检测是面部检测媒体处理器的可选组件，它根据检测到的面部返回多个情绪属性的分析，包括快乐、悲伤、恐惧、愤怒等等。\r\n\r\n**Azure 媒体面部检测器** MP 目前以预览版提供。\r\n\r\n本主题提供有关 Azure Media Face Detector 的详细信息，并演示如何通过适用于 .NET 的媒体服务 SDK 使用它。\r\n\r\n## <a name=\"face-detector-input-files\"></a>面部检测器输入文件\r\n\r\n视频文件。 目前支持以下格式：MP4、MOV 和 WMV。\r\n\r\n## <a name=\"face-detector-output-files\"></a>面部检测器输出文件\r\n\r\n面部检测器和跟踪 API 可提供高精确度的面部位置检测和跟踪功能，并在单个视频中检测到最多 64 个人脸。 正面的面部可提供最佳效果，而侧面的面部和较小的面部（小于或等于 24x24 像素）可能就无法获得相同的精确度。\r\n\r\n已检测到并已跟踪的面部会在坐标（左侧、顶部、宽度和高度）中返回，其中会在以像素为单位的图像中指明面部的位置，以及表示正在跟踪该人员的面部 ID 编号。 在正面面部长时间于帧中消失或重叠的情况下，面部 ID 编号很容易重置，导致某些人员被分配多个 ID。\r\n\r\n## <a id=\"output_elements\"></a>输出 JSON 文件中的元素\r\n\r\n[!INCLUDE [media-services-analytics-output-json](../../includes/media-services-analytics-output-json.md)]\r\n\r\n面部检测器使用分片（元数据可以分解为基于时间的区块，可以只下载需要的部分）和分段（可以在事件数过于庞大的情况下对事件进行分解）技术。 一些简单的计算可帮助转换数据。 例如，如果事件从 6300（刻度）开始，其时间刻度为 2997（刻度/秒），帧速率为 29.97（帧/秒），那么：\r\n\r\n* 开始时间/时间刻度 = 2.1 秒\r\n* 秒数 x 帧速率 = 63 帧\r\n\r\n## <a name=\"face-detection-input-and-output-example\"></a>面部检测输入和输出示例\r\n\r\n### <a name=\"input-video\"></a>输入视频\r\n\r\n[输入视频](http://ampdemo.azureedge.net/azuremediaplayer.html?url=https%3A%2F%2Freferencestream-samplestream.streaming.mediaservices.windows.net%2Fc8834d9f-0b49-4b38-bcaf-ece2746f1972%2FMicrosoft%20Convergence%202015%20%20Keynote%20Highlights.ism%2Fmanifest&amp;autoplay=false)\r\n\r\n### <a name=\"task-configuration-preset\"></a>任务配置（预设）\r\n\r\n在使用 **Azure 媒体面部检测器**创建任务时，必须指定配置预设。 以下配置预设仅适用于面部检测。\r\n\r\n    {\r\n      \"version\":\"1.0\",\r\n      \"options\":{\r\n          \"TrackingMode\": \"Fast\"\r\n      }\r\n    }\r\n\r\n#### <a name=\"attribute-descriptions\"></a>属性说明\r\n\r\n| 属性名称 | 说明 |\r\n| --- | --- |\r\n| Mode |快速 - 处理速度快，但准确度较低（默认）。|\r\n\r\n### <a name=\"json-output\"></a>JSON 输出\r\n\r\n下面是 JSON 输出被截断的示例。\r\n\r\n    {\r\n    \"version\": 1,\r\n    \"timescale\": 30000,\r\n    \"offset\": 0,\r\n    \"framerate\": 29.97,\r\n    \"width\": 1280,\r\n    \"height\": 720,\r\n    \"fragments\": [\r\n        {\r\n        \"start\": 0,\r\n        \"duration\": 60060\r\n        },\r\n        {\r\n        \"start\": 60060,\r\n        \"duration\": 60060,\r\n        \"interval\": 1001,\r\n        \"events\": [\r\n            [\r\n            {\r\n                \"id\": 0,\r\n                \"x\": 0.519531,\r\n                \"y\": 0.180556,\r\n                \"width\": 0.0867188,\r\n                \"height\": 0.154167\r\n            }\r\n            ],\r\n            [\r\n            {\r\n                \"id\": 0,\r\n                \"x\": 0.517969,\r\n                \"y\": 0.181944,\r\n                \"width\": 0.0867188,\r\n                \"height\": 0.154167\r\n            }\r\n            ],\r\n            [\r\n            {\r\n                \"id\": 0,\r\n                \"x\": 0.517187,\r\n                \"y\": 0.183333,\r\n                \"width\": 0.0851562,\r\n                \"height\": 0.151389\r\n            }\r\n            ],\r\n\r\n        . . .\r\n\r\n## <a name=\"emotion-detection-input-and-output-example\"></a>情绪检测输入和输出示例\r\n\r\n### <a name=\"input-video\"></a>输入视频\r\n\r\n[输入视频](http://ampdemo.azureedge.net/azuremediaplayer.html?url=https%3A%2F%2Freferencestream-samplestream.streaming.mediaservices.windows.net%2Fc8834d9f-0b49-4b38-bcaf-ece2746f1972%2FMicrosoft%20Convergence%202015%20%20Keynote%20Highlights.ism%2Fmanifest&amp;autoplay=false)\r\n\r\n### <a name=\"task-configuration-preset\"></a>任务配置（预设）\r\n\r\n在使用 **Azure 媒体面部检测器**创建任务时，必须指定配置预设。 以下配置预设指定基于情绪检测创建 JSON。\r\n\r\n    {\r\n      \"version\": \"1.0\",\r\n      \"options\": {\r\n        \"aggregateEmotionWindowMs\": \"987\",\r\n        \"mode\": \"aggregateEmotion\",\r\n        \"aggregateEmotionIntervalMs\": \"342\"\r\n      }\r\n    }\r\n\r\n#### <a name=\"attribute-descriptions\"></a>属性说明\r\n\r\n| 属性名称 | 说明 |\r\n| --- | --- |\r\n| Mode |Faces：仅人脸检测。<br/>PerFaceEmotion：独立返回每个人脸检测的情绪。<br/>AggregateEmotion：返回帧中所有面部的平均情绪值。 |\r\n| AggregateEmotionWindowMs |在已选择 AggregateEmotion 模式时使用。 指定用于生成每个聚合结果的视频的长度，以毫秒为单位。 |\r\n| AggregateEmotionIntervalMs |在已选择 AggregateEmotion 模式时使用。 指定生成聚合结果的频率。 |\r\n\r\n#### <a name=\"aggregate-defaults\"></a>聚合默认值\r\n\r\n下面是聚合窗口和间隔设置的建议值。 AggregateEmotionWindowMs 应该超过 AggregateEmotionIntervalMs。\r\n\r\n|| 默认值 | 最大值 | 最小值 |\r\n|--- | --- | --- | --- |\r\n| AggregateEmotionWindowMs |0.5 |2 |0.25|\r\n| AggregateEmotionIntervalMs |0.5 |1 |0.25|\r\n\r\n### <a name=\"json-output\"></a>JSON 输出\r\n\r\n聚合情绪的 JSON 输出（已截断）：\r\n\r\n    {\r\n     \"version\": 1,\r\n     \"timescale\": 30000,\r\n     \"offset\": 0,\r\n     \"framerate\": 29.97,\r\n     \"width\": 1280,\r\n     \"height\": 720,\r\n     \"fragments\": [\r\n       {\r\n         \"start\": 0,\r\n         \"duration\": 60060,\r\n         \"interval\": 15015,\r\n         \"events\": [\r\n           [\r\n             {\r\n               \"windowFaceDistribution\": {\r\n                 \"neutral\": 0,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               },\r\n               \"windowMeanScores\": {\r\n                 \"neutral\": 0,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               }\r\n             }\r\n           ],\r\n           [\r\n             {\r\n               \"windowFaceDistribution\": {\r\n                 \"neutral\": 0,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               },\r\n               \"windowMeanScores\": {\r\n                 \"neutral\": 0,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               }\r\n             }\r\n           ],\r\n           [\r\n             {\r\n               \"windowFaceDistribution\": {\r\n                 \"neutral\": 0,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               },\r\n               \"windowMeanScores\": {\r\n                 \"neutral\": 0,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               }\r\n             }\r\n           ],\r\n           [\r\n             {\r\n               \"windowFaceDistribution\": {\r\n                 \"neutral\": 0,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               },\r\n               \"windowMeanScores\": {\r\n                 \"neutral\": 0,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               }\r\n             }\r\n           ]\r\n         ]\r\n       },\r\n       {\r\n         \"start\": 60060,\r\n         \"duration\": 60060,\r\n         \"interval\": 15015,\r\n         \"events\": [\r\n           [\r\n             {\r\n               \"windowFaceDistribution\": {\r\n                 \"neutral\": 1,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n                 \"contempt\": 0\r\n               },\r\n               \"windowMeanScores\": {\r\n                 \"neutral\": 0.688541,\r\n                 \"happiness\": 0.0586323,\r\n                 \"surprise\": 0.227184,\r\n                 \"sadness\": 0.00945675,\r\n                 \"anger\": 0.00592107,\r\n                 \"disgust\": 0.00154993,\r\n                 \"fear\": 0.00450447,\r\n                 \"contempt\": 0.0042109\r\n               }\r\n             }\r\n           ],\r\n           [\r\n             {\r\n               \"windowFaceDistribution\": {\r\n                 \"neutral\": 1,\r\n                 \"happiness\": 0,\r\n                 \"surprise\": 0,\r\n                 \"sadness\": 0,\r\n                 \"anger\": 0,\r\n                 \"disgust\": 0,\r\n                 \"fear\": 0,\r\n\r\n## <a name=\"limitations\"></a>限制\r\n\r\n* 支持的输入视频格式包括 MP4、MOV 和 WMV。\r\n* 可检测的面部大小范围为 24x24 到 2048x2048 像素。 无法检测此范围以外的面部。\r\n* 对于每个视频，返回的面部数上限为 64。\r\n* 某些面部可能因技术难题而无法检测，例如非常大的面部角度（头部姿势），以及较大的阻挡物。 正面和接近正面的面部可提供最佳效果。\r\n\r\n## <a name=\"net-sample-code\"></a>.NET 示例代码\r\n\r\n以下程序演示如何：\r\n\r\n1. 创建资产并将媒体文件上传到资产。\r\n2. 使用人脸检测任务创建一个作业，所根据的配置文件包含以下 json 预设。 \r\n   \r\n        {\r\n            \"version\": \"1.0\"\r\n        }\r\n3. 下载输出 JSON 文件。 \r\n\r\n#### <a name=\"create-and-configure-a-visual-studio-project\"></a>创建和配置 Visual Studio 项目\r\n\r\n设置开发环境，并根据[使用 .NET 进行媒体服务开发](media-services-dotnet-how-to-use.md)中所述，在 app.config 文件中填充连接信息。 \r\n\r\n#### <a name=\"example\"></a>示例\r\n\r\n    using System;\r\n    using System.Configuration;\r\n    using System.IO;\r\n    using System.Linq;\r\n    using Microsoft.WindowsAzure.MediaServices.Client;\r\n    using System.Threading;\r\n    using System.Threading.Tasks;\r\n\r\n    namespace FaceDetection\r\n    {\r\n        class Program\r\n        {\r\n            private static readonly string _AADTenantDomain =\r\n                      ConfigurationManager.AppSettings[\"AADTenantDomain\"];\r\n            private static readonly string _RESTAPIEndpoint =\r\n                      ConfigurationManager.AppSettings[\"MediaServiceRESTAPIEndpoint\"];\r\n\r\n            // Field for service context.\r\n            private static CloudMediaContext _context = null;\r\n\r\n            static void Main(string[] args)\r\n            {\r\n                var tokenCredentials = new AzureAdTokenCredentials(_AADTenantDomain, AzureEnvironments.AzureChinaCloudEnvironment);\r\n                var tokenProvider = new AzureAdTokenProvider(tokenCredentials);\r\n\r\n                _context = new CloudMediaContext(new Uri(_RESTAPIEndpoint), tokenProvider);\r\n\r\n                // Run the FaceDetection job.\r\n                var asset = RunFaceDetectionJob(@\"C:\\supportFiles\\FaceDetection\\BigBuckBunny.mp4\",\r\n                                            @\"C:\\supportFiles\\FaceDetection\\config.json\");\r\n\r\n                // Download the job output asset.\r\n                DownloadAsset(asset, @\"C:\\supportFiles\\FaceDetection\\Output\");\r\n            }\r\n\r\n            static IAsset RunFaceDetectionJob(string inputMediaFilePath, string configurationFile)\r\n            {\r\n                // Create an asset and upload the input media file to storage.\r\n                IAsset asset = CreateAssetAndUploadSingleFile(inputMediaFilePath,\r\n                    \"My Face Detection Input Asset\",\r\n                    AssetCreationOptions.None);\r\n\r\n                // Declare a new job.\r\n                IJob job = _context.Jobs.Create(\"My Face Detection Job\");\r\n\r\n                // Get a reference to Azure Media Face Detector.\r\n                string MediaProcessorName = \"Azure Media Face Detector\";\r\n\r\n                var processor = GetLatestMediaProcessorByName(MediaProcessorName);\r\n\r\n                // Read configuration from the specified file.\r\n                string configuration = File.ReadAllText(configurationFile);\r\n\r\n                // Create a task with the encoding details, using a string preset.\r\n                ITask task = job.Tasks.AddNew(\"My Face Detection Task\",\r\n                    processor,\r\n                    configuration,\r\n                    TaskOptions.None);\r\n\r\n                // Specify the input asset.\r\n                task.InputAssets.Add(asset);\r\n\r\n                // Add an output asset to contain the results of the job.\r\n                task.OutputAssets.AddNew(\"My Face Detectoion Output Asset\", AssetCreationOptions.None);\r\n\r\n                // Use the following event handler to check job progress.  \r\n                job.StateChanged += new EventHandler<JobStateChangedEventArgs>(StateChanged);\r\n\r\n                // Launch the job.\r\n                job.Submit();\r\n\r\n                // Check job execution and wait for job to finish.\r\n                Task progressJobTask = job.GetExecutionProgressTask(CancellationToken.None);\r\n\r\n                progressJobTask.Wait();\r\n\r\n                // If job state is Error, the event handling\r\n                // method for job progress should log errors.  Here we check\r\n                // for error state and exit if needed.\r\n                if (job.State == JobState.Error)\r\n                {\r\n                    ErrorDetail error = job.Tasks.First().ErrorDetails.First();\r\n                    Console.WriteLine(string.Format(\"Error: {0}. {1}\",\r\n                                                    error.Code,\r\n                                                    error.Message));\r\n                    return null;\r\n                }\r\n\r\n                return job.OutputMediaAssets[0];\r\n            }\r\n\r\n            static IAsset CreateAssetAndUploadSingleFile(string filePath, string assetName, AssetCreationOptions options)\r\n            {\r\n                IAsset asset = _context.Assets.Create(assetName, options);\r\n\r\n                var assetFile = asset.AssetFiles.Create(Path.GetFileName(filePath));\r\n                assetFile.Upload(filePath);\r\n\r\n                return asset;\r\n            }\r\n\r\n            static void DownloadAsset(IAsset asset, string outputDirectory)\r\n            {\r\n                foreach (IAssetFile file in asset.AssetFiles)\r\n                {\r\n                    file.Download(Path.Combine(outputDirectory, file.Name));\r\n                }\r\n            }\r\n\r\n            static IMediaProcessor GetLatestMediaProcessorByName(string mediaProcessorName)\r\n            {\r\n                var processor = _context.MediaProcessors\r\n                    .Where(p => p.Name == mediaProcessorName)\r\n                    .ToList()\r\n                    .OrderBy(p => new Version(p.Version))\r\n                    .LastOrDefault();\r\n\r\n                if (processor == null)\r\n                    throw new ArgumentException(string.Format(\"Unknown media processor\",\r\n                                                               mediaProcessorName));\r\n\r\n                return processor;\r\n            }\r\n\r\n            static private void StateChanged(object sender, JobStateChangedEventArgs e)\r\n            {\r\n                Console.WriteLine(\"Job state changed event:\");\r\n                Console.WriteLine(\"  Previous state: \" + e.PreviousState);\r\n                Console.WriteLine(\"  Current state: \" + e.CurrentState);\r\n\r\n                switch (e.CurrentState)\r\n                {\r\n                    case JobState.Finished:\r\n                        Console.WriteLine();\r\n                        Console.WriteLine(\"Job is finished.\");\r\n                        Console.WriteLine();\r\n                        break;\r\n                    case JobState.Canceling:\r\n                    case JobState.Queued:\r\n                    case JobState.Scheduled:\r\n                    case JobState.Processing:\r\n                        Console.WriteLine(\"Please wait...\\n\");\r\n                        break;\r\n                    case JobState.Canceled:\r\n                    case JobState.Error:\r\n                        // Cast sender as a job.\r\n                        IJob job = (IJob)sender;\r\n                        // Display or log error details as needed.\r\n                        // LogJobStop(job.Id);\r\n                        break;\r\n                    default:\r\n                        break;\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n\r\n## <a name=\"related-links\"></a>相关链接\r\n[Azure 媒体服务分析概述](media-services-analytics-overview.md)\r\n\r\n[Azure Media Analytics demos（Azure 媒体分析演示）](http://amslabs.azurewebsites.net/demos/Analytics.html)\r\n\r\n<!--Update_Description: update Aggregate defaults table-->"}