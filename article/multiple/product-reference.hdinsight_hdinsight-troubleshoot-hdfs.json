{"Title":"使用 Azure HDinsight 排除 HDFS 故障","Description":"获取有关使用 HDFS 和 Azure HDInsight 的常见问题答案。","Content":"# <a name=\"troubleshoot-hdfs-by-using-azure-hdinsight\"></a>使用 Azure HDInsight 排除 HDFS 故障\r\n\r\n了解在 Apache Ambari 中使用 Hadoop 分布式文件系统 (HDFS) 有效负载时遇到的常见问题及其解决方法。\r\n\r\n## <a name=\"how-do-i-access-local-hdfs-from-inside-a-cluster\"></a>如何从群集内访问本地 HDFS？\r\n\r\n### <a name=\"issue\"></a>问题\r\n\r\n从 HDInsight 群集内通过命令行和应用程序代码（而不是 Azure Blob 存储）访问本地 HDFS。   \r\n\r\n### <a name=\"resolution-steps\"></a>解决步骤\r\n\r\n1. 在命令提示符下，按原义使用 `hdfs dfs -D \"fs.default.name=hdfs://mycluster/\" ...`，如以下命令中所示：\r\n\r\n    ```apache\r\n    hdiuser@hn0-spark2:~$ hdfs dfs -D \"fs.default.name=hdfs://mycluster/\" -ls /\r\n    Found 3 items\r\n    drwxr-xr-x   - hdiuser hdfs          0 2017-03-24 14:12 /EventCheckpoint-30-8-24-11102016-01\r\n    drwx-wx-wx   - hive    hdfs          0 2016-11-10 18:42 /tmp\r\n    drwx------   - hdiuser hdfs          0 2016-11-10 22:22 /user\r\n    ```\r\n\r\n2. 从源代码按原义使用 URI `hdfs://mycluster/`，如以下示例应用程序中所示：\r\n\r\n    ```csharp\r\n    import java.io.IOException;\r\n    import java.net.URI;\r\n    import org.apache.commons.io.IOUtils;\r\n    import org.apache.hadoop.conf.Configuration;\r\n    import org.apache.hadoop.fs.*;\r\n    \r\n    public class JavaUnitTests {\r\n    \r\n        public static void main(String[] args) throws Exception {\r\n    \r\n            Configuration conf = new Configuration();\r\n            String hdfsUri = \"hdfs://mycluster/\";\r\n            conf.set(\"fs.defaultFS\", hdfsUri);\r\n            FileSystem fileSystem = FileSystem.get(URI.create(hdfsUri), conf);\r\n            RemoteIterator<LocatedFileStatus> fileStatusIterator = fileSystem.listFiles(new Path(\"/tmp\"), true);\r\n            while(fileStatusIterator.hasNext()) {\r\n                System.out.println(fileStatusIterator.next().getPath().toString());\r\n            }\r\n        }\r\n    }\r\n    ```\r\n    \r\n3. 使用以下命令在 HDInsight 群集上运行已编译的 .jar 文件（例如，名为 `java-unit-tests-1.0.jar` 的文件）：\r\n\r\n    ```apache\r\n    hdiuser@hn0-spark2:~$ hadoop jar java-unit-tests-1.0.jar JavaUnitTests\r\n    hdfs://mycluster/tmp/hive/hive/5d9cf301-2503-48c7-9963-923fb5ef79a7/inuse.info\r\n    hdfs://mycluster/tmp/hive/hive/5d9cf301-2503-48c7-9963-923fb5ef79a7/inuse.lck\r\n    hdfs://mycluster/tmp/hive/hive/a0be04ea-ae01-4cc4-b56d-f263baf2e314/inuse.info\r\n    hdfs://mycluster/tmp/hive/hive/a0be04ea-ae01-4cc4-b56d-f263baf2e314/inuse.lck\r\n    ```\r\n\r\n\r\n## <a name=\"how-do-i-force-disable-hdfs-safe-mode-in-a-cluster\"></a>如何在群集中强制禁用 HDFS 安全模式？\r\n\r\n### <a name=\"issue\"></a>问题\r\n\r\n本地 HDFS 在 HDInsight 群集上的安全模式下停止响应。   \r\n\r\n### <a name=\"detailed-description\"></a>详细说明\r\n\r\n运行以下 HDFS 命令时，将发生错误：\r\n\r\n```apache\r\nhdfs dfs -D \"fs.default.name=hdfs://mycluster/\" -mkdir /temp\r\n```\r\n\r\n运行命令时，会看到以下错误：\r\n\r\n```apache\r\nhdiuser@hn0-spark2:~$ hdfs dfs -D \"fs.default.name=hdfs://mycluster/\" -mkdir /temp\r\n17/04/05 16:20:52 WARN retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.mkdirs over hn0-spark2.2oyzcdm4sfjuzjmj5dnmvscjpg.dx.internal.chinacloudapp.cn/10.0.0.22:8020. Not retrying because try once and fail.\r\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /temp. Name node is in safe mode.\r\nIt was turned on manually. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off.\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1359)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4010)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1102)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:630)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)\r\n        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1496)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1396)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n        at com.sun.proxy.$Proxy10.mkdirs(Unknown Source)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:603)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)\r\n        at com.sun.proxy.$Proxy11.mkdirs(Unknown Source)\r\n        at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3061)\r\n        at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3031)\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1162)\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1158)\r\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1158)\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1150)\r\n        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1898)\r\n        at org.apache.hadoop.fs.shell.Mkdir.processNonexistentPath(Mkdir.java:76)\r\n        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:273)\r\n        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:255)\r\n        at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:119)\r\n        at org.apache.hadoop.fs.shell.Command.run(Command.java:165)\r\n        at org.apache.hadoop.fs.FsShell.run(FsShell.java:297)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\r\n        at org.apache.hadoop.fs.FsShell.main(FsShell.java:350)\r\nmkdir: Cannot create directory /temp. Name node is in safe mode.\r\n```\r\n\r\n### <a name=\"probable-cause\"></a>可能的原因\r\n\r\nHDInsight 群集已减少到很少的节点。 节点数低于或接近于 HDFS 复制因子。\r\n\r\n### <a name=\"resolution-steps\"></a>解决步骤 \r\n\r\n1. 使用以下命令获取 HDInsight 群集上的 HDFS 状态：\r\n\r\n    ```apache\r\n    hdfs dfsadmin -D \"fs.default.name=hdfs://mycluster/\" -report\r\n    ```\r\n\r\n    ```apache\r\n    hdiuser@hn0-spark2:~$ hdfs dfsadmin -D \"fs.default.name=hdfs://mycluster/\" -report\r\n    Safe mode is ON\r\n    Configured Capacity: 3372381241344 (3.07 TB)\r\n    Present Capacity: 3138625077248 (2.85 TB)\r\n    DFS Remaining: 3102710317056 (2.82 TB)\r\n    DFS Used: 35914760192 (33.45 GB)\r\n    DFS Used%: 1.14%\r\n    Under replicated blocks: 0\r\n    Blocks with corrupt replicas: 0\r\n    Missing blocks: 0\r\n    Missing blocks (with replication factor 1): 0\r\n    \r\n    -------------------------------------------------\r\n    Live datanodes (8):\r\n    \r\n    Name: 10.0.0.17:30010 (10.0.0.17)\r\n    Hostname: 10.0.0.17\r\n    Decommission Status : Normal\r\n    Configured Capacity: 421547655168 (392.60 GB)\r\n    DFS Used: 5288128512 (4.92 GB)\r\n    Non DFS Used: 29087272960 (27.09 GB)\r\n    DFS Remaining: 387172253696 (360.58 GB)\r\n    DFS Used%: 1.25%\r\n    DFS Remaining%: 91.85%\r\n    Configured Cache Capacity: 0 (0 B)\r\n    Cache Used: 0 (0 B)\r\n    Cache Remaining: 0 (0 B)\r\n    Cache Used%: 100.00%\r\n    Cache Remaining%: 0.00%\r\n    Xceivers: 2\r\n    Last contact: Wed Apr 05 16:22:00 UTC 2017\r\n    ...\r\n    ```\r\n\r\n2. 使用以下命令检查 HDInsight 群集上的 HDFS 完整性：\r\n\r\n    ```apache\r\n    hdiuser@hn0-spark2:~$ hdfs fsck -D \"fs.default.name=hdfs://mycluster/\" /\r\n    ```\r\n    \r\n    ```apache\r\n    Connecting to namenode via http://hn0-spark2.2oyzcdm4sfjuzjmj5dnmvscjpg.dx.internal.chinacloudapp.cn:30070/fsck?ugi=hdiuser&path=%2F\r\n    FSCK started by hdiuser (auth:SIMPLE) from /10.0.0.22 for path / at Wed Apr 05 16:40:28 UTC 2017\r\n    ....................................................................................................\r\n    \r\n    ....................................................................................................\r\n    ..................Status: HEALTHY\r\n     Total size:    9330539472 B\r\n     Total dirs:    37\r\n     Total files:   2618\r\n     Total symlinks:                0 (Files currently being written: 2)\r\n     Total blocks (validated):      2535 (avg. block size 3680686 B)\r\n     Minimally replicated blocks:   2535 (100.0 %)\r\n     Over-replicated blocks:        0 (0.0 %)\r\n     Under-replicated blocks:       0 (0.0 %)\r\n     Mis-replicated blocks:         0 (0.0 %)\r\n     Default replication factor:    3\r\n     Average block replication:     3.0\r\n     Corrupt blocks:                0\r\n     Missing replicas:              0 (0.0 %)\r\n     Number of data-nodes:          8\r\n     Number of racks:               1\r\n    FSCK ended at Wed Apr 05 16:40:28 UTC 2017 in 187 milliseconds\r\n    \r\n    The filesystem under path '/' is HEALTHY\r\n    ```\r\n\r\n3. 如果确定没有缺失、损坏或复制不足的块，或者这些块可以忽略，请运行以下命令使名称节点退出安全模式：\r\n\r\n    ```apache\r\n    hdfs dfsadmin -D \"fs.default.name=hdfs://mycluster/\" -safemode leave\r\n    ```\r\n### <a name=\"see-also\"></a>另请参阅\r\n[使用 Azure HDInsight 进行故障排除](hdinsight-troubleshoot-guide.md)\r\n\r\n\r\n<!--Update_Description: update wording and link references-->"}